[
  {
    "paper_title": "The Voice of Equity: A Systematic Evaluation of Bias Mitigation Techniques for Speech-Based Cognitive Impairment Detection Across Architectures and Demographics",
    "paper_title_zh": "公平之声：跨架构和人口统计的基于语音的认知障碍检测的偏见缓解技术系统评估",
    "paper_id": "2601.16989",
    "paper_abstract": "Speech-based detection of cognitive impairment offers a scalable, non-invasive screening, yet algorithmic bias across demographic and linguistic subgroups remains critically underexplored. We present the first comprehensive fairness analysis framework for speech-based multi-class cognitive impairment detection, systematically evaluating bias mitigation across architectures, and demographic subgroups. We developed two transformer-based architectures, SpeechCARE-AGF and Whisper-LWF-LoRA, on the multilingual NIA PREPARE Challenge dataset. Unlike prior work that typically examines single mitigation techniques, we compared pre-processing, in-processing, and post-processing approaches, assessing fairness via Equality of Opportunity and Equalized Odds across gender, age, education, and language. Both models achieved strong performance (F1: SpeechCARE-AGF 70.87, Whisper-LWF-LoRA 71.46) but exhibited substantial fairness disparities. Adults >=80 showed lower sensitivity versus younger groups; Spanish speakers demonstrated reduced TPR versus English speakers. Mitigation effectiveness varied by architecture: oversampling improved SpeechCARE-AGF for older adults (80+ TPR: 46.19%=>49.97%) but minimally affected Whisper-LWF-LoRA. This study addresses a critical healthcare AI gap by demonstrating that architectural design fundamentally shapes bias patterns and mitigation effectiveness. Adaptive fusion mechanisms enable flexible responses to data interventions, while frequency reweighting offers robust improvements across architectures. Our findings establish that fairness interventions must be tailored to both model architecture and demographic characteristics, providing a systematic framework for developing equitable speech-based screening tools essential for reducing diagnostic disparities in cognitive healthcare.",
    "paper_abstract_zh": "基于语音的认知障碍检测提供了一种可扩展、非侵入性的筛查方法，然而算法在不同人口统计和语言子群体中的偏见仍然严重未被探索。我们提出了首个用于基于语音的多类认知障碍检测的综合公平性分析框架，系统性地评估了跨架构和人口统计子群体的偏见缓解技术。我们在多语言NIA PREPARE挑战赛数据集上开发了两种基于Transformer的架构：SpeechCARE-AGF和Whisper-LWF-LoRA。与以往通常研究单一缓解技术的工作不同，我们比较了预处理、处理中和后处理方法，通过平等机会和均衡赔率指标评估了性别、年龄、教育和语言方面的公平性。两种模型都取得了良好的性能（F1分数：SpeechCARE-AGF为70.87，Whisper-LWF-LoRA为71.46），但表现出显著的公平性差异。80岁及以上的成年人比年轻组表现出更低的敏感性；西班牙语说话人的真正阳性率低于英语说话人。缓解效果因架构而异：过采样改善了SpeechCARE-AGF对老年人的效果（80+ TPR：46.19%=>49.97%），但对Whisper-LWF-LoRA影响甚微。本研究通过证明架构设计从根本上塑造了偏见模式和缓解效果，解决了医疗保健AI中的一个关键空白。自适应融合机制使模型能够灵活响应数据干预，而频率重加权则在各种架构上提供了稳健的改进。我们的研究结果表明，公平性干预必须针对模型架构和人口统计特征量身定制，为开发公平的语音筛查工具提供了系统框架，这对于减少认知医疗保健中的诊断差异至关重要。",
    "subjects": [
      "Sound (cs.SD)",
      "Computation and Language (cs.CL)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2026-01-27",
    "paper_authors": "Yasaman Haghbin, Sina Rashidi, Ali Zolnour, Maryam Zolnoori",
    "topic": [
      "Audio Classification",
      "Speech Recognition"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "BickGraphing: Web-Based Application for Visual Inspection of Audio Recordings",
    "paper_title_zh": "BickGraphing：用于音频录音视觉检查的Web应用程序",
    "paper_id": "2601.17014",
    "paper_abstract": "BickGraphing is a browser based research tool that enables visual inspection of acoustic recordings. The tool was built in support of visualizing crop feeding pest sounds in support of the Insect Eavesdropper project; however, it is widely applicable to all audiovisualizations in research. It allows multiple uploads of large .wav files, computes waveforms and spectrograms locally, and supports interactive exploration of audio events in time and frequency. The application is implemented as a SvelteKit and TypeScript web app with a client side signal processing pipeline using WebAssembly compiled FFmpeg and custom FFT utilities. The software is released on an open Git repository (this https URL) and archived under a standard MIT license and can be reused for rapid visual quality checks of .wav recordings in insect bioacoustics and related fields. BickGraphing has the potential to be a local, easy to use coding free visualization platform for audio data in research.",
    "paper_abstract_zh": "BickGraphing是一个基于浏览器的研究工具，支持对声学录音进行视觉检查。该工具为可视化农作物害虫声音而开发，以支持Insect Eavesdropper项目；然而，它广泛应用于研究中的所有音频可视化。它支持上传多个大型.wav文件，本地计算波形和频谱图，并支持在时间和频率上对音频事件进行交互式探索。该应用程序使用SvelteKit和TypeScript实现为Web应用，采用客户端信号处理管道，使用WebAssembly编译的FFmpeg和自定义FFT工具。软件在开放的Git仓库（this https URL）上发布，采用标准MIT许可证归档，可在昆虫生物声学及相关领域重复使用，用于快速检查.wav录音的视觉质量。BickGraphing有望成为研究中音频数据的本地、易用、无需编码的可视化平台。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "update_time": "2026-01-27",
    "paper_authors": "Kayley Seow, Alexander Arovas, Grace Steinmetz, Emily Bick",
    "topic": [
      "Other"
    ],
    "category": [
      "Other"
    ]
  },
  {
    "paper_title": "PC-MCL: Patient-Consistent Multi-Cycle Learning with multi-label bias correction for respiratory sound classification",
    "paper_title_zh": "PC-MCL: 基于多标签偏差校正的患者一致多周期学习用于呼吸音分类",
    "paper_id": "2601.17080",
    "paper_abstract": "Automated respiratory sound classification supports the diagnosis of pulmonary diseases. However, many deep models still rely on cycle-level analysis and suffer from patient-specific overfitting. We propose PC-MCL (Patient-Consistent Multi-Cycle Learning) to address these limitations by utilizing three key components: multi-cycle concatenation, a 3-label formulation, and a patient-matching auxiliary task. Our work resolves a multi-label distributional bias in respiratory sound classification, a critical issue inherent to applying multi-cycle concatenation with the conventional 2-label formulation (crackle, wheeze). This bias manifests as a systematic loss of normal signal information when normal and abnormal cycles are combined. Our proposed 3-label formulation (normal, crackle, wheeze) corrects this by preserving information from all constituent cycles in mixed samples. Furthermore, the patient-matching auxiliary task acts as a multi-task regularizer, encouraging the model to learn more robust features and improving generalization. On the ICBHI 2017 benchmark, PC-MCL achieves an ICBHI Score of 65.37%, outperforming existing baselines. Ablation studies confirm that all three components are essential, working synergistically to improve the detection of abnormal respiratory events.",
    "paper_abstract_zh": "自动呼吸音分类有助于肺部疾病的诊断。然而，许多深度模型仍然依赖周期级分析，并受到患者特定过拟合的影响。我们提出了PC-MCL（患者一致多周期学习），通过利用三个关键组件来解决这些限制：多周期连接、三标签公式和患者匹配辅助任务。我们的工作解决了呼吸音分类中的多标签分布偏差，这是一个关键问题，该问题源于将传统二标签公式（爆裂音、喘鸣音）与多周期连接结合使用时产生的偏差。这种偏差表现为在正常和异常周期组合时，正常信号信息的系统性丢失。我们提出的三标签公式（正常、爆裂音、喘鸣音）通过保留混合样本中所有组成周期的信息来纠正这一问题。此外，患者匹配辅助任务作为多任务正则化器，鼓励模型学习更鲁棒的特征并提高泛化能力。在ICBHI 2017基准测试中，PC-MCL实现了65.37%的ICBHI分数，优于现有基线。消融研究证实所有三个组件都是必不可少的，它们协同工作以提高异常呼吸事件的检测能力。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)",
      "Artificial Intelligence (cs.AI)",
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)"
    ],
    "update_time": "2026-01-27",
    "paper_authors": "Seung Gyu Jeong, Seong-Eun Kim",
    "topic": [
      "Audio Classification",
      "Audio Representation Learning"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Recovering Performance in Speech Emotion Recognition from Discrete Tokens via Multi-Layer Fusion and Paralinguistic Feature Integration",
    "paper_title_zh": "通过多层融合和副语言特征集成从离散令牌中恢复语音情感识别性能",
    "paper_id": "2601.17085",
    "paper_abstract": "Discrete speech tokens offer significant advantages for storage and language model integration, but their application in speech emotion recognition (SER) is limited by paralinguistic information loss during quantization. This paper presents a comprehensive investigation of discrete tokens for SER. Using a fine-tuned WavLM-Large model, we systematically quantify performance degradation across different layer configurations and k-means quantization granularities. To recover the information loss, we propose two key strategies: (1) attention-based multi-layer fusion to recapture complementary information from different layers, and (2) integration of openSMILE features to explicitly reintroduce paralinguistic cues. We also compare mainstream neural codec tokenizers (SpeechTokenizer, DAC, EnCodec) and analyze their behaviors when fused with acoustic features. Our findings demonstrate that through multi-layer fusion and acoustic feature integration, discrete tokens can close the performance gap with continuous representations in SER tasks.",
    "paper_abstract_zh": "离散语音令牌在存储和语言模型集成方面具有显著优势，但在语音情感识别(SER)中的应用受到量化过程中副语言信息丢失的限制。本文对离散令牌在SER中的应用进行了全面研究。使用微调的WavLM-Large模型，我们系统地量化了不同层配置和k-means量化粒度下的性能退化。为了恢复信息丢失，我们提出了两种关键策略：(1)基于注意力的多层融合，以重新捕获来自不同层的互补信息；(2)集成openSMILE特征，以明确重新引入副语言线索。我们还比较了主流的神经编解码器令牌化器(SpeechTokenizer、DAC、EnCodec)，并分析了它们与声学特征融合时的行为。我们的研究结果表明，通过多层融合和声学特征集成，离散令牌可以在SER任务中缩小与连续表示的性能差距。",
    "subjects": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2026-01-27",
    "paper_authors": "Esther Sun, Abinay Reddy Naini, Carlos Busso",
    "topic": [
      "Audio Representation Learning",
      "Audio Classification",
      "Audio Codec"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Spoofing-Aware Speaker Verification via Wavelet Prompt Tuning and Multi-Model Ensembles",
    "paper_title_zh": "基于小波提示调优和多模型集成的欺骗感知说话人验证",
    "paper_id": "2601.17557",
    "paper_abstract": "This paper describes the UZH-CL system submitted to the SASV section of the WildSpoof 2026 challenge. The challenge focuses on the integrated defense against generative spoofing attacks by requiring the simultaneous verification of speaker identity and audio authenticity. We proposed a cascaded Spoofing-Aware Speaker Verification framework that integrates a Wavelet Prompt-Tuned XLSR-AASIST countermeasure with a multi-model ensemble. The ASV component utilizes the ResNet34, ResNet293, and WavLM-ECAPA-TDNN architectures, with Z-score normalization followed by score averaging. Trained on VoxCeleb2 and SpoofCeleb, the system obtained a Macro a-DCF of 0.2017 and a SASV EER of 2.08%. While the system achieved a 0.16% EER in spoof detection on the in-domain data, results on unseen datasets, such as the ASVspoof5, highlight the critical challenge of cross-domain generalization.",
    "paper_abstract_zh": "本文描述了提交给WildSpoof 2026挑战赛SASV部分的UZH-CL系统。该挑战赛要求同时验证说话人身份和音频真实性，以实现对生成性欺骗攻击的综合防御。我们提出了一种级联的欺骗感知说话人验证框架，集成了小波提示调优的XLSR-AASIST反措施和多模型集成。ASV组件利用ResNet34、ResNet293和WavLM-ECAPA-TDNN架构，采用Z分数标准化后进行分数平均。在VoxCeleb2和SpoofCeleb上训练后，系统获得了0.2017的Macro a-DCF和2.08%的SASV EER。尽管系统在域内数据上的欺骗检测EER达到了0.16%，但在ASVspoof5等未见数据集上的结果凸显了跨域泛化的关键挑战。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "update_time": "2026-01-27",
    "paper_authors": "Aref Farhadipour, Ming Jin, Valeriia Vyshnevetska, Xiyang Li, Elisa Pellegrino, Srikanth Madikeri",
    "topic": [
      "Audio Classification",
      "Speech Recognition"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "ToS: A Team of Specialists ensemble framework for Stereo Sound Event Localization and Detection with distance estimation in Video",
    "paper_title_zh": "ToS：一种用于视频立体声音事件定位与检测及距离估计的专家团队集成框架",
    "paper_id": "2601.17611",
    "paper_abstract": "Sound event localization and detection with distance estimation (3D SELD) in video involves identifying active sound events at each time frame while estimating their spatial coordinates. This multimodal task requires joint reasoning across semantic, spatial, and temporal dimensions, a challenge that single models often struggle to address effectively. To tackle this, we introduce the Team of Specialists (ToS) ensemble framework, which integrates three complementary sub-networks: a spatio-linguistic model, a spatio-temporal model, and a tempo-linguistic model. Each sub-network specializes in a unique pair of dimensions, contributing distinct insights to the final prediction, akin to a collaborative team with diverse expertise. ToS has been benchmarked against state-of-the-art audio-visual models for 3D SELD on the DCASE2025 Task 3 Stereo SELD development set, consistently outperforming existing methods across key metrics. Future work will extend this proof of concept by strengthening the specialists with appropriate tasks, training, and pre-training curricula.",
    "paper_abstract_zh": "视频中的声音事件定位与检测及距离估计（3D SELD）涉及在每个时间帧识别活动的声音事件，同时估计它们的空间坐标。这种多模态任务需要在语义、空间和时间维度上进行联合推理，这是单一模型通常难以有效解决的挑战。为此，我们引入了专家团队（ToS）集成框架，该框架集成了三个互补的子网络：空间-语言模型、空间-时间模型和时间-语言模型。每个子网络专注于一个独特的维度对，为最终预测提供不同的见解，类似于具有多样化专业知识的协作团队。ToS已在DCASE2025任务3立体声SELD开发集上针对最先进的音频-视觉模型进行了3D SELD基准测试，并在关键指标上持续优于现有方法。未来的工作将通过适当的任务、训练和预训练课程来加强这些专家，从而扩展这个概念验证。",
    "subjects": [
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)",
      "Image and Video Processing (eess.IV)",
      "Sound (cs.SD)",
      "Signal Processing (eess.SP)"
    ],
    "update_time": "2026-01-27",
    "paper_authors": "Davide Berghi, Philip J. B. Jackson",
    "topic": [
      "Audio Classification",
      "Other"
    ],
    "category": [
      "Other"
    ]
  },
  {
    "paper_title": "End-to-End Joint ASR and Speaker Role Diarization with Child-Adult Interactions",
    "paper_title_zh": "端到端联合ASR和儿童-成人互动中的说话人角色分离",
    "paper_id": "2601.17640",
    "paper_abstract": "Accurate transcription and speaker diarization of child-adult spoken interactions are crucial for developmental and clinical research. However, manual annotation is time-consuming and challenging to scale. Existing automated systems typically rely on cascaded speaker diarization and speech recognition pipelines, which can lead to error propagation. This paper presents a unified end-to-end framework that extends the Whisper encoder-decoder architecture to jointly model ASR and child-adult speaker role diarization. The proposed approach integrates: (i) a serialized output training scheme that emits speaker tags and start/end timestamps, (ii) a lightweight frame-level diarization head that enhances speaker-discriminative encoder representations, (iii) diarization-guided silence suppression for improved temporal precision, and (iv) a state-machine-based forced decoding procedure that guarantees structurally valid outputs. Comprehensive evaluations on two datasets demonstrate consistent and substantial improvements over two cascaded baselines, achieving lower multi-talker word error rates and demonstrating competitive diarization accuracy across both Whisper-small and Whisper-large models. These findings highlight the effectiveness and practical utility of the proposed joint modeling framework for generating reliable, speaker-attributed transcripts of child-adult interactions at scale. The code and model weights are publicly available",
    "paper_abstract_zh": "准确的儿童-成人口语互动转录和说话人分离对发展和临床研究至关重要。然而，手动标注耗时且难以扩展。现有的自动化系统通常依赖于级联的说话人分离和语音识别流水线，这可能导致错误传播。本文提出了一个统一的端到端框架，扩展了Whisper编码器-解码器架构，以联合建模ASR和儿童-成人说话人角色分离。所提出的方法包括：(i)一种序列化输出训练方案，输出说话人标签和开始/结束时间戳，(ii)一个轻量级的帧级别分离头，增强说话人区分性编码器表示，(iii)分离引导的静音抑制，以提高时间精度，(iv)基于状态机的强制解码过程，确保结构有效的输出。在两个数据集上的全面评估表明，与两个级联基线相比，实现了持续且显著的改进，获得了更低的多说话人词错误率，并在Whisper-small和Whisper-large模型上展示了具有竞争力的分离准确性。这些发现强调了所提出的联合建模框架在生成可靠的、具有说话人属性的儿童-成人互动大规模转录方面的有效性和实用性。代码和模型权重已公开提供。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "update_time": "2026-01-27",
    "paper_authors": "Anfeng Xu, Tiantian Feng, Somer Bishop, Catherine Lord, Shrikanth Narayanan",
    "topic": [
      "Speech Recognition",
      "Audio Representation Learning"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Speech Emotion Recognition with ASR Integration",
    "paper_title_zh": "结合ASR的语音情感识别",
    "paper_id": "2601.17901",
    "paper_abstract": "Speech Emotion Recognition (SER) plays a pivotal role in understanding human communication, enabling emotionally intelligent systems, and serving as a fundamental component in the development of Artificial General Intelligence (AGI). However, deploying SER in real-world, spontaneous, and low-resource scenarios remains a significant challenge due to the complexity of emotional expression and the limitations of current speech and language technologies. This thesis investigates the integration of Automatic Speech Recognition (ASR) into SER, with the goal of enhancing the robustness, scalability, and practical applicability of emotion recognition from spoken language.",
    "paper_abstract_zh": "语音情感识别(SER)在理解人类交流、实现情感智能系统以及作为通用人工智能(AGI)发展基础组件方面发挥着关键作用。然而，由于情感表达的复杂性以及当前语音和语言技术的局限性，在真实、自发性且资源有限的情况下部署SER仍然是一个重大挑战。本论文研究了将自动语音识别(ASR)集成到SER中，旨在提高从 spoken language 中情感识别的鲁棒性、可扩展性和实际适用性。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "update_time": "2026-01-27",
    "paper_authors": "Yuanchao Li",
    "topic": [
      "Speech Recognition",
      "Audio Classification"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "AmbER$^2$: Dual Ambiguity-Aware Emotion Recognition Applied to Speech and Text",
    "paper_title_zh": "AmbER²：应用于语音和文本的双重歧义感知情感识别",
    "paper_id": "2601.18010",
    "paper_abstract": "Emotion recognition is inherently ambiguous, with uncertainty arising both from rater disagreement and from discrepancies across modalities such as speech and text. There is growing interest in modeling rater ambiguity using label distributions. However, modality ambiguity remains underexplored, and multimodal approaches often rely on simple feature fusion without explicitly addressing conflicts between modalities. In this work, we propose AmbER$^2$, a dual ambiguity-aware framework that simultaneously models rater-level and modality-level ambiguity through a teacher-student architecture with a distribution-wise training objective. Evaluations on IEMOCAP and MSP-Podcast show that AmbER$^2$ consistently improves distributional fidelity over conventional cross-entropy baselines and achieves performance competitive with, or superior to, recent state-of-the-art systems. For example, on IEMOCAP, AmbER$^2$ achieves relative improvements of 20.3% on Bhattacharyya coefficient (0.83 vs. 0.69), 13.6% on R$^2$ (0.67 vs. 0.59), 3.8% on accuracy (0.683 vs. 0.658), and 4.5% on F1 (0.675 vs. 0.646). Further analysis across ambiguity levels shows that explicitly modeling ambiguity is particularly beneficial for highly uncertain samples. These findings highlight the importance of jointly addressing rater and modality ambiguity when building robust emotion recognition systems.",
    "paper_abstract_zh": "情感识别本质上具有歧义性，这种歧义既来自评分者之间的意见不一致，也来自语音和文本等多模态之间的差异。目前，使用标签分布来建模评分者歧义正受到越来越多的关注。然而，模态歧义仍未得到充分探索，多模态方法通常依赖于简单的特征融合，而没有明确解决模态之间的冲突。在这项工作中，我们提出了AmbER²，一个双重歧义感知框架，通过具有分布级训练目标的教师-学生架构，同时建模评分者级和模态级的歧义。在IEMOCAP和MSP-Podcast上的评估表明，AmbER²在分布保真度上始终优于传统的交叉熵基线，并实现了与近期最先进系统相当或更优的性能。例如，在IEMOCAP上，AmbER²在Bhattacharyya系数上实现了20.3%的相对改进（0.83对比0.69），在R²上实现了13.6%的改进（0.67对比0.59），在准确率上实现了3.8%的改进（0.683对比0.658），在F1分数上实现了4.5%的改进（0.675对比0.646）。跨歧义级别的进一步分析表明，明确建模歧义对高度不确定的样本特别有益。这些发现强调了在构建稳健的情感识别系统时，联合解决评分者和模态歧义的重要性。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "update_time": "2026-01-27",
    "paper_authors": "Jingyao Wu, Grace Lin, Yinuo Song, Rosalind Picard",
    "topic": [
      "Audio Classification",
      "Speech Recognition"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "SpatialEmb: Extract and Encode Spatial Information for 1-Stage Multi-channel Multi-speaker ASR on Arbitrary Microphone Arrays",
    "paper_title_zh": "SpatialEmb: 提取并编码任意麦克风阵列上单阶段多通道多说话人自动语音识别的空间信息",
    "paper_id": "2601.18037",
    "paper_abstract": "Spatial information is a critical clue for multi-channel multi-speaker target speech recognition. Most state-of-the-art multi-channel Automatic Speech Recognition (ASR) systems extract spatial features only during the speech separation stage, followed by standard single-channel ASR on the separated speech. This approach results in an inefficient, lengthy pipeline and sub-optimal ASR performance due to the accumulated errors from preprocessing modules. Furthermore, most spatial feature extraction methods depend on the knowledge of speaker positions and microphone topology, making the systems reliant on specific settings and challenging to adapt to new equipment. In this work, we propose a solution to these issues with a lightweight embedding module named SpatialEmb, which extracts and encodes spatial information directly for the ASR model, supporting both fixed and arbitrary microphone topology. We conduct comprehensive experiments on AliMeeting, a real meeting corpus, to determine the optimal model design for SpatialEmb in terms of both performance and efficiency. Our best model trained with 105 hours Train-Ali-far achieves 17.04% and 20.32% character error rates (CER) on the Eval and Test sets, establishing a new state-of-the-art result with the same training data.",
    "paper_abstract_zh": "空间信息是多通道多说话人目标语音识别的关键线索。最先进的多通道自动语音识别(ASR)系统通常仅在语音分离阶段提取空间特征，然后在分离后的语音上进行标准的单通道ASR处理。这种方法导致效率低下、流程冗长，并且由于预处理模块累积的误差，ASR性能次优。此外，大多数空间特征提取方法依赖于说话人位置和麦克风拓扑的知识，这使得系统依赖于特定设置，难以适应新设备。在这项工作中，我们提出了一种名为SpatialEmb的轻量级嵌入模块来解决这些问题，该模块直接为ASR模型提取和编码空间信息，支持固定和任意麦克风拓扑。我们在AliMeeting（一个真实的会议语料库）上进行了全面的实验，以确定SpatialEmb在性能和效率方面的最佳模型设计。我们使用105小时的Train-Ali-far数据训练的最佳模型在Eval和Test集上分别实现了17.04%和20.32%的字符错误率(CER)，使用相同的训练数据建立了新的最先进结果。",
    "subjects": [
      "Artificial Intelligence (cs.AI)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2026-01-27",
    "paper_authors": "Yiwen Shao, Yong Xu, Sanjeev Khudanpur, Dong Yu",
    "topic": [
      "Speech Recognition",
      "Audio Representation Learning"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "OneVoice: One Model, Triple Scenarios-Towards Unified Zero-shot Voice Conversion",
    "paper_title_zh": "OneVoice：一个模型，三种场景——迈向统一的零样本语音转换",
    "paper_id": "2601.18094",
    "paper_abstract": "Recent progress of voice conversion~(VC) has achieved a new milestone in speaker cloning and linguistic preservation. But the field remains fragmented, relying on specialized models for linguistic-preserving, expressive, and singing scenarios. We propose OneVoice, a unified zero-shot framework capable of handling all three scenarios within a single model. OneVoice is built upon a continuous language model trained with VAE-free next-patch diffusion, ensuring high fidelity and efficient sequence modeling. Its core design for unification lies in a Mixture-of-Experts (MoE) designed to explicitly model shared conversion knowledge and scenario-specific expressivity. Expert selection is coordinated by a dual-path routing mechanism, including shared expert isolation and scenario-aware domain expert assignment with global-local cues. For precise conditioning, scenario-specific prosodic features are fused into each layer via a gated mechanism, allowing adaptive usage of prosody information. Furthermore, to enable the core idea and alleviate the imbalanced issue (abundant speech vs. scarce singing), we adopt a two-stage progressive training that includes foundational pre-training and scenario enhancement with LoRA-based domain experts. Experiments show that OneVoice matches or surpasses specialized models across all three scenarios, while verifying flexible control over scenarios and offering a fast decoding version as few as 2 steps. Code and model will be released soon.",
    "paper_abstract_zh": "语音转换(VC)的最新进展在说话人克隆和语言保留方面取得了新的里程碑。然而，该领域仍然分散，依赖于专门的语言保留、表达性和歌唱场景模型。我们提出了OneVoice，一个统一的零样本框架，能够在单个模型中处理所有三种场景。OneVoice基于一个使用VAF-free next-patch扩散训练的连续语言模型构建，确保高保真和高效的序列建模。其统一的核心设计在于一个混合专家(MoE)系统，旨在明确建模共享的转换知识和场景特定的表达性。专家选择由双路径路由机制协调，包括共享专家隔离和具有全局-局部提示的场景感知领域专家分配。为了精确的条件化，场景特定的韵律特征通过门控机制融合到每一层，允许韵律信息的自适应使用。此外，为实现核心思想并缓解不平衡问题（丰富的语音vs稀缺的歌唱），我们采用两阶段渐进式训练，包括基础预训练和基于LoRA的领域专家的场景增强。实验表明，OneVoice在所有三种场景下都能匹配或超越专门模型，同时验证了对场景的灵活控制，并提供了一个仅需2步的快速解码版本。代码和模型即将发布。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "update_time": "2026-01-27",
    "paper_authors": "Zhichao Wang, Tao Li, Wenshuo Ge, Zihao Cui, Shilei Zhang, Junlan Feng",
    "topic": [
      "Speech Synthesis",
      "Audio Representation Learning"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Efficient Rehearsal for Continual Learning in ASR via Singular Value Tuning",
    "paper_title_zh": "通过奇异值调优实现ASR持续学习的高效重演方法",
    "paper_id": "2601.18266",
    "paper_abstract": "Continual Learning (CL) in Automatic Speech Recognition (ASR) suffers from catastrophic forgetting when adapting to new tasks, domains, or speakers. A common strategy to mitigate this is to store a subset of past data in memory for rehearsal. However, rehearsal-based methods face key limitations: storing data is often costly, infeasible with pre-trained models, or restricted by privacy regulations. Running existing rehearsal-based methods with smaller memory sizes to alleviate these issues usually leads to degraded performance.\nWe propose a rehearsal-based CL method that remains effective even with minimal memory. It operates in two stages: first, fine-tuning on the new task; second, applying Singular Value Decomposition (SVD) to the changes in linear layers and, in a parameter-efficient manner, retraining only gating vectors on the singular values, which control to extent to which updates from the first stage are accepted, using rehearsal. We extensively test and analyze our method on two monolingual and two multilingual benchmarks. Our method reduces forgetting and outperforms state-of-the-art CL approaches for ASR, even when limited to a single utterance per previous task.",
    "paper_abstract_zh": "自动语音识别(ASR)中的持续学习(CL)在适应新任务、领域或说话者时会遭受灾难性遗忘。缓解这一问题的一种常见策略是在内存中存储过去数据的子集用于重演。然而，基于重演的方法面临关键限制：存储数据通常成本高昂，对于预训练模型不可行，或受到隐私法规的限制。使用较小的内存大小运行现有的基于重演的方法以缓解这些问题通常会导致性能下降。我们提出了一种基于重演的CL方法，即使在最小内存下也能保持有效。该方法分为两个阶段：首先在新任务上进行微调；其次，对线性层的变化应用奇异值分解(SVD)，并以参数高效的方式，仅重演基于奇异值的门控向量，这些向量控制第一阶段更新的接受程度。我们在两个单语和两个多语种基准上广泛测试并分析了我们的方法。我们的方法减少了遗忘，并优于最先进的ASR CL方法，即使每个先前任务仅限于一个语音片段。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "update_time": "2026-01-27",
    "paper_authors": "Steven Vander Eeckt, Hugo Van hamme",
    "topic": [
      "Speech Recognition",
      "Audio Representation Learning"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Noise-Robust Contrastive Learning with an MFCC-Conformer For Coronary Artery Disease Detection",
    "paper_title_zh": "基于MFCC-Conformer的噪声鲁棒对比学习用于冠状动脉疾病检测",
    "paper_id": "2601.18295",
    "paper_abstract": "Cardiovascular diseases (CVD) are the leading cause of death worldwide, with coronary artery disease (CAD) comprising the largest subcategory of CVDs. Recently, there has been increased focus on detecting CAD using phonocardiogram (PCG) signals, with high success in clinical environments with low noise and optimal sensor placement. Multichannel techniques have been found to be more robust to noise; however, achieving robust performance on real-world data remains a challenge. This work utilises a novel multichannel energy-based noisy-segment rejection algorithm, using heart and noise-reference microphones, to discard audio segments with large amounts of nonstationary noise before training a deep learning classifier. This conformer-based classifier takes mel-frequency cepstral coefficients (MFCCs) from multiple channels, further helping improve the model's noise robustness. The proposed method achieved 78.4% accuracy and 78.2% balanced accuracy on 297 subjects, representing improvements of 4.1% and 4.3%, respectively, compared to training without noisy-segment rejection.",
    "paper_abstract_zh": "心血管疾病（CVD）是全球死亡的主要原因，其中冠状动脉疾病（CAD）是CVD的最大亚类。最近，使用心音图（PCG）信号检测CAD的研究日益增多，在低噪声和最佳传感器放置的临床环境中取得了很高的成功率。多通道技术被发现对噪声具有更强的鲁棒性；然而，在真实世界数据上实现鲁棒性能仍然是一个挑战。这项工作利用了一种新颖的多通道能量基于噪声段拒绝算法，通过心脏和噪声参考麦克风，在训练深度学习分类器之前丢弃含有大量非平稳噪声的音频段。这种基于Conformer的分类器从多通道中提取梅尔频率倒谱系数（MFCC），进一步提高了模型的噪声鲁棒性。该方法在297名受试者上达到了78.4%的准确率和78.2%的平衡准确率，相比不进行噪声段拒绝的训练，分别提高了4.1%和4.3%。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "update_time": "2026-01-27",
    "paper_authors": "Milan Marocchi, Matthew Fynn, Yue Rong",
    "topic": [
      "Audio Classification",
      "Audio Representation Learning"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Residual Learning for Neural Ambisonics Encoders",
    "paper_title_zh": "用于神经Ambisonics编码器的残差学习",
    "paper_id": "2601.18322",
    "paper_abstract": "Emerging wearable devices such as smartglasses and extended reality headsets demand high-quality spatial audio capture from compact, head-worn microphone arrays. Ambisonics provides a device-agnostic spatial audio representation by mapping array signals to spherical harmonic (SH) coefficients. In practice, however, accurate encoding remains challenging. While traditional linear encoders are signal-independent and robust, they amplify low-frequency noise and suffer from high-frequency spatial aliasing. On the other hand, neural network approaches can outperform linear encoders but they often assume idealized microphones and may perform inconsistently in real-world scenarios. To leverage their complementary strengths, we introduce a residual-learning framework that refines a linear encoder with corrections from a neural network. Using measured array transfer functions from smartglasses, we compare a UNet-based encoder from the literature with a new recurrent attention model. Our analysis reveals that both neural encoders only consistently outperform the linear baseline when integrated within the residual learning framework. In the residual configuration, both neural models achieve consistent and significant improvements across all tested metrics for in-domain data and moderate gains for out-of-domain data. Yet, coherence analysis indicates that all neural encoder configurations continue to struggle with directionally accurate high-frequency encoding.",
    "paper_abstract_zh": "新兴的可穿戴设备，如智能眼镜和扩展现实头显，需要从紧凑的头部佩戴式麦克风阵列中捕获高质量的空间音频。Ambisonics通过将阵列信号映射到球谐(SH)系数，提供了一种与设备无关的空间音频表示。然而，在实践中，准确的编码仍然具有挑战性。虽然传统的线性编码器是与信号无关且鲁棒的，但它们会放大低频噪声并遭受高频空间混叠。另一方面，神经网络方法可以优于线性编码器，但它们通常假设理想化的麦克风，并且在真实场景中可能表现不一致。为了利用它们的互补优势，我们引入了一个残差学习框架，通过神经网络的校正来优化线性编码器。使用来自智能眼镜的测量阵列传递函数，我们将文献中的基于UNet的编码器与一个新的循环注意力模型进行比较。我们的分析表明，只有当集成在残差学习框架中时，两种神经编码器才能始终优于线性基线。在残差配置下，两种神经模型在所有测试的域内数据指标上实现了一致且显著的改进，在域外数据上获得了适度的增益。然而，相干性分析表明，所有神经编码器配置在方向准确的高频编码方面仍然存在困难。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "update_time": "2026-01-27",
    "paper_authors": "Thomas Deppisch, Yang Gao, Manan Mittal, Benjamin Stahl, Christoph Hold, David Alon, Zamir Ben-Hur",
    "topic": [
      "Audio Representation Learning",
      "Speech Enhancement"
    ],
    "category": [
      "Other"
    ]
  },
  {
    "paper_title": "Noise-Robust AV-ASR Using Visual Features Both in the Whisper Encoder and Decoder",
    "paper_title_zh": "在Whisper编码器和解码器中同时使用视觉特征的抗噪声AV-ASR方法",
    "paper_id": "2601.18396",
    "paper_abstract": "In audiovisual automatic speech recognition (AV-ASR) systems, information fusion of visual features in a pre-trained ASR has been proven as a promising method to improve noise robustness. In this work, based on the prominent Whisper ASR, first, we propose a simple and effective visual fusion method -- use of visual features both in encoder and decoder (dual-use) -- to learn the audiovisual interactions in the encoder and to weigh modalities in the decoder. Second, we compare visual fusion methods in Whisper models of various sizes. Our proposed dual-use method shows consistent noise robustness improvement, e.g., a 35% relative improvement (WER: 4.41% vs. 6.83%) based on Whisper small, and a 57% relative improvement (WER: 4.07% vs. 9.53%) based on Whisper medium, compared to typical reference middle fusion in babble noise with a signal-to-noise ratio (SNR) of 0dB. Third, we conduct ablation studies examining the impact of various module designs and fusion options. Fine-tuned on 1929 hours of audiovisual data, our dual-use method using Whisper medium achieves 4.08% (MUSAN babble noise) and 4.43% (NoiseX babble noise) average WER across various SNRs, thereby establishing a new state-of-the-art in noisy conditions on the LRS3 AV-ASR benchmark. Our code is at this https URL",
    "paper_abstract_zh": "在视听自动语音识别(AV-ASR)系统中，将视觉特征信息融合到预训练的ASR模型中已被证明是一种提高噪声鲁棒性的有效方法。在这项工作中，基于著名的Whisper ASR模型，首先，我们提出了一种简单而有效的视觉融合方法——在编码器和解码器中同时使用视觉特征(双重使用)——以在编码器中学习视听交互，并在解码器中对不同模态进行加权。其次，我们比较了不同大小的Whisper模型中的视觉融合方法。我们提出的双重使用方法在噪声鲁棒性上表现出一致的改进，例如，在0dB信噪比(SNR)的 babble 噪声条件下，基于Whisper small模型相对改进35%(WER: 4.41% vs. 6.83%)，基于Whisper medium模型相对改进57%(WER: 4.07% vs. 9.53%)，与典型的中间融合方法相比。第三，我们进行了消融研究，考察了各种模块设计和融合选项的影响。在1929小时的视听数据上微调后，我们使用Whisper medium的双重使用方法在各种SNR条件下平均WER达到4.08%(MUSAN babble噪声)和4.43%(NoiseX babble噪声)，从而在LRS3 AV-ASR基准测试的噪声条件下建立了新的最先进水平。我们的代码位于此https URL",
    "subjects": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)",
      "Computation and Language (cs.CL)"
    ],
    "update_time": "2026-01-27",
    "paper_authors": "Zhengyang Li, Thomas Graave, Björn Möller, Zehang Wu, Matthias Franz, Tim Fingscheidt",
    "topic": [
      "Speech Recognition",
      "Audio Representation Learning"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Audio Inpainting in Time-Frequency Domain with Phase-Aware Prior",
    "paper_title_zh": "基于相位感知先验的时频域音频修复",
    "paper_id": "2601.18535",
    "paper_abstract": "The so-called audio inpainting problem in the time domain refers to estimating missing segments of samples within a signal. Over the years, several methods have been developed for such type of audio inpainting. In contrast to this case, a time-frequency variant of inpainting appeared in the literature, where the challenge is to reconstruct missing spectrogram columns with reliable information. We propose a method to address this time-frequency audio inpainting problem. Our approach is based on the recently introduced phase-aware signal prior that exploits an estimate of the instantaneous frequency. An optimization problem is formulated and solved using the generalized Chambolle-Pock algorithm. The proposed method is evaluated both objectively and subjectively against other time-frequency inpainting methods, specifically a deep-prior neural network and the autoregression-based approach known as Janssen-TF. Our proposed approach surpassed these methods in the objective evaluation as well as in the conducted listening test. Moreover, this outcome is achieved with a substantially reduced computational requirement compared to alternative methods.",
    "paper_abstract_zh": "所谓的时域音频修复问题是指估计信号中缺失的样本段。多年来，已经开发出几种用于此类音频修复的方法。与此相反，文献中出现了一种时频变体的修复方法，其挑战在于用可靠的信息重建缺失的频谱图列。我们提出了一种解决时频音频修复问题的方法。我们的方法基于最近引入的相位感知信号先验，该先验利用了瞬时频率的估计。我们构建了一个优化问题，并使用广义Chambolle-Pock算法进行求解。我们提出的方法在客观和主观评估中与其他时频修复方法进行了比较，特别是基于深度先验的神经网络和称为Janssen-TF的自回归方法。在客观评估和进行的听力测试中，我们提出的方法都优于这些方法。此外，与替代方法相比，这一成果是在计算需求大幅减少的情况下实现的。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "update_time": "2026-01-27",
    "paper_authors": "Peter Balušík, Pavel Rajmic",
    "topic": [
      "Audio Representation Learning",
      "Other"
    ],
    "category": [
      "Other"
    ]
  },
  {
    "paper_title": "Learning to Discover: A Generalized Framework for Raga Identification without Forgetting",
    "paper_title_zh": "学习发现：一种不遗忘的通用拉加识别框架",
    "paper_id": "2601.18766",
    "paper_abstract": "Raga identification in Indian Art Music (IAM) remains challenging due to the presence of numerous rarely performed Ragas that are not represented in available training datasets. Traditional classification models struggle in this setting, as they assume a closed set of known categories and therefore fail to recognise or meaningfully group previously unseen Ragas. Recent works have tried categorizing unseen Ragas, but they run into a problem of catastrophic forgetting, where the knowledge of previously seen Ragas is diminished. To address this problem, we adopt a unified learning framework that leverages both labeled and unlabeled audio, enabling the model to discover coherent categories corresponding to the unseen Ragas, while retaining the knowledge of previously known ones. We test our model on benchmark Raga Identification datasets and demonstrate its performance in categorizing previously seen, unseen, and all Raga classes. The proposed approach surpasses the previous NCD-based pipeline even in discovering the unseen Raga categories, offering new insights into representation learning for IAM tasks.",
    "paper_abstract_zh": "在印度艺术音乐(IAM)中，拉加识别仍然具有挑战性，因为存在大量在可用训练数据集中未体现的极少表演的拉加。传统分类模型在这种设置下表现不佳，因为它们假设有一组已知的封闭类别，因此无法识别或有意义地分组以前未见过的拉加。最近的研究尝试对未见过的拉加进行分类，但遇到了灾难性遗忘的问题，即对先前见过拉加的知识会减弱。为了解决这个问题，我们采用了一个统一的学习框架，利用有标签和无标签的音频，使模型能够发现与未见拉加相对应的连贯类别，同时保留对已知拉加的知识。我们在基准拉加识别数据集上测试了我们的模型，并展示了其在分类已见、未见和所有拉加类别方面的性能。所提出的方法甚至在发现未见拉加类别方面也优于之前的基于NCD的流水线，为IAM任务的表示学习提供了新的见解。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)",
      "Machine Learning (cs.LG)"
    ],
    "update_time": "2026-01-27",
    "paper_authors": "Parampreet Singh, Somya Kumar, Chaitanya Shailendra Nitawe, Vipul Arora",
    "topic": [
      "Audio Classification",
      "Music Information Retrieval",
      "Audio Representation Learning"
    ],
    "category": [
      "Music"
    ]
  },
  {
    "paper_title": "SonoEdit: Null-Space Constrained Knowledge Editing for Pronunciation Correction in LLM-Based TTS",
    "paper_title_zh": "SonoEdit：基于LLM的TTS中用于发音校正的零空间约束知识编辑",
    "paper_id": "2601.17086",
    "paper_abstract": "Neural text-to-speech (TTS) systems systematically mispronounce low-resource proper nouns, particularly non-English names, brands, and geographic locations, due to their underrepresentation in predominantly English training corpora. Existing solutions typically rely on expensive multilingual data collection, supervised finetuning, or manual phonetic annotation, which limits the deployment of TTS systems in linguistically diverse settings. We introduce SonoEdit, a model editing technique that surgically corrects pronunciation errors in pre-trained TTS models without retraining. Instead of costly finetuning or explicit phoneme injection, we propose a parsimonious alternative based on Null-Space Pronunciation Editing, which performs a single-shot parameter update to modify the pronunciation of specific words while provably preserving all other model behavior. We first adapt Acoustic Causal Tracing to identify the Transformer layers responsible for text-to-pronunciation mapping. We then apply Null-Space Constrained Editing to compute a closed-form weight update that corrects the target pronunciation while remaining mathematically orthogonal to the subspace governing general speech generation. This constrained update steers the model's acoustic output toward a desired pronunciation exemplar while guaranteeing zero first-order change on a preserved speech corpus.",
    "paper_abstract_zh": "神经文本转语音（TTS）系统在处理资源匮乏的专有名词时，尤其是非英语姓名、品牌和地理位置，会系统地出现发音错误，这主要是因为这些内容在以英语为主的训练语料库中代表性不足。现有解决方案通常依赖于昂贵的多语言数据收集、监督微调或手动音标标注，这限制了TTS系统在语言多样化环境中的部署。我们引入了SonoEdit，一种模型编辑技术，可以在不重新训练的情况下，精确地纠正预训练TTS模型中的发音错误。我们提出了一种基于零空间发音编辑的简约替代方案，它通过单次参数更新来修改特定单词的发音，同时保证保留模型的所有其他行为。我们首先采用声学因果追踪来识别负责文本到音素映射的Transformer层。然后，我们应用零空间约束编辑来计算闭式权重更新，该更新可以校正目标发音，同时在数学上保持与控制通用语音生成的子空间正交。这种约束更新将模型的声学输出引导向期望的发音示例，同时保证在保留的语音语料库上零一阶变化。",
    "subjects": [
      "Artificial Intelligence (cs.AI)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2026-01-27",
    "paper_authors": "Ayush Pratap Singh, Harshit Singh, Nityanand Mathur, Akshat Mandloi, Sudarshan Kamath",
    "topic": [
      "Speech Synthesis",
      "Audio Representation Learning"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Sink or SWIM: Tackling Real-Time ASR at Scale",
    "paper_title_zh": "沉浮还是SWIM：规模化实时自动语音识别的解决方案",
    "paper_id": "2601.17097",
    "paper_abstract": "Real-time automatic speech recognition systems are increasingly integrated into interactive applications, from voice assistants to live transcription services. However, scaling these systems to support multiple concurrent clients while maintaining low latency and high accuracy remains a major challenge. In this work, we present SWIM, a novel real-time ASR system built on top of OpenAI's Whisper model that enables true model-level parallelization for scalable, multilingual transcription. SWIM supports multiple concurrent audio streams without modifying the underlying model. It introduces a buffer merging strategy that maintains transcription fidelity while ensuring efficient resource usage. We evaluate SWIM in multi-client settings -- scaling up to 20 concurrent users -- and show that it delivers accurate real-time transcriptions in English, Italian, and Spanish, while maintaining low latency and high throughput. While Whisper-Streaming achieves a word error rate of approximately 8.2% with an average delay of approximately 3.4 s in a single-client, English-only setting, SWIM extends this capability to multilingual, multi-client environments. It maintains comparable accuracy with significantly lower delay -- around 2.4 s with 5 clients -- and continues to scale effectively up to 20 concurrent clients without degrading transcription quality and increasing overall throughput. Our approach advances scalable ASR by improving robustness and efficiency in dynamic, multi-user environments.",
    "paper_abstract_zh": "实时自动语音识别系统越来越多地集成到交互式应用中，从语音助手到实时转录服务。然而，在保持低延迟和高准确性的同时扩展这些系统以支持多个并发客户端仍然是一个重大挑战。在这项工作中，我们提出了SWIM，一个构建在OpenAI的Whisper模型之上的新型实时ASR系统，实现了可扩展的多语言转录的真正模型级并行化。SWIM无需修改底层模型即可支持多个并发音频流。它引入了一种缓冲区合并策略，在确保高效资源利用的同时保持转录保真度。我们在多客户端环境中评估了SWIM——扩展到20个并发用户——并证明它能够提供英语、意大利语和西班牙语的准确实时转录，同时保持低延迟和高吞吐量。虽然Whisper-Streaming在单客户端、仅英语设置中实现了约8.2%的词错误率和约3.4秒的平均延迟，但SWIM将这一能力扩展到多语言、多客户端环境。它保持了相当的准确性，同时显著降低了延迟——5个客户端时约为2.4秒——并且能够有效扩展到20个并发客户端，而不会降低转录质量并增加整体吞吐量。我们的方法通过在动态、多用户环境中提高鲁棒性和效率，推动了可扩展ASR的发展。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "update_time": "2026-01-27",
    "paper_authors": "Federico Bruzzone, Walter Cazzola, Matteo Brancaleoni, Dario Pellegrino",
    "topic": [
      "Speech Recognition"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Window Size Versus Accuracy Experiments in Voice Activity Detectors",
    "paper_title_zh": "语音活动检测器中窗口大小与准确率实验",
    "paper_id": "2601.17270",
    "paper_abstract": "Voice activity detection (VAD) plays a vital role in enabling applications such as speech recognition. We analyze the impact of window size on the accuracy of three VAD algorithms: Silero, WebRTC, and Root Mean Square (RMS) across a set of diverse real-world digital audio streams. We additionally explore the use of hysteresis on top of each VAD output. Our results offer practical references for optimizing VAD systems. Silero significantly outperforms WebRTC and RMS, and hysteresis provides a benefit for WebRTC.",
    "paper_abstract_zh": "语音活动检测(VAD)在实现语音识别等应用中起着至关重要的作用。我们分析了窗口大小对三种VAD算法(Silero、WebRTC和均方根(RMS))在多样化真实世界数字音频流上准确率的影响。此外，我们还探索了在每个VAD输出上使用滞后的效果。我们的结果为优化VAD系统提供了实用的参考。Silero的性能显著优于WebRTC和RMS，而滞后处理对WebRTC有积极影响。",
    "subjects": [
      "Sound (cs.SD)",
      "Computation and Language (cs.CL)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2026-01-27",
    "paper_authors": "Max McKinnon, Samir Khaki, Chandan KA Reddy, William Huang",
    "topic": [
      "Speech Recognition",
      "Audio Classification"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "EuleroDec: A Complex-Valued RVQ-VAE for Efficient and Robust Audio Coding",
    "paper_title_zh": "EuleroDec: 一种用于高效鲁棒音频编码的复值RVQ-VAE",
    "paper_id": "2601.17517",
    "paper_abstract": "Audio codecs power discrete music generative modelling, music streaming, and immersive media by shrinking PCM audio to bandwidth-friendly bitrates. Recent works have gravitated towards processing in the spectral domain; however, spectrogram domains typically struggle with phase modeling, which is naturally complex-valued. Most frequency-domain neural codecs either disregard phase information or encode it as two separate real-valued channels, limiting spatial fidelity. This entails the need to introduce adversarial discriminators at the expense of convergence speed and training stability to compensate for the inadequate representation power of the audio signal. In this work we introduce an end-to-end complex-valued RVQ-VAE audio codec that preserves magnitude-phase coupling across the entire analysis-quantization-synthesis pipeline and removes adversarial discriminators and diffusion post-filters. Without GANs or diffusion, we match or surpass much longer-trained baselines in-domain and reach SOTA out-of-domain performance on phase coherence and waveform fidelity. Compared to standard baselines that train for hundreds of thousands of steps, our model, which reduces the training budget by an order of magnitude, is markedly more compute-efficient while preserving high perceptual quality.",
    "paper_abstract_zh": "音频编解码器通过将PCM音频压缩为带宽友好的比特率，为离散音乐生成建模、音乐流媒体和沉浸式媒体提供支持。近期研究倾向于在频谱域进行处理；然而，频谱域通常难以处理自然为复数值的相位建模问题。大多数频域神经编解码器要么忽略相位信息，要么将其编码为两个独立的实值通道，这限制了空间保真度。这需要引入对抗性判别器，以收敛速度和训练稳定性为代价，来补偿音频信号表示能力的不足。在本工作中，我们引入了一种端到端的复值RVQ-VAE音频编解码器，它在整个分析-量化-合成流程中保持幅度-相位耦合，并移除了对抗性判别器和扩散后滤波器。在没有GAN或扩散的情况下，我们在域内匹配或超越了训练时间长得多的基线模型，并在相位相干性和波形保真度方面达到了域外最先进的性能。与训练数十万步的标准基线相比，我们的模型将训练预算减少了一个数量级，同时保持了高感知质量，计算效率显著提高。",
    "subjects": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2026-01-27",
    "paper_authors": "Luca Cerovaz, Michele Mancusi, Emanuele Rodolà",
    "topic": [
      "Audio Codec",
      "Audio Representation Learning"
    ],
    "category": [
      "Music"
    ]
  },
  {
    "paper_title": "Home Health System Deployment Experience for Geriatric Care Remote Monitoring",
    "paper_title_zh": "老年护理远程监测的家庭健康系统部署经验",
    "paper_id": "2601.17608",
    "paper_abstract": "To support aging-in-place, adult children often provide care to their aging parents from a distance. These informal caregivers desire plug-and-play remote care solutions for privacy-preserving continuous monitoring that enabling real-time activity monitoring and intuitive, actionable information. This short paper presents insights from three iterations of deployment experience for remote monitoring system and the iterative improvement in hardware, modeling, and user interface guided by the Geriatric 4Ms framework (matters most, mentation, mobility, and medication). An LLM-assisted solution is developed to balance user experience (privacy-preserving, plug-and-play) and system performance.",
    "paper_abstract_zh": "为支持居家养老，成年子女常常从远处照顾年迈的父母。这些非正式护理者希望即插即用的远程护理解决方案，能够进行隐私保护的持续监测，实现实时活动监测并提供直观、可操作的信息。本文简述了远程监测系统三次迭代的部署经验，以及在老年4Ms框架（最重要事项、精神状态、活动能力和药物管理）指导下，硬件、建模和用户界面的迭代改进。开发了一种基于大语言模型的解决方案，以平衡用户体验（隐私保护、即插即用）和系统性能。",
    "subjects": [
      "Human-Computer Interaction (cs.HC)",
      "Systems and Control (eess.SY)",
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "update_time": "2026-01-27",
    "paper_authors": "Dong Yoon Lee, Alyssa Weakley, Hui Wei, Daniel Cardona, Shijia Pan",
    "topic": [
      "Other"
    ],
    "category": [
      "Other"
    ]
  },
  {
    "paper_title": "AVMeme Exam: A Multimodal Multilingual Multicultural Benchmark for LLMs' Contextual and Cultural Knowledge and Thinking",
    "paper_title_zh": "AVMeme考试：用于评估大语言模型在语境和文化知识及思维能力的多模态多语言多文化基准测试",
    "paper_id": "2601.17645",
    "paper_abstract": "Internet audio-visual clips convey meaning through time-varying sound and motion, which extend beyond what text alone can represent. To examine whether AI models can understand such signals in human cultural contexts, we introduce AVMeme Exam, a human-curated benchmark of over one thousand iconic Internet sounds and videos spanning speech, songs, music, and sound effects. Each meme is paired with a unique Q&A assessing levels of understanding from surface content to context and emotion to usage and world knowledge, along with metadata such as original year, transcript, summary, and sensitivity. We systematically evaluate state-of-the-art multimodal large language models (MLLMs) alongside human participants using this benchmark. Our results reveal a consistent limitation: current models perform poorly on textless music and sound effects, and struggle to think in context and in culture compared to surface content. These findings highlight a key gap in human-aligned multimodal intelligence and call for models that can perceive contextually and culturally beyond the surface of what they hear and see. Project page: this http URL",
    "paper_abstract_zh": "互联网视听内容通过随时间变化的声学和动态传递意义，这些意义超越了文本单独所能表达的范围。为了检验人工智能模型能否在人类文化背景下理解这些信号，我们引入了AVMeme考试，这是一个由人工策划的基准测试，包含一千多个标志性互联网声音和视频，涵盖语音、歌曲、音乐和音效。每个迷因都配有独特的问答，评估从表面内容到语境、情感、使用方式和世界知识等不同层次的理解，并附带原始年份、转录文本、摘要和敏感性等元数据。我们使用这一基准系统评估了最先进的多模态大语言模型（MLLMs）和人类参与者。结果显示，当前模型存在一个普遍局限：在无文本的音乐和音效方面表现不佳，与表面内容相比，在语境和文化思维方面存在困难。这些发现强调了人类对齐的多模态智能中的一个关键差距，呼吁开发能够超越所见所闻表面、进行语境和文化感知的模型。",
    "subjects": [
      "Multimedia (cs.MM)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)",
      "Computation and Language (cs.CL)"
    ],
    "update_time": "2026-01-27",
    "paper_authors": "Xilin Jiang, Qiaolin Wang, Junkai Wu, Xiaomin He, Zhongweiyang Xu, Yinghao Ma, Minshuo Piao, Kaiyi Yang, Xiuwen Zheng, Riki Shimizu, Yicong Chen, Arsalan Firoozi, Gavin Mischler, Sukru Samet Dindar, Richard Antonello, Linyang He, Tsun-An Hsieh, Xulin Fan, Yulun Wu, Yuesheng Ma, Chaitanya Amballa, Weixiong Chen, Jiarui Hai, Ruisi Li, Vishal Choudhari, Cong Han, Yinghao Aaron Li, Adeen Flinker, Mounya Elhilali, Emmanouil Benetos, Mark Hasegawa-Johnson, Romit Roy Choudhury, Nima Mesgarani",
    "topic": [
      "Audio Classification",
      "Music Information Retrieval",
      "Other"
    ],
    "category": [
      "Other"
    ]
  },
  {
    "paper_title": "BanglaRobustNet: A Hybrid Denoising-Attention Architecture for Robust Bangla Speech Recognition",
    "paper_title_zh": "BanglaRobustNet: 一种用于孟加拉语语音识别的混合降噪-注意力架构",
    "paper_id": "2601.17679",
    "paper_abstract": "Bangla, one of the most widely spoken languages, remains underrepresented in state-of-the-art automatic speech recognition (ASR) research, particularly under noisy and speaker-diverse conditions. This paper presents BanglaRobustNet, a hybrid denoising-attention framework built on Wav2Vec-BERT, designed to address these challenges. The architecture integrates a diffusion-based denoising module to suppress environmental noise while preserving Bangla-specific phonetic cues, and a contextual cross-attention module that conditions recognition on speaker embeddings for robustness across gender, age, and dialects. Trained end-to-end with a composite objective combining CTC loss, phonetic consistency, and speaker alignment, BanglaRobustNet achieves substantial reductions in word error rate (WER) and character error rate (CER) compared to Wav2Vec-BERT and Whisper baselines. Evaluations on Mozilla Common Voice Bangla and augmented noisy speech confirm the effectiveness of our approach, establishing BanglaRobustNet as a robust ASR system tailored to low-resource, noise-prone linguistic settings.",
    "paper_abstract_zh": "孟加拉语作为使用最广泛的语言之一，在先进的自动语音识别（ASR）研究中仍然代表性不足，特别是在嘈杂和说话人多样化的条件下。本文提出了BanglaRobustNet，一种基于Wav2Vec-BERT构建的混合降噪-注意力框架，旨在解决这些挑战。该架构集成了一个基于扩散的降噪模块，以抑制环境噪声同时保留孟加拉语特有的语音线索，以及一个上下文交叉注意力模块，该模块基于说话人嵌入进行识别条件处理，以在性别、年龄和方言方面实现鲁棒性。通过结合CTC损失、语音一致性和说话人对齐的复合目标进行端到端训练，BanglaRobustNet与Wav2Vec-BERT和Whisper基线相比，显著降低了词错误率（WER）和字符错误率（CER）。在Mozilla Common Voice孟加拉语和增强的嘈杂语音上的评估证实了我们方法的有效性，确立了BanglaRobustNet作为一种针对低资源、易受噪声影响的语言环境量身定制的鲁棒ASR系统。",
    "subjects": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "update_time": "2026-01-27",
    "paper_authors": "Md Sazzadul Islam Ridoy, Mubaswira Ibnat Zidney, Sumi Akter, Md. Aminur Rahman",
    "topic": [
      "Speech Recognition",
      "Speech Enhancement"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Segment Length Matters: A Study of Segment Lengths on Audio Fingerprinting Performance",
    "paper_title_zh": "分段长度很重要：分段长度对音频指纹识别性能的研究",
    "paper_id": "2601.17690",
    "paper_abstract": "Audio fingerprinting provides an identifiable representation of acoustic signals, which can be later used for identification and retrieval systems. To obtain a discriminative representation, the input audio is usually segmented into shorter time intervals, allowing local acoustic features to be extracted and analyzed. Modern neural approaches typically operate on short, fixed-duration audio segments, yet the choice of segment duration is often made heuristically and rarely examined in depth. In this paper, we study how segment length affects audio fingerprinting performance. We extend an existing neural fingerprinting architecture to adopt various segment lengths and evaluate retrieval accuracy across different segment lengths and query durations. Our results show that short segment lengths (0.5-second) generally achieve better performance. Moreover, we evaluate LLM capacity in recommending the best segment length, which shows that GPT-5-mini consistently gives the best suggestions across five considerations among three studied LLMs. Our findings provide practical guidance for selecting segment duration in large-scale neural audio retrieval systems.",
    "paper_abstract_zh": "音频指纹为声学信号提供了可识别的表示，可用于后续的识别和检索系统。为了获得具有区分度的表示，输入音频通常被分割成较短的时间间隔，以便提取和分析局部声学特征。现代神经方法通常在短时固定持续时间的音频片段上运行，但片段持续时间的选择通常是启发式的，很少被深入探讨。在本文中，我们研究了分段长度如何影响音频指纹识别性能。我们扩展了一个现有的神经指纹识别架构，以采用不同的分段长度，并评估了不同分段长度和查询持续时间下的检索准确性。我们的结果表明，较短的分段长度（0.5秒）通常能取得更好的性能。此外，我们评估了大型语言模型在推荐最佳分段长度方面的能力，结果显示在研究的三个大型语言模型中，GPT-5-mini在五个考量因素中始终给出最佳建议。我们的发现为大规模神经音频检索系统中选择分段持续时间提供了实用指导。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)",
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)",
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)"
    ],
    "update_time": "2026-01-27",
    "paper_authors": "Ziling Gong, Yunyan Ouyang, Iram Kamdar, Melody Ma, Hongjie Chen, Franck Dernoncourt, Ryan A. Rossi, Nesreen K. Ahmed",
    "topic": [
      "Audio Representation Learning",
      "Music Information Retrieval"
    ],
    "category": [
      "Music"
    ]
  },
  {
    "paper_title": "CaSNet: Compress-and-Send Network Based Multi-Device Speech Enhancement Model for Distributed Microphone Arrays",
    "paper_title_zh": "CaSNet: 基于压缩发送网络的分布式麦克风阵列多设备语音增强模型",
    "paper_id": "2601.17711",
    "paper_abstract": "Distributed microphone array (DMA) is a promising next-generation platform for speech interaction, where speech enhancement (SE) is still required to improve the speech quality in noisy cases. Existing SE methods usually first gather raw waveforms at a fusion center (FC) from all devices and then design a multi-microphone model, causing high bandwidth and energy costs. In this work, we propose a \\emph{Compress-and-Send Network (CaSNet)} for resource-constrained DMAs, where one microphone serves as the FC and reference. Each of other devices encodes the measured raw data into a feature matrix, which is then compressed by singular value decomposition (SVD) to produce a more compact representation. The received features at the FC are aligned via cross window query with respect to the reference, followed by neural decoding to yield spatially coherent enhanced speech. Experiments on multiple datasets show that the proposed CaSNet can save the data amount with a negligible impact on the performance compared to the uncompressed case. The reproducible code is available at this https URL.",
    "paper_abstract_zh": "分布式麦克风阵列(DMA)是有前景的下一代语音交互平台，但在噪声情况下仍需要语音增强(SE)来提高语音质量。现有的SE方法通常首先将所有设备的原始波形收集到融合中心(FC)，然后设计多麦克风模型，导致高带宽和能耗成本。在这项工作中，我们提出了一种资源受限DMA的压缩发送网络(CaSNet)，其中一个麦克风作为FC和参考。其他每个设备将测量的原始数据编码为特征矩阵，然后通过奇异值分解(SVD)压缩以产生更紧凑的表示。FC接收到的特征通过相对于参考的跨窗口查询进行对齐，然后通过神经解码产生空间连贯的增强语音。在多个数据集上的实验表明，与未压缩情况相比，所提出的CaSNet可以在性能影响可忽略不计的情况下节省数据量。可复现代码可通过此https URL获取。",
    "subjects": [
      "Artificial Intelligence (cs.AI)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2026-01-27",
    "paper_authors": "Chengqian Jiang, Jie Zhang, Haoyin Yan",
    "topic": [
      "Speech Enhancement",
      "Audio Representation Learning"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "dLLM-ASR: A Faster Diffusion LLM-based Framework for Speech Recognition",
    "paper_title_zh": "dLLM-ASR：一种基于扩散大语言模型的更快语音识别框架",
    "paper_id": "2601.17902",
    "paper_abstract": "Automatic speech recognition (ASR) systems based on large language models (LLMs) achieve superior performance by leveraging pretrained LLMs as decoders, but their token-by-token generation mechanism leads to inference latency that grows linearly with sequence length. Meanwhile, discrete diffusion large language models (dLLMs) offer a promising alternative, enabling high-quality parallel sequence generation with pretrained decoders. However, directly applying native text-oriented dLLMs to ASR leads to a fundamental mismatch between open-ended text generation and the acoustically conditioned transcription paradigm required by ASR. As a result, it introduces unnecessary difficulty and computational redundancy, such as denoising from pure noise, inflexible generation lengths, and fixed denoising steps. We propose dLLM-ASR, an efficient dLLM-based ASR framework that formulates dLLM's decoding as a prior-guided and adaptive denoising process. It leverages an ASR prior to initialize the denoising process and provide an anchor for sequence length. Building upon this prior, length-adaptive pruning dynamically removes redundant tokens, while confidence-based denoising allows converged tokens to exit the denoising loop early, enabling token-level adaptive computation. Experiments demonstrate that dLLM-ASR achieves recognition accuracy comparable to autoregressive LLM-based ASR systems and delivers a 4.44$\\times$ inference speedup, establishing a practical and efficient paradigm for ASR.",
    "paper_abstract_zh": "基于大语言模型(LLM)的自动语音识别(ASR)系统通过将预训练的LLM作为解码器来利用，从而实现卓越的性能，但其逐个标记的生成机制导致推理延迟随序列长度线性增长。同时，离散扩散大语言模型(dLLMs)提供了一种有前景的替代方案，能够使用预训练解码器实现高质量的并行序列生成。然而，将原生面向文本的dLLMs直接应用于ASR会导致开放式文本生成与ASR所需的声学条件转录范式之间的根本不匹配。因此，它引入了不必要的困难和计算冗余，例如从纯噪声去噪、生成长度不灵活以及固定的去噪步骤。我们提出了dLLM-ASR，一种高效的基于dLLM的ASR框架，将dLLM的解码制定为先验引导的自适应去噪过程。它利用ASR先验来初始化去噪过程并为序列长度提供锚点。基于此先验，长度自适应剪枝动态地去除冗余标记，而基于置信度的去噪允许已收敛的标记提前退出去噪循环，从而实现标记级别的自适应计算。实验证明，dLLM-ASR实现了与自回归LLM-based ASR系统相当的识别准确率，并将推理速度提高了4.44倍，为ASR建立了一种实用高效的范式。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "update_time": "2026-01-27",
    "paper_authors": "Wenjie Tian, Bingshen Mu, Guobin Ma, Xuelong Geng, Zhixian Zhao, Lei Xie",
    "topic": [
      "Speech Recognition",
      "Audio Representation Learning"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "From Human Speech to Ocean Signals: Transferring Speech Large Models for Underwater Acoustic Target Recognition",
    "paper_title_zh": "从人类语音到海洋信号：将语音大模型迁移用于水下声学目标识别",
    "paper_id": "2601.18086",
    "paper_abstract": "Underwater acoustic target recognition (UATR) plays a vital role in marine applications but remains challenging due to limited labeled data and the complexity of ocean environments. This paper explores a central question: can speech large models (SLMs), trained on massive human speech corpora, be effectively transferred to underwater acoustics? To investigate this, we propose UATR-SLM, a simple framework that reuses the speech feature pipeline, adapts the SLM as an acoustic encoder, and adds a lightweight this http URL on the DeepShip and ShipsEar benchmarks show that UATR-SLM achieves over 99% in-domain accuracy, maintains strong robustness across variable signal lengths, and reaches up to 96.67% accuracy in cross-domain evaluation. These results highlight the strong transferability of SLMs to UATR, establishing a promising paradigm for leveraging speech foundation models in underwater acoustics.",
    "paper_abstract_zh": "水下声学目标识别(UATR)在海洋应用中起着至关重要的作用，但由于标记数据有限和海洋环境的复杂性，它仍然具有挑战性。本文探讨了一个核心问题：在海量人类语音语料库上训练的语音大模型(SLM)能否有效地迁移到水下声学领域？为此，我们提出了UATR-SLM，这是一个简单的框架，它重用了语音特征处理流程，将SLM适配为声学编码器，并添加了一个轻量级分类器。在DeepShip和ShipsEar基准测试上的结果表明，UATR-SLM在领域内准确率超过99%，在可变信号长度上保持强大的鲁棒性，并在跨域评估中达到高达96.67%的准确率。这些结果突显了SLM向UATR迁移的强大能力，为在水下声学中利用语音基础模型建立了一个有前景的范式。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "update_time": "2026-01-27",
    "paper_authors": "Mengcheng Huang, Xue Zhou, Chen Xu, Dapeng Man",
    "topic": [
      "Audio Representation Learning",
      "Audio Classification"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "VIBEVOICE-ASR Technical Report",
    "paper_title_zh": "VIBEVOICE-ASR 技术报告",
    "paper_id": "2601.18184",
    "paper_abstract": "This report presents VibeVoice-ASR, a general-purpose speech understanding framework built upon VibeVoice, designed to address the persistent challenges of context fragmentation and multi-speaker complexity in long-form audio (e.g., meetings, podcasts) that remain despite recent advancements in short-form speech recognition. Unlike traditional pipelined approaches that rely on audio chunking, VibeVoice-ASRsupports single-pass processing for up to 60 minutes of audio. It unifies Automatic Speech Recognition, Speaker Diarization, and Timestamping into a single end-to-end generation task. In addition, VibeVoice-ASR supports over 50 languages, requires no explicit language setting, and natively handles code-switching within and across utterances. Furthermore, we introduce a prompt-based context injection mechanism that allows users to supply customized conetxt, significantly improving accuracy on domain-specific terminology and polyphonic character disambiguation.",
    "paper_abstract_zh": "本报告介绍了VibeVoice-ASR，一个基于VibeVoice构建的通用语音理解框架，旨在解决长音频（如会议、播客）中持续存在的上下文碎片化和多说话人复杂性问题，尽管短语音识别领域最近取得了进展。与依赖音频分块的传统流水线方法不同，VibeVoice-ASR支持单次处理长达60分钟的音频。它将自动语音识别、说话人分离和时间戳统一为单个端到端生成任务。此外，VibeVoice-ASR支持50多种语言，无需明确语言设置，并能自然处理语句内和跨语句的代码切换。此外，我们引入了一种基于提示的上下文注入机制，允许用户提供自定义上下文，显著提高特定领域术语和歧义字符的准确性。",
    "subjects": [
      "Artificial Intelligence (cs.AI)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2026-01-27",
    "paper_authors": "Zhiliang Peng, Jianwei Yu, Yaoyao Chang, Zilong Wang, Li Dong, Yingbo Hao, Yujie Tu, Chenyu Yang, Wenhui Wang, Songchen Xu, Yutao Sun, Hangbo Bao, Weijiang Xu, Yi Zhu, Zehua Wang, Ting Song, Yan Xia, Zewen Chi, Shaohan Huang, Liang Wang, Chuang Ding, Shuai Wang, Xie Chen, Furu Wei",
    "topic": [
      "Speech Recognition"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "LLM-ForcedAligner: A Non-Autoregressive and Accurate LLM-Based Forced Aligner for Multilingual and Long-Form Speech",
    "paper_title_zh": "LLM-ForcedAligner：一种用于多语言和长语音的非自回归且准确的基于大语言模型的强制对齐器",
    "paper_id": "2601.18220",
    "paper_abstract": "Forced alignment (FA) predicts start and end timestamps for words or characters in speech, but existing methods are language-specific and prone to cumulative temporal shifts. The multilingual speech understanding and long-sequence processing abilities of speech large language models (SLLMs) make them promising for FA in multilingual, crosslingual, and long-form speech settings. However, directly applying the next-token prediction paradigm of SLLMs to FA results in hallucinations and slow inference. To bridge the gap, we propose LLM-ForcedAligner, reformulating FA as a slot-filling paradigm: timestamps are treated as discrete indices, and special timestamp tokens are inserted as slots into the transcript. Conditioned on the speech embeddings and the transcript with slots, the SLLM directly predicts the time indices at slots. During training, causal attention masking with non-shifted input and label sequences allows each slot to predict its own timestamp index based on itself and preceding context, with loss computed only at slot positions. Dynamic slot insertion enables FA at arbitrary positions. Moreover, non-autoregressive inference is supported, avoiding hallucinations and improving speed. Experiments across multilingual, crosslingual, and long-form speech scenarios show that LLM-ForcedAligner achieves a 69%~78% relative reduction in accumulated averaging shift compared with prior methods. The checkpoint and inference code will be released later.",
    "paper_abstract_zh": "强制对齐（FA）预测语音中单词或字符的开始和结束时间戳，但现有方法具有语言特定性且容易产生累积时间偏移。语音大语言模型（SLLMs）在多语言语音理解和长序列处理方面的能力使其在多语言、跨语言和长语音场景中的FA应用具有潜力。然而，直接将SLLMs的下一个词预测范式应用于FA会导致幻觉和推理速度慢。为了弥合这一差距，我们提出了LLM-ForcedAligner，将FA重新表述为槽填充范式：时间戳被视为离散索引，特殊时间戳标记作为槽插入到转录文本中。基于语音嵌入和带有槽的转录文本，SLLM直接预测槽处的时间索引。在训练过程中，使用非偏移输入和标签序列的因果注意力掩码，使每个槽能够基于自身和前文预测自己的时间戳索引，且仅在槽位置计算损失。动态槽插入支持在任意位置进行FA。此外，该方法支持非自回归推理，避免了幻觉并提高了速度。在多语言、跨语言和长语音场景的实验表明，与先前方法相比，LLM-ForcedAligner实现了累积平均偏移69%~78%的相对减少。模型检查点和推理代码将在后续发布。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "update_time": "2026-01-27",
    "paper_authors": "Bingshen Mu, Xian Shi, Xiong Wang, Hexin Liu, Jin Xu, Lei Xie",
    "topic": [
      "Speech Recognition",
      "Audio Representation Learning"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "OCR-Enhanced Multimodal ASR Can Read While Listening",
    "paper_title_zh": "OCR增强的多模态ASR可在听的同时阅读",
    "paper_id": "2601.18393",
    "paper_abstract": "Visual information, such as subtitles in a movie, often helps automatic speech recognition. In this paper, we propose Donut-Whisper, an audio-visual ASR model with dual encoder to leverage visual information to improve speech recognition performance in both English and Chinese. Donut-Whisper combines the advantage of the linear and the Q-Former-based modality alignment structures via a cross-attention module, generating more powerful audio-visual features. Meanwhile, we propose a lightweight knowledge distillation scheme showcasing the potential of using audio-visual models to teach audio-only models to achieve better performance. Moreover, we propose a new multilingual audio-visual speech recognition dataset based on movie clips containing both Chinese and English partitions. As a result, Donut-Whisper achieved significantly better performance on both English and Chinese partition of the dataset compared to both Donut and Whisper large V3 baselines. In particular, an absolute 5.75% WER reduction and a 16.5% absolute CER reduction were achieved on the English and Chinese sets respectively compared to the Whisper ASR baseline.",
    "paper_abstract_zh": "视觉信息，如电影中的字幕，通常有助于自动语音识别。在本文中，我们提出了Donut-Whisper，一个具有双编码器的视听ASR模型，利用视觉信息来提高英语和中文的语音识别性能。Donut-Whisper通过交叉注意力模块结合了线性和基于Q-Former的模态对齐结构的优势，生成更强大的视听特征。同时，我们提出了一种轻量级知识蒸馏方案，展示了使用视听模型教授仅音频模型以实现更好性能的潜力。此外，我们基于包含中英文分区的电影片段提出了一种新的多模态视听语音识别数据集。结果表明，与Donut和Whisper large V3基线相比，Donut-Whisper在数据集的中英文分区上都取得了显著更好的性能。特别是，与Whisper ASR基线相比，在英文和中文集上分别实现了绝对5.75%的WER降低和16.5%的绝对CER降低。",
    "subjects": [
      "Sound (cs.SD)",
      "Computation and Language (cs.CL)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2026-01-27",
    "paper_authors": "Junli Chen, Changli Tang, Yixuan Li, Guangzhi Sun, Chao Zhang",
    "topic": [
      "Speech Recognition",
      "Audio Representation Learning"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Pisets: A Robust Speech Recognition System for Lectures and Interviews",
    "paper_title_zh": "Pisets：面向讲座和访谈的鲁棒语音识别系统",
    "paper_id": "2601.18415",
    "paper_abstract": "This work presents a speech-to-text system \"Pisets\" for scientists and journalists which is based on a three-component architecture aimed at improving speech recognition accuracy while minimizing errors and hallucinations associated with the Whisper model. The architecture comprises primary recognition using Wav2Vec2, false positive filtering via the Audio Spectrogram Transformer (AST), and final speech recognition through Whisper. The implementation of curriculum learning methods and the utilization of diverse Russian-language speech corpora significantly enhanced the system's effectiveness. Additionally, advanced uncertainty modeling techniques were introduced, contributing to further improvements in transcription quality. The proposed approaches ensure robust transcribing of long audio data across various acoustic conditions compared to WhisperX and the usual Whisper model. The source code of \"Pisets\" system is publicly available at GitHub: this https URL.",
    "paper_abstract_zh": "这项工作为科学家和记者提出了一个名为'Pisets'的语音转文本系统，该系统基于三组件架构，旨在提高语音识别准确性的同时，最小化与Whisper模型相关的错误和幻觉。该架构包括使用Wav2Vec2进行初步识别，通过音频频谱变换器(AST)进行假阳性过滤，以及通过Whisper进行最终语音识别。课程学习方法的实施和多样化俄语语音语料库的利用显著提高了系统的有效性。此外，先进的不确定性建模技术的引入进一步提高了转录质量。与WhisperX和常规Whisper模型相比，所提出的方法确保在各种声学条件下对长音频数据进行鲁棒转录。'Pisets'系统的源代码已在GitHub上公开：this https URL。",
    "subjects": [
      "Sound (cs.SD)",
      "Computation and Language (cs.CL)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2026-01-27",
    "paper_authors": "Ivan Bondarenko, Daniil Grebenkin, Oleg Sedukhin, Mikhail Klementev, Roman Derunets, Lyudmila Budneva",
    "topic": [
      "Speech Recognition",
      "Audio Representation Learning"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Geneses: Unified Generative Speech Enhancement and Separation",
    "paper_title_zh": "Geneses：统一的生成式语音增强与分离",
    "paper_id": "2601.18456",
    "paper_abstract": "Real-world audio recordings often contain multiple speakers and various degradations, which limit both the quantity and quality of speech data available for building state-of-the-art speech processing models. Although end-to-end approaches that concatenate speech enhancement (SE) and speech separation (SS) to obtain a clean speech signal for each speaker are promising, conventional SE-SS methods suffer from complex degradations beyond additive noise. To this end, we propose \\textbf{Geneses}, a generative framework to achieve unified, high-quality SE--SS. Our Geneses leverages latent flow matching to estimate each speaker's clean speech features using multi-modal diffusion Transformer conditioned on self-supervised learning representation from noisy mixture. We conduct experimental evaluation using two-speaker mixtures from LibriTTS-R under two conditions: additive-noise-only and complex degradations. The results demonstrate that Geneses significantly outperforms a conventional mask-based SE--SS method across various objective metrics with high robustness against complex degradations. Audio samples are available in our demo page.",
    "paper_abstract_zh": "现实世界的音频录音通常包含多个说话者和各种退化因素，这限制了用于构建最先进语音处理模型的语音数据的数量和质量。尽管将语音增强（SE）和语音分离（SS）连接以获得每个说话者的干净语音信号的端到端方法很有前景，但传统的SE-SS方法在处理除加性噪声外的复杂退化因素时存在局限性。为此，我们提出了Geneses，一个生成式框架，用于实现统一、高质量的SE-SS。Geneses利用潜在流匹配，根据来自 noisy mixture 的自监督学习表示，通过多模态扩散 Transformer 估计每个说话者的干净语音特征。我们在两种条件下使用 LibriTTS-R 中的双说话者混合物进行实验评估：仅加性噪声和复杂退化。结果表明，在各种客观指标上，Geneses 显著优于传统的基于掩码的SE-SS方法，并且对复杂退化具有很高的鲁棒性。音频样本可在我们的演示页面获取。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "update_time": "2026-01-27",
    "paper_authors": "Kohei Asai, Wataru Nakata, Yuki Saito, Hiroshi Saruwatari",
    "topic": [
      "Speech Enhancement",
      "Audio Representation Learning"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Analytic Incremental Learning For Sound Source Localization With Imbalance Rectification",
    "paper_title_zh": "用于不平衡修正的声源定位分析增量学习",
    "paper_id": "2601.18335",
    "paper_abstract": "Sound source localization (SSL) demonstrates remarkable results in controlled settings but struggles in real-world deployment due to dual imbalance challenges: intra-task imbalance arising from long-tailed direction-of-arrival (DoA) distributions, and inter-task imbalance induced by cross-task skews and overlaps. These often lead to catastrophic forgetting, significantly degrading the localization accuracy. To mitigate these issues, we propose a unified framework with two key innovations. Specifically, we design a GCC-PHAT-based data augmentation (GDA) method that leverages peak characteristics to alleviate intra-task distribution skews. We also propose an Analytic dynamic imbalance rectifier (ADIR) with task-adaption regularization, which enables analytic updates that adapt to inter-task dynamics. On the SSLR benchmark, our proposal achieves state-of-the-art (SoTA) results of 89.0% accuracy, 5.3° mean absolute error, and 1.6 backward transfer, demonstrating robustness to evolving imbalances without exemplar storage.",
    "paper_abstract_zh": "声源定位(SSL)在受控环境中表现出色，但由于双重不平衡挑战而在实际部署中遇到困难：由长尾到达方向(DoA)分布导致的任务内不平衡，以及由跨任务偏差和重叠引起的任务间不平衡。这些问题通常会导致灾难性遗忘，显著降低定位精度。为缓解这些问题，我们提出了一个统一框架，包含两个关键创新。具体而言，我们设计了一种基于GCC-PHAT的数据增强(GDA)方法，利用峰值特征来减轻任务内分布偏差。我们还提出了一个具有任务适应正则化的分析动态不平衡修正器(ADIR)，它能够进行适应任务间动态的分析更新。在SSLR基准测试中，我们的方法实现了89.0%的准确率、5.3°的平均绝对误差和1.6的后向转移，展示了在不存储样本的情况下对 evolving 不平衡的鲁棒性。",
    "subjects": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)"
    ],
    "update_time": "2026-01-27",
    "paper_authors": "Zexia Fan, Yu Chen, Qiquan Zhang, Kainan Chen, Xinyuan Qian",
    "topic": [
      "Audio Representation Learning",
      "Audio Classification"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "A Dataset for Automatic Vocal Mode Classification",
    "paper_title_zh": "用于自动声带模式分类的数据集",
    "paper_id": "2601.18339",
    "paper_abstract": "The Complete Vocal Technique (CVT) is a school of singing developed in the past decades by Cathrin Sadolin et al.. CVT groups the use of the voice into so called vocal modes, namely Neutral, Curbing, Overdrive and Edge. Knowledge of the desired vocal mode can be helpful for singing students. Automatic classification of vocal modes can thus be important for technology-assisted singing teaching. Previously, automatic classification of vocal modes has been attempted without major success, potentially due to a lack of data. Therefore, we recorded a novel vocal mode dataset consisting of sustained vowels recorded from four singers, three of which professional singers with more than five years of CVT-experience. The dataset covers the entire vocal range of the subjects, totaling 3,752 unique samples. By using four microphones, thereby offering a natural data augmentation, the dataset consists of more than 13,000 samples combined. An annotation was created using three CVT-experienced annotators, each providing an individual annotation. The merged annotation as well as the three individual annotations come with the published dataset. Additionally, we provide some baseline classification results. The best balanced accuracy across a 5-fold cross validation of 81.3\\,\\% was achieved with a ResNet18. The dataset can be downloaded under this https URL.",
    "paper_abstract_zh": "完整声乐技术(CVT)是由Cathrin Sadolin等人在过去几十年中发展起来的一种歌唱学派。CVT将声音的使用分为所谓的声带模式，即中性(Neutral)、抑制(Curbing)、过载(Overdrive)和边缘(Edge)。了解所需的声带模式对歌唱学生可能有帮助。因此，声带模式的自动分类对于技术辅助的歌唱教学可能很重要。此前，声带模式的自动分类尝试未取得重大成功，可能是由于数据不足。因此，我们录制了一个新的声带模式数据集，包含四位歌手录制的持续元音，其中三位是拥有五年以上CVT经验的专业歌手。该数据集涵盖了受试者的整个音域，总计3,752个独特样本。通过使用四个麦克风，提供了自然的数据增强，该数据集总共包含超过13,000个样本。使用三位具有CVT经验的注释者创建了注释，每位注释者提供单独的注释。合并的注释以及三个单独的注释均随发布的数据集一起提供。此外，我们还提供了一些基线分类结果。在5折交叉验证中，使用ResNet18实现了81.3%的最佳平衡准确率。该数据集可通过此https URL下载。",
    "subjects": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)"
    ],
    "update_time": "2026-01-27",
    "paper_authors": "Reemt Hinrichs, Sonja Stephan, Alexander Lange, Jörn Ostermann",
    "topic": [
      "Audio Classification",
      "Speech Recognition"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "UrgentMOS: Unified Multi-Metric and Preference Learning for Robust Speech Quality Assessment",
    "paper_title_zh": "UrgentMOS：用于鲁棒语音质量评估的统一多指标和偏好学习",
    "paper_id": "2601.18438",
    "paper_abstract": "Automatic speech quality assessment has become increasingly important as modern speech generation systems continue to advance, while human listening tests remain costly, time-consuming, and difficult to scale. Most existing learning-based assessment models rely primarily on scarce human-annotated mean opinion score (MOS) data, which limits robustness and generalization, especially when training across heterogeneous datasets. In this work, we propose UrgentMOS, a unified speech quality assessment framework that jointly learns from diverse objective and perceptual quality metrics, while explicitly tolerating the absence of arbitrary subsets of metrics during training. By leveraging complementary quality facets under heterogeneous supervision, UrgentMOS enables effective utilization of partially annotated data and improves robustness when trained on large-scale, multi-source datasets. Beyond absolute score prediction, UrgentMOS explicitly models pairwise quality preferences by directly predicting comparative MOS (CMOS), making it well suited for preference-based evaluation scenarios commonly adopted in system benchmarking. Extensive experiments across a wide range of speech quality datasets, including simulated distortions, speech enhancement, and speech synthesis, demonstrate that UrgentMOS consistently achieves state-of-the-art performance in both absolute and comparative evaluation settings.",
    "paper_abstract_zh": "随着现代语音生成系统的不断发展，自动语音质量评估变得越来越重要，而人工听音测试仍然成本高昂、耗时且难以扩展。大多数现有的基于学习的评估模型主要依赖于稀缺的人工标注平均意见分数(MOS)数据，这限制了模型的鲁棒性和泛化能力，特别是在跨异构数据集进行训练时。在这项工作中，我们提出了UrgentMOS，一个统一的语音质量评估框架，它可以从多样化的客观和感知质量指标中联合学习，同时明确容忍训练过程中任意指标子集的缺失。通过在异构监督下利用互补的质量方面，UrgentMOS能够有效利用部分标注的数据，并在大规模多源数据集上训练时提高鲁棒性。除了绝对分数预测外，UrgentMOS还通过直接预测比较MOS(CMOS)来明确建模成对的质量偏好，使其非常适合系统基准测试中常用的基于偏好的评估场景。在广泛的语音质量数据集上进行的大量实验，包括模拟失真、语音增强和语音合成，证明UrgentMOS在绝对和比较评估设置中均持续达到最先进的性能。",
    "subjects": [
      "Sound (cs.SD)"
    ],
    "update_time": "2026-01-27",
    "paper_authors": "Wei Wang, Wangyou Zhang, Chenda Li, Jiahe Wang, Samuele Cornell, Marvin Sach, Kohei Saijo, Yihui Fu, Zhaoheng Ni, Bing Han, Xun Gong, Mengxiao Bi, Tim Fingscheidt, Shinji Watanabe, Yanmin Qian",
    "topic": [
      "Speech Enhancement",
      "Speech Synthesis"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Neural Multi-Speaker Voice Cloning for Nepali in Low-Resource Settings",
    "paper_title_zh": "低资源环境下尼泊尔语的多说话人神经语音克隆",
    "paper_id": "2601.18694",
    "paper_abstract": "This research presents a few-shot voice cloning system for Nepali speakers, designed to synthesize speech in a specific speaker's voice from Devanagari text using minimal data. Voice cloning in Nepali remains largely unexplored due to its low-resource nature. To address this, we constructed separate datasets: untranscribed audio for training a speaker encoder and paired text-audio data for training a Tacotron2-based synthesizer. The speaker encoder, optimized with Generative End2End loss, generates embeddings that capture the speaker's vocal identity, validated through Uniform Manifold Approximation and Projection (UMAP) for dimension reduction visualizations. These embeddings are fused with Tacotron2's text embeddings to produce mel-spectrograms, which are then converted into audio using a WaveRNN vocoder. Audio data were collected from various sources, including self-recordings, and underwent thorough preprocessing for quality and alignment. Training was performed using mel and gate loss functions under multiple hyperparameter settings. The system effectively clones speaker characteristics even for unseen voices, demonstrating the feasibility of few-shot voice cloning for the Nepali language and establishing a foundation for personalized speech synthesis in low-resource scenarios.",
    "paper_abstract_zh": "本研究为尼泊尔语说话者提出了一种少样本语音克隆系统，旨在使用最少数据从天城文文本中合成特定说话者的语音。由于尼泊尔语属于低资源语言，其语音克隆领域尚未得到充分探索。为此，我们构建了两个独立的数据集：未转录的音频数据用于训练说话人编码器，配对的文本-音频数据用于训练基于Tacotron2的合成器。说话人编码器通过生成端到端损失函数进行优化，生成能够捕捉说话人语音特征的嵌入向量，并通过均匀流形近似和投影(UMAP)进行降维可视化以验证。这些嵌入向量与Tacotron2的文本嵌入相融合，生成梅尔频谱图，然后使用WaveRNN声码器转换为音频。音频数据从多种来源收集，包括自我录制，并经过严格预处理以确保质量和对齐。训练过程中采用了多种超参数设置，并使用了梅尔损失和门控损失函数。该系统能够有效克隆未见说话者的语音特征，证明了尼泊尔语少样本语音克隆的可行性，并为低资源场景下的个性化语音合成奠定了基础。",
    "subjects": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)"
    ],
    "update_time": "2026-01-27",
    "paper_authors": "Aayush M. Shrestha, Aditya Bajracharya, Projan Shakya, Dinesh B. Kshatri",
    "topic": [
      "Speech Synthesis",
      "Audio Representation Learning"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Reflecting Twice before Speaking with Empathy: Self-Reflective Alternating Inference for Empathy-Aware End-to-End Spoken Dialogue",
    "paper_title_zh": "说话前两次反思并怀有同理心：用于同理心感知的端到端口语对话的自反思交替推理",
    "paper_id": "2601.18281",
    "paper_abstract": "End-to-end Spoken Language Models (SLMs) hold great potential for paralinguistic perception, and numerous studies have aimed to enhance their capabilities, particularly for empathetic dialogue. However, current approaches largely depend on rigid supervised signals, such as ground-truth response in supervised fine-tuning or preference scores in reinforcement learning. Such reliance is fundamentally limited for modeling complex empathy, as there is no single \"correct\" response and a simple numerical score cannot fully capture the nuances of emotional expression or the appropriateness of empathetic behavior. To address these limitations, we sequentially introduce EmpathyEval, a descriptive natural-language-based evaluation model for assessing empathetic quality in spoken dialogues. Building upon EmpathyEval, we propose ReEmpathy, an end-to-end SLM that enhances empathetic dialogue through a novel Empathetic Self-Reflective Alternating Inference mechanism, which interleaves spoken response generation with free-form, empathy-related reflective reasoning. Extensive experiments demonstrate that ReEmpathy substantially improves empathy-sensitive spoken dialogue by enabling reflective reasoning, offering a promising approach toward more emotionally intelligent and empathy-aware human-computer interactions.",
    "paper_abstract_zh": "端到端口语语言模型（SLMs）在副语言感知方面具有巨大潜力，许多研究致力于提升其能力，特别是在同理心对话方面。然而，当前方法主要依赖于严格的监督信号，如监督微调中的真实响应或强化学习中的偏好评分。这种依赖性在建模复杂同理心方面存在根本性局限，因为没有单一的'正确'响应，简单的数值评分也无法完全捕捉情感表达的细微差别或同理心行为的适当性。为解决这些局限，我们依次介绍了EmpathyEval，一种基于描述性自然语言的评估模型，用于评估口语对话中的同理心质量。基于EmpathyEval，我们提出了ReEmpathy，一种端到端SLM，通过新颖的同理心自反思交替推理机制增强同理心对话，该机制将口语响应生成与自由形式的、与同理心相关的反思推理交替进行。大量实验表明，ReEmpathy通过启用反思推理显著提升了同理心敏感的口语对话，为更具情感智能和同理心感知的人机交互提供了有前景的方法。",
    "subjects": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)"
    ],
    "update_time": "2026-01-27",
    "paper_authors": "Yuhang Jia, Pei Liu, Haoqin Sun, Jiaming Zhou, Xuxin Cheng, Cao Liu, Ke Zeng, Xunliang Cai, Yong Qin",
    "topic": [
      "Speech Recognition",
      "Speech Synthesis"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "3DGesPolicy: Phoneme-Aware Holistic Co-Speech Gesture Generation Based on Action Control",
    "paper_title_zh": "3DGesPolicy：基于动作控制的音素感知整体协同手势生成",
    "paper_id": "2601.18451",
    "paper_abstract": "Generating holistic co-speech gestures that integrate full-body motion with facial expressions suffers from semantically incoherent coordination on body motion and spatially unstable meaningless movements due to existing part-decomposed or frame-level regression methods, We introduce 3DGesPolicy, a novel action-based framework that reformulates holistic gesture generation as a continuous trajectory control problem through diffusion policy from robotics. By modeling frame-to-frame variations as unified holistic actions, our method effectively learns inter-frame holistic gesture motion patterns and ensures both spatially and semantically coherent movement trajectories that adhere to realistic motion manifolds. To further bridge the gap in expressive alignment, we propose a Gesture-Audio-Phoneme (GAP) fusion module that can deeply integrate and refine multi-modal signals, ensuring structured and fine-grained alignment between speech semantics, body motion, and facial expressions. Extensive quantitative and qualitative experiments on the BEAT2 dataset demonstrate the effectiveness of our 3DGesPolicy across other state-of-the-art methods in generating natural, expressive, and highly speech-aligned holistic gestures.",
    "paper_abstract_zh": "生成整合全身动作与面部表情的整体协同手势，由于现有的部分分解或帧级回归方法，导致身体运动在语义上不协调，且空间上存在无意义的稳定运动。我们引入了3DGesPolicy，一种新颖的基于动作的框架，它通过来自机器人技术的扩散策略，将整体手势生成重新表述为连续轨迹控制问题。通过将帧间变化建模为统一的整体动作，我们的方法有效学习了帧间整体手势运动模式，并确保了空间和语义上一致的运动轨迹，这些轨迹遵循真实的运动流形。为了进一步弥合表达对齐的差距，我们提出了一个手势-音素-音频（GAP）融合模块，该模块能够深度整合和细化多模态信号，确保语音语义、身体运动和面部表情之间结构化和细粒度的对齐。在BEAT2数据集上进行的大量定量和定性实验证明了我们的3DGesPolicy在生成自然、富有表现力且高度语音对齐的整体手势方面优于其他最先进方法的有效性。",
    "subjects": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Multimedia (cs.MM)",
      "Sound (cs.SD)"
    ],
    "update_time": "2026-01-27",
    "paper_authors": "Xuanmeng Sha, Liyun Zhang, Tomohiro Mashita, Naoya Chiba, Yuki Uranishi",
    "topic": [
      "Video Generation",
      "Speech Recognition"
    ],
    "category": [
      "Image&Video"
    ]
  }
]