[
  {
    "paper_title": "An Adaptive Method for Target Curve Selection",
    "paper_title_zh": "一种目标曲线选择的自适应方法",
    "paper_id": "2512.08313",
    "paper_abstract": "In this paper, we introduce an adaptation of the \"Interactive Differential Evolution\" (IDE) algorithm to the audio domain for the task of identifying the preferred over-the-ear headphone frequency response target among consumers. The method is based on data collection using an adaptive paired rating listening test paradigm (paired comparison with a scale). The IDE algorithm and its parameters are explained in detail. Additionally, data collected from three listening experiments with more than 20 consumers is presented, and the algorithm's performance in this untested domain is investigated on the basis of two convergence measures. The results indicate that this method can converge and may ease the task of 'extracting' frequency response preference from untrained consumers.",
    "paper_abstract_zh": "本文介绍了将“交互式差分进化”（IDE）算法应用于音频领域，用于识别消费者偏好的头戴式耳机频率响应目标。该方法基于使用自适应配对评分听力测试范式（带量表的配对比较）的数据收集。详细解释了IDE算法及其参数。此外，展示了来自三个听力实验的数据，涉及超过20名消费者，并基于两个收敛性指标研究了该算法在这一未测试领域的性能。结果表明，该方法可以收敛，并可能简化从未经训练的消费者中“提取”频率响应偏好的任务。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2025-12-10",
    "paper_authors": "Gabriele Ravizza, Julián Villegas, Christer P. Volk, Tore Stegenborg-Andersen, Yan Pei",
    "topic": [
      "Other"
    ],
    "category": [
      "Other"
    ]
  },
  {
    "paper_title": "BUT Systems for Environmental Sound Deepfake Detection in the ESDD 2026 Challenge",
    "paper_title_zh": "BUT系统用于ESDD 2026挑战赛中的环境声音深度伪造检测",
    "paper_id": "2512.08319",
    "paper_abstract": "This paper describes the BUT submission to the ESDD 2026 Challenge, specifically focusing on Track 1: Environmental Sound Deepfake Detection with Unseen Generators. To address the critical challenge of generalizing to audio generated by unseen synthesis algorithms, we propose a robust ensemble framework leveraging diverse Self-Supervised Learning (SSL) models. We conduct a comprehensive analysis of general audio SSL models (including BEATs, EAT, and Dasheng) and speech-specific SSLs. These front-ends are coupled with a lightweight Multi-Head Factorized Attention (MHFA) back-end to capture discriminative representations. Furthermore, we introduce a feature domain augmentation strategy based on distribution uncertainty modeling to enhance model robustness against unseen spectral distortions. All models are trained exclusively on the official EnvSDD data, without using any external resources. Experimental results demonstrate the effectiveness of our approach: our best single system achieved Equal Error Rates (EER) of 0.00\\%, 4.60\\%, and 4.80\\% on the Development, Progress (Track 1), and Final Evaluation sets, respectively. The fusion system further improved generalization, yielding EERs of 0.00\\%, 3.52\\%, and 4.38\\% across the same partitions.",
    "paper_abstract_zh": "本文描述了BUT团队对ESDD 2026挑战赛的提交方案，特别专注于赛道1：面向未见生成器的环境声音深度伪造检测。为解决对未见合成算法生成的音频进行泛化的关键挑战，我们提出了一种利用多样化自监督学习(SSL)模型的鲁棒集成框架。我们对通用音频SSL模型（包括BEATs、EAT和Dasheng）以及语音特定的SSL模型进行了全面分析。这些前端与轻量级多头因子化注意力(MHFA)后端相结合，以捕获判别性表示。此外，我们引入了一种基于分布不确定性建模的特征域增强策略，以增强模型对未见频谱失真的鲁棒性。所有模型仅在官方EnvSDD数据上训练，未使用任何外部资源。实验结果证明了我们方法的有效性：我们的最佳单一系统在开发集、进度集（赛道1）和最终评估集上分别实现了0.00%、4.60%和4.80%的等错误率(EER)。融合系统进一步提高了泛化能力，在同一数据集上分别实现了0.00%、3.52%和4.38%的EER。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2025-12-10",
    "paper_authors": "Junyi Peng, Lin Zhang, Jin Li, Oldrich Plchot, Jan Cernocky",
    "topic": [
      "Audio Representation Learning",
      "Audio Classification"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "AudioScene: Integrating Object-Event Audio into 3D Scenes",
    "paper_title_zh": "AudioScene：将对象-事件音频集成到3D场景中",
    "paper_id": "2512.07845",
    "paper_abstract": "The rapid advances in audio analysis underscore its vast potential for humancomputer interaction, environmental monitoring, and public safety; yet, existing audioonly datasets often lack spatial context. To address this gap, we present two novel audiospatial scene datasets, AudioScanNet and AudioRoboTHOR, designed to explore audioconditioned tasks within 3D environments. By integrating audio clips with spatially aligned 3D scenes, our datasets enable research on how audio signals interact with spatial context. To associate audio events with corresponding spatial information, we leverage the common sense reasoning ability of large language models and supplement them with rigorous human verification, This approach offers greater scalability compared to purely manual annotation while maintaining high standards of accuracy, completeness, and diversity, quantified through inter annotator agreement and performance on two benchmark tasks audio based 3D visual grounding and audio based robotic zeroshot navigation. The results highlight the limitations of current audiocentric methods and underscore the practical challenges and significance of our datasets in advancing audio guided spatial learning.",
    "paper_abstract_zh": "音频分析的快速进展凸显了其在人机交互、环境监测和公共安全方面的巨大潜力；然而，现有的纯音频数据集通常缺乏空间上下文。为解决这一差距，我们提出了两个新颖的音频空间场景数据集：AudioScanNet和AudioRoboTHOR，旨在探索3D环境中的音频条件任务。通过将音频片段与空间对齐的3D场景相结合，我们的数据集使研究音频信号如何与空间上下文交互成为可能。为了将音频事件与相应的空间信息关联起来，我们利用大型语言模型的常识推理能力，并通过严格的人工验证进行补充。与纯手动标注相比，这种方法具有更好的可扩展性，同时保持高标准的准确性、完整性和多样性，通过标注者间一致性和两个基准任务（基于音频的3D视觉定位和基于音频的机器人零样本导航）的性能进行量化。结果突出了当前以音频为中心的方法的局限性，并强调了我们的数据集在推进音频引导的空间学习方面的实际挑战和意义。",
    "subjects": [
      "Artificial Intelligence (cs.AI)",
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "update_time": "2025-12-10",
    "paper_authors": "Shuaihang Yuan, Congcong Wen, Muhammad Shafique, Anthony Tzes, Yi Fang",
    "topic": [
      "Audio Representation Learning",
      "Other"
    ],
    "category": [
      "Other"
    ]
  },
  {
    "paper_title": "LocaGen: Sub-Sample Time-Delay Learning for Beam Localization",
    "paper_title_zh": "LocaGen：用于波束定位的子样本时延学习",
    "paper_id": "2512.07872",
    "paper_abstract": "The goal of LocaGen is to improve the localization performance of audio signals in the 2-D beam localization problem. LocaGen reduces sampling quantization errors through machine learning models trained on realistic synthetic data generated by a simulation. The system increases the accuracy of both direction-of-arrival (DOA) and precise location estimation of an audio beam from an array of three microphones. We demonstrate LocaGen's efficacy on a low-powered embedded system with an increased localization accuracy with a minimal increase in real-time resource usage. LocaGen was demonstrated to reduce DOA error by approximately 67% even with a microphone array of only 10 kHz in audio processing.",
    "paper_abstract_zh": "LocaGen的目标是提高二维波束定位问题中音频信号的定位性能。LocaGen通过在由仿真生成的真实合成数据上训练的机器学习模型来减少采样量化误差。该系统提高了来自三个麦克风阵列的音频波束的到达方向(DOA)和精确位置估计的准确性。我们在低功耗嵌入式系统上展示了LocaGen的有效性，以最小的实时资源使用量增加了定位准确性。即使在处理音频的麦克风阵列仅为10 kHz的情况下，LocaGen也被证明可以将DOA误差减少约67%。",
    "subjects": [
      "Signal Processing (eess.SP)",
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "update_time": "2025-12-10",
    "paper_authors": "Ishaan Kunwar, Henry Cantor, Tyler Rizzo, Ayaan Qayyum",
    "topic": [
      "Audio Representation Learning",
      "Other"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Beyond Unified Models: A Service-Oriented Approach to Low Latency, Context Aware Phonemization for Real Time TTS",
    "paper_title_zh": "超越统一模型：面向实时TTS的低延迟、上下文感知音素化的服务导向方法",
    "paper_id": "2512.08006",
    "paper_abstract": "Lightweight, real-time text-to-speech systems are crucial for accessibility. However, the most efficient TTS models often rely on lightweight phonemizers that struggle with context-dependent challenges. In contrast, more advanced phonemizers with a deeper linguistic understanding typically incur high computational costs, which prevents real-time performance.\nThis paper examines the trade-off between phonemization quality and inference speed in G2P-aided TTS systems, introducing a practical framework to bridge this gap. We propose lightweight strategies for context-aware phonemization and a service-oriented TTS architecture that executes these modules as independent services. This design decouples heavy context-aware components from the core TTS engine, effectively breaking the latency barrier and enabling real-time use of high-quality phonemization models. Experimental results confirm that the proposed system improves pronunciation soundness and linguistic accuracy while maintaining real-time responsiveness, making it well-suited for offline and end-device TTS applications.",
    "paper_abstract_zh": "轻量级、实时的文本转语音系统对无障碍访问至关重要。然而，最高效的TTS模型通常依赖于轻量级音素化器，这些器在处理上下文相关挑战时存在困难。相比之下，具有更深层次语言理解的先进音素化器通常会产生高计算成本，从而阻碍实时性能。本文探讨了在G2P辅助TTS系统中音素化质量和推理速度之间的权衡，并提出了一个实用的框架来弥合这一差距。我们提出了上下文感知音素化的轻量级策略，以及一种将这些模块作为独立服务执行的服务导向TTS架构。这种设计将重型上下文感知组件与核心TTS引擎解耦，有效打破了延迟障碍，使高质量音素化模型的实时应用成为可能。实验结果证实，所提出的系统在保持实时响应能力的同时，提高了发音的合理性和语言准确性，使其非常适合离线和终端设备TTS应用。",
    "subjects": [
      "Computation and Language (cs.CL)",
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "update_time": "2025-12-10",
    "paper_authors": "Mahta Fetrat, Donya Navabi, Zahra Dehghanian, Morteza Abolghasemi, Hamid R. Rabiee",
    "topic": [
      "Speech Synthesis"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Error-Resilient Semantic Communication for Speech Transmission over Packet-Loss Networks",
    "paper_title_zh": "基于生成潜在先验的抗丢包网络语音传输错误弹性语义通信",
    "paper_id": "2512.08203",
    "paper_abstract": "Real-time speech communication over wireless networks remains challenging, as conventional channel protection mechanisms cannot effectively counter packet loss under stringent bandwidth and latency constraints. Semantic communication has emerged as a promising paradigm for enhancing the robustness of speech transmission by means of joint source-channel coding (JSCC). However, its cross-layer design hinders practical deployment due to the incompatibility with existing digital communication systems. In this case, the robustness of speech communication is consequently evaluated primarily by the error-resilience to packet loss over wireless networks. To address these challenges, we propose \\emph{Glaris}, a generative latent-prior-based resilient speech semantic communication framework that performs resilient speech coding in the generative latent space. Generative latent priors enable high-quality packet loss concealment (PLC) at the receiver side, well-balancing semantic consistency and reconstruction fidelity. Additionally, an integrated error resilience mechanism is designed to mitigate the error propagation and improve the effectiveness of PLC. Compared with traditional packet-level forward error correction (FEC) strategies, our new method achieves enhanced robustness over dynamic wireless networks while reducing redundancy overhead significantly. Experimental results on the LibriSpeech dataset demonstrate that \\emph{Glaris} consistently outperforms existing error-resilient codecs, achieving JSCC-level robustness while maintaining seamless compatibility with existing systems, and it also strikes a favorable balance between transmission efficiency and speech reconstruction quality.",
    "paper_abstract_zh": "在无线网络上进行实时语音通信仍然具有挑战性，因为传统的信道保护机制在严格的带宽和延迟约束下无法有效应对丢包问题。语义通信作为一种新兴的范式，通过联合信源信道编码（JSCC）提高了语音传输的鲁棒性。然而，其跨层设计由于与现有数字通信系统的不兼容性，阻碍了实际部署。在这种情况下，语音通信的鲁棒性主要通过对无线网络丢包的错误弹性来评估。为应对这些挑战，我们提出了Glaris，一种基于生成潜在先验的弹性语音语义通信框架，该框架在生成潜在空间中执行弹性语音编码。生成潜在先验使得接收端能够实现高质量的丢包隐藏（PLC），很好地平衡了语义一致性和重建保真度。此外，我们还设计了一种集成的错误弹性机制，以减轻错误传播并提高PLC的有效性。与传统数据包级前向纠错（FEC）策略相比，我们的新方法在动态无线网络上实现了更强的鲁棒性，同时显著减少了冗余开销。在LibriSpeech数据集上的实验结果表明，Glaris持续优于现有的错误弹性编解码器，在保持与现有系统无缝兼容的同时，达到了JSCC级别的鲁棒性，并在传输效率和语音重建质量之间取得了良好的平衡。",
    "subjects": [
      "Sound (cs.SD)"
    ],
    "update_time": "2025-12-10",
    "paper_authors": "Zhuohang Han, Jincheng Dai, Shengshi Yao, Junyi Wang, Yanlong Li, Kai Niu, Wenjun Xu, Ping Zhang",
    "topic": [
      "Audio Codec",
      "Speech Enhancement"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "SpeechQualityLLM: LLM-Based Multimodal Assessment of Speech Quality",
    "paper_title_zh": "SpeechQualityLLM: 基于LLM的多模态语音质量评估",
    "paper_id": "2512.08238",
    "paper_abstract": "Objective speech quality assessment is central to telephony, VoIP, and streaming systems, where large volumes of degraded audio must be monitored and optimized at scale. Classical metrics such as PESQ and POLQA approximate human mean opinion scores (MOS) but require carefully controlled conditions and expensive listening tests, while learning-based models such as NISQA regress MOS and multiple perceptual dimensions from waveforms or spectrograms, achieving high correlation with subjective ratings yet remaining rigid: they do not support interactive, natural-language queries and do not natively provide textual rationales. In this work, we introduce SpeechQualityLLM, a multimodal speech quality question-answering (QA) system that couples an audio encoder with a language model and is trained on the NISQA corpus using template-based question-answer pairs covering overall MOS and four perceptual dimensions (noisiness, coloration, discontinuity, and loudness) in both single-ended (degraded only) and double-ended (degraded plus clean reference) setups. Instead of directly regressing scores, our system is supervised to generate textual answers from which numeric predictions are parsed and evaluated with standard regression and ranking metrics; on held-out NISQA clips, the double-ended model attains a MOS mean absolute error (MAE) of 0.41 with Pearson correlation of 0.86, with competitive performance on dimension-wise tasks. Beyond these quantitative gains, it offers a flexible natural-language interface in which the language model acts as an audio quality expert: practitioners can query arbitrary aspects of degradations, prompt the model to emulate different listener profiles to capture human variability and produce diverse but plausible judgments rather than a single deterministic score, and thereby reduce reliance on large-scale crowdsourced tests and their monetary cost.",
    "paper_abstract_zh": "客观语音质量评估是电话、VoIP和流媒体系统的核心，需要大规模监控和优化大量降级音频。传统指标如PESQ和POLQA近似于人类平均意见得分(MOS)，但需要严格控制条件和昂贵的听音测试；而基于学习的模型如NISQA则从波形或频谱图中回归MOS和多个感知维度，与主观评分高度相关，但仍存在局限性：它们不支持交互式自然语言查询，也不提供文本理由。在这项工作中，我们引入了SpeechQualityLLM，一个多模态语音质量问答(QA)系统，该系统将音频编码器与语言模型相结合，并在NISQA语料库上使用模板化问答对进行训练，涵盖整体MOS和四个感知维度（噪声度、色彩度、不连续性和响度），适用于单端（仅降级）和双端（降级加干净参考）设置。我们的系统不是直接回归分数，而是通过生成文本答案进行监督，然后从这些答案中解析出数值预测，并使用标准回归和排序指标进行评估；在保留的NISQA片段上，双端模型实现了0.41的MOS平均绝对误差(MAE)和0.86的皮尔逊相关系数，在维度级任务上具有竞争力。除了这些定量改进外，它还提供了一个灵活的自然语言界面，其中语言模型充当音频质量专家：从业者可以查询降级的任意方面，提示模型模拟不同的听众特征以捕捉人类变异性，并产生多样但合理的判断，而不是单一确定性分数，从而减少对大规模众包测试及其货币成本的依赖。",
    "subjects": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)"
    ],
    "update_time": "2025-12-10",
    "paper_authors": "Mahathir Monjur, Shahriar Nirjon",
    "topic": [
      "Audio Representation Learning",
      "Speech Recognition"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "DFALLM: Achieving Generalizable Multitask Deepfake Detection by Optimizing Audio LLM Components",
    "paper_title_zh": "DFALLM: 通过优化音频LLM组件实现可泛化的多任务深度伪造检测",
    "paper_id": "2512.08403",
    "paper_abstract": "Audio deepfake detection has recently garnered public concern due to its implications for security and reliability. Traditional deep learning methods have been widely applied to this task but often lack generalisability when confronted with newly emerging spoofing techniques and more tasks such as spoof attribution recognition rather than simple binary classification. In principle, Large Language Models (LLMs) are considered to possess the needed generalisation capabilities. However, previous research on Audio LLMs (ALLMs) indicates a generalization bottleneck in audio deepfake detection performance, even when sufficient data is available. Consequently, this study investigates the model architecture and examines the effects of the primary components of ALLMs, namely the audio encoder and the text-based LLM. Our experiments demonstrate that the careful selection and combination of audio encoders and text-based LLMs are crucial for unlocking the deepfake detection potential of ALLMs. We further propose an ALLM structure capable of generalizing deepfake detection abilities to out-of-domain spoofing tests and other deepfake tasks, such as spoof positioning and spoof attribution recognition. Our proposed model architecture achieves state-of-the-art (SOTA) performance across multiple datasets, including ASVSpoof2019, InTheWild, and Demopage, with accuracy reaching up to 95.76% on average, and exhibits competitive capabilities in other deepfake detection tasks such as attribution, and localisation compared to SOTA audio understanding models. Data and codes are provided in supplementary materials.",
    "paper_abstract_zh": "音频深度伪造检测因其对安全和可靠性的影响而最近引起了公众关注。传统深度学习方法已广泛应用于此任务，但当面对新出现的欺骗技术以及更多任务（如欺骗属性识别而非简单的二元分类）时，往往缺乏泛化能力。原则上，大型语言模型（LLMs）被认为具备所需的泛化能力。然而，先前关于音频LLM（ALLM）的研究表明，即使有足够的数据，音频深度伪造检测性能仍存在泛化瓶颈。因此，本研究调查了模型架构并检查了ALLM主要组件（即音频编码器和基于文本的LLM）的影响。我们的实验证明，仔细选择和组合音频编码器和基于文本的LLM对于释放ALLM的深度伪造检测潜力至关重要。我们进一步提出了一种ALLM结构，能够将深度伪造检测能力泛化到域外欺骗测试和其他深度伪造任务，如欺骗定位和欺骗属性识别。我们提出的模型架构在多个数据集（包括ASVSpoof2019、InTheWild和Demopage）上实现了最先进（SOTA）的性能，平均准确率高达95.76%，并且在其他深度伪造检测任务（如属性识别和定位）上与SOTA音频理解模型相比具有竞争力。数据和代码在补充材料中提供。",
    "subjects": [
      "Sound (cs.SD)"
    ],
    "update_time": "2025-12-10",
    "paper_authors": "Yupei Li, Li Wang, Yuxiang Wang, Lei Wang, Rizhao Cai, Jie Shi, Björn W. Schuller, Zhizheng Wu",
    "topic": [
      "Audio Classification",
      "Speech Recognition"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Emovectors: assessing emotional content in jazz improvisations for creativity evaluation",
    "paper_title_zh": "情感向量：评估爵士即兴创作中的情感内容以进行创造力评价",
    "paper_id": "2512.08812",
    "paper_abstract": "Music improvisation is fascinating to study, being essentially a live demonstration of a creative process. In jazz, musicians often improvise across predefined chord progressions (leadsheets). How do we assess the creativity of jazz improvisations? And can we capture this in automated metrics for creativity for current LLM-based generative systems? Demonstration of emotional involvement is closely linked with creativity in improvisation. Analysing musical audio, can we detect emotional involvement? This study hypothesises that if an improvisation contains more evidence of emotion-laden content, it is more likely to be recognised as creative. An embeddings-based method is proposed for capturing the emotional content in musical improvisations, using a psychologically-grounded classification of musical characteristics associated with emotions. Resulting 'emovectors' are analysed to test the above hypothesis, comparing across multiple improvisations. Capturing emotional content in this quantifiable way can contribute towards new metrics for creativity evaluation that can be applied at scale.",
    "paper_abstract_zh": "音乐即兴创作是很有趣的研究对象，它本质上是一种创造性过程的实时展示。在爵士乐中，音乐家经常在预先定义的和进行走位（lead sheets）上进行即兴创作。我们如何评估爵士即兴创作的创造力？我们能否将其捕捉为当前基于大型语言模型的生成系统的自动化创造力指标？情感表现与即兴创作中的创造力密切相关。通过分析音乐音频，我们能否检测到情感表现？本研究假设，如果一个即兴创作包含更多情感丰富的内容证据，它更有可能被认定为具有创造力。提出了一种基于嵌入的方法来捕捉音乐即兴创作中的情感内容，该方法使用与情感相关的音乐特征的心理分类基础。分析由此产生的'情感向量'以测试上述假设，并在多个即兴创作之间进行比较。以这种可量化的方式捕捉情感内容可以为创造力评价的新指标做出贡献，这些指标可以大规模应用。",
    "subjects": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)"
    ],
    "update_time": "2025-12-10",
    "paper_authors": "Anna Jordanous",
    "topic": [
      "Music Information Retrieval",
      "Audio Representation Learning"
    ],
    "category": [
      "Music"
    ]
  },
  {
    "paper_title": "PAVAS: Physics-Aware Video-to-Audio Synthesis",
    "paper_title_zh": "PAVAS: 物理感知的视频到音频合成",
    "paper_id": "2512.08282",
    "paper_abstract": "Recent advances in Video-to-Audio (V2A) generation have achieved impressive perceptual quality and temporal synchronization, yet most models remain appearance-driven, capturing visual-acoustic correlations without considering the physical factors that shape real-world sounds. We present Physics-Aware Video-to-Audio Synthesis (PAVAS), a method that incorporates physical reasoning into a latent diffusion-based V2A generation through the Physics-Driven Audio Adapter (Phy-Adapter). The adapter receives object-level physical parameters estimated by the Physical Parameter Estimator (PPE), which uses a Vision-Language Model (VLM) to infer the moving-object mass and a segmentation-based dynamic 3D reconstruction module to recover its motion trajectory for velocity computation. These physical cues enable the model to synthesize sounds that reflect underlying physical factors. To assess physical realism, we curate VGG-Impact, a benchmark focusing on object-object interactions, and introduce Audio-Physics Correlation Coefficient (APCC), an evaluation metric that measures consistency between physical and auditory attributes. Comprehensive experiments show that PAVAS produces physically plausible and perceptually coherent audio, outperforming existing V2A models in both quantitative and qualitative evaluations. Visit this https URL for demo videos.",
    "paper_abstract_zh": "最近在视频到音频（V2A）生成方面的进展已经取得了令人印象深刻的感知质量和时间同步性，但大多数模型仍然是外观驱动的，它们捕捉视觉-声学相关性而没有考虑塑造现实世界声音的物理因素。我们提出了物理感知的视频到音频合成（PAVAS），一种通过物理驱动的音频适配器（Phy-Adapter）将物理推理融入基于潜在扩散的V2A生成的方法。该适配器接收由物理参数估计器（PPE）估计的对象级物理参数，PPE使用视觉语言模型（VLM）推断移动物体的质量，并基于分割的动态3D重建模块恢复其运动轨迹以计算速度。这些物理线索使模型能够合成反映潜在物理因素的声音。为了评估物理真实性，我们策划了VGG-Impact基准，专注于对象-对象交互，并引入了音频-物理相关系数（APCC），这是一种衡量物理和听觉属性一致性的评估指标。全面的实验表明，PAVAS生成的物理上合理且感知上连贯的音频，在定量和定性评估中都优于现有的V2A模型。访问此https URL查看演示视频。",
    "subjects": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Multimedia (cs.MM)",
      "Sound (cs.SD)"
    ],
    "update_time": "2025-12-10",
    "paper_authors": "Oh Hyun-Bin, Yuhta Takida, Toshimitsu Uesaka, Tae-Hyun Oh, Yuki Mitsufuji",
    "topic": [
      "Video Generation",
      "Audio Representation Learning"
    ],
    "category": [
      "Image&Video"
    ]
  }
]