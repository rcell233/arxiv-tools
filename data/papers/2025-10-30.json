[
  {
    "paper_title": "EasyEyes: Online hearing research using speakers calibrated by phones",
    "paper_title_zh": "EasyEyes: 使用手机校准的扬声器进行在线听力研究",
    "paper_id": "2510.25048",
    "paper_abstract": "Hearing research requires a calibrated sound source, traditionally as lab equipment. Online research is quicker and more inclusive, but most participants lack calibration equipment and their sound sources are uncalibrated and diverse. This article explains how the open-source this http URL calibrates loudspeakers online. A library of smartphone-microphone profiles allows EasyEyes to use the participant's phone to calibrate their computer's loudspeaker in three minutes. Participants select their phone model, which is verified by screen size. Calibration employs the Novak et al. nonsynchronous maximum-length-sequence (MLS) algorithm. The computer's loudspeaker is corrected by convolving its input with the inverse of its impulse response. Researchers can contribute to the open-access library by calibrating phones with a measurement microphone. In the library, each profile is linked back to the profile used to produce it, back to the manufacturer profile of a measurement microphone. Correction accuracy is such that playing the flat-spectrum MLS through the corrected loudspeaker produces a nearly flat spectrum, with standard deviation less than 3 dB. A survey shows that a library of 94 phone models from major brands will support most participants in the USA (87%) and UK (80%). This method facilitates efficient and inclusive online hearing research.",
    "paper_abstract_zh": "听力研究需要校准过的声源，传统上使用实验室设备。在线研究更快且更具包容性，但大多数参与者缺乏校准设备，他们的声源未经校准且多样化。本文解释了开源项目EasyEyes如何在线校准扬声器。智能手机麦克风库允许EasyEyes使用参与者的手机在三分钟内校准其计算机扬声器。参与者选择其手机型号，该型号通过屏幕尺寸进行验证。校准采用Novak等人提出的非同步最大长度序列(MLS)算法。通过将计算机扬声器的输入与其脉冲响应的逆卷积来校正扬声器。研究人员可以通过使用测量麦克风校准手机来为开放访问库做出贡献。在库中，每个配置文件都链接回用于生成它的配置文件，并回溯到测量麦克风的制造商配置文件。校正精度如此之高，即通过校正后的扬声器播放平坦频谱的MLS会产生接近平坦的频谱，标准差小于3分贝。一项调查显示，包含来自主要品牌的94款手机型号的库将支持美国(87%)和英国(80%)的大多数参与者。这种方法促进了高效且包容的在线听力研究。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2025-10-30",
    "paper_authors": "Ivan Vican, Hugo De Moraes, Chongjun Liao, Nathnael H. Tsegaye, William O'Gara, Jasper Inamoto, Denis G. Pelli",
    "topic": [
      "Audio Representation Learning",
      "Other"
    ],
    "category": [
      "Other"
    ]
  },
  {
    "paper_title": "Retaining Mixture Representations for Domain Generalized Anomalous Sound Detection",
    "paper_title_zh": "保留混合表示的领域泛化异常声音检测",
    "paper_id": "2510.25182",
    "paper_abstract": "Anomalous sound detection (ASD) in the wild requires robustness to distribution shifts such as unseen low-SNR input mixtures of machine and noise types. State-of-the-art systems extract embeddings from an adapted audio encoder and detect anomalies via nearest-neighbor search, but fine tuning on noisy machine sounds often acts like a denoising objective, suppressing noise and reducing generalization under mismatched mixtures or inconsistent labeling. Training-free systems with frozen self-supervised learning (SSL) encoders avoid this issue and show strong first-shot generalization, yet their performance drops when mixture embeddings deviate from clean-source embeddings. We propose to improve SSL backbones with a retain-not-denoise strategy that better preserves information from mixed sound sources. The approach combines a multi-label audio tagging loss with a mixture alignment loss that aligns student mixture embeddings to convex teacher embeddings of clean and noise inputs. Controlled experiments on stationary, non-stationary, and mismatched noise subsets demonstrate improved robustness under distribution shifts, narrowing the gap toward oracle mixture representations.",
    "paper_abstract_zh": "野外异常声音检测（ASD）需要具备对分布偏移（如未见过的低信噪比机器与噪声混合输入）的鲁棒性。现有系统通过自适应音频编码器提取嵌入表示并采用近邻搜索进行异常检测，但在噪声机器声音上微调往往会起到降噪作用，抑制噪声的同时降低了对不匹配混合场景或标签不一致情况的泛化能力。无需训练的冻结自监督学习（SSL）编码器系统避免了这一问题并展现出优异的一次性泛化性能，但当混合嵌入偏离干净信号嵌入时其性能会下降。本文提出通过保留而非降噪策略改进SSL骨干网络，更好地保留混合声源信息。该方法结合多标签音频分类损失与混合对齐损失，将学生端混合嵌入与教师端干净信号和噪声输入的凸组合嵌入对齐。在平稳、非平稳及不匹配噪声子集上的受控实验证明了该方法在分布偏移下的鲁棒性提升，缩小了与理想混合表示之间的性能差距。",
    "subjects": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2025-10-30",
    "paper_authors": "Phurich Saengthong, Tomoya Nishida, Kota Dohi, Natsuo Yamashita, Yohei Kawaguchi",
    "topic": [
      "Audio Representation Learning",
      "Audio Classification"
    ],
    "category": [
      "Other"
    ]
  },
  {
    "paper_title": "Separating peripheral and higher-level effects on speech intelligibility using a hearing loss simulator and an objective intelligibility measure",
    "paper_title_zh": "使用听力损失模拟器和客观可懂度度量方法分离外周和高级过程对语音可懂度的影响",
    "paper_id": "2510.25235",
    "paper_abstract": "This paper presents a new method for separating the effects of peripheral hearing loss (HL) and higher-level processes on speech intelligibility (SI). In a previous study, we conducted an SI experiment with 14 older adult (OA) listeners, using speech-in-noise sounds that were either processed with an ideal ratio mask (IRM) enhancement technique or left unprocessed. The current study involved an SI experiment with 15 young, normal-hearing (YNH) listeners. This experiment used simulated HL sounds processed with the WHIS simulator that reflected the hearing level of a specific OA from the previous study. The results showed that the target OA's SI scores were higher than the average YNH scores. This implies that the target OA's higher-level processes may be more effective than those of the average YNH. To understand the characteristics of other OAs, we used the GESI objective intelligibility measure to predict SI. First, we confirmed that GESI could fairly accurately predict the SI scores for both the YNH and OA listeners. Next, we predicted the SI scores of the 14 OA listeners using the parameters estimated in the YNH experiment. The results showed that some OAs had higher SI scores than the average YNH, while one OA had lower scores. These differences in SI scores may reflect variations in the efficiency of higher-level this http URL results imply that WHIS and GESI could facilitate contrastive experiments between YNH and OA listeners, regardless of hearing level. This would allow us to study the effects of higher-level processes in OA listeners individually.",
    "paper_abstract_zh": "本文提出了一种新方法，用于分离外周听力损失（HL）和高级过程对语音可懂度（SI）的影响。在先前的研究中，我们对14名老年成人（OA）听众进行了SI实验，使用的是经过理想比例掩码（IRM）增强技术处理或未处理的噪声中语音。当前研究涉及15名年轻、正常听力（YNH）听众的SI实验。该实验使用了WHIS模拟器处理的模拟HL声音，这些声音反映了先前研究中特定OA的听力水平。结果表明，目标OA的SI得分高于YNH的平均得分。这意味着目标OA的高级过程可能比普通YNH更有效。为了解其他OA的特征，我们使用GESI客观可懂度度量方法来预测SI。首先，我们确认GESI能够相当准确地预测YNH和OA听众的SI得分。接下来，我们使用YNH实验中估计的参数预测了14名OA听众的SI得分。结果显示，一些OA的SI得分高于YNH的平均得分，而一名OA的得分较低。这些SI得分的差异可能反映了高级过程效率的变化。这些结果表明，无论听力水平如何，WHIS和GESI都可以促进YNH和OA听众之间的对比实验。这将使我们能够 individually 研究OA听众中高级过程的影响。",
    "subjects": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2025-10-30",
    "paper_authors": "Toshio Irino, Ayako Yamamoto, Fuki Miyazaki",
    "topic": [
      "Speech Recognition",
      "Speech Enhancement"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "PitchFlower: A flow-based neural audio codec with pitch controllability",
    "paper_title_zh": "PitchFlower：一种具有音高可控性的基于流模型的神经音频编解码器",
    "paper_id": "2510.25566",
    "paper_abstract": "We present PitchFlower, a flow-based neural audio codec with explicit pitch controllability. Our approach enforces disentanglement through a simple perturbation: during training, F0 contours are flattened and randomly shifted, while the true F0 is provided as conditioning. A vector-quantization bottleneck prevents pitch recovery, and a flow-based decoder generates high quality audio. Experiments show that PitchFlower achieves more accurate pitch control than WORLD at much higher audio quality, and outperforms SiFiGAN in controllability while maintaining comparable quality. Beyond pitch, this framework provides a simple and extensible path toward disentangling other speech attributes.",
    "paper_abstract_zh": "我们提出了一种具有显式音高可控性的基于流模型的神经音频编解码器。我们的方法通过简单的扰动实现特征解耦：在训练过程中，基频轮廓被平滑化并随机偏移，同时提供真实基频作为条件信息。向量量化瓶颈阻止了音高的恢复，而基于流模型的解码器生成高质量音频。实验表明，PitchFlower在音高控制精度上优于WORLD系统，同时保持了更高的音频质量，并且在可控性方面超越了SiFiGAN，同时维持了相当的音质水平。除了音高控制外，该框架为语音其他属性的解耦提供了简单且可扩展的实现路径。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)",
      "Machine Learning (cs.LG)"
    ],
    "update_time": "2025-10-30",
    "paper_authors": "Diego Torres, Axel Roebel, Nicolas Obin",
    "topic": [
      "Audio Codec",
      "Audio Representation Learning"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Lost in Phonation: Voice Quality Variation as an Evaluation Dimension for Speech Foundation Models",
    "paper_title_zh": "发声中的迷失：语音质量变化作为语音基础模型的评估维度",
    "paper_id": "2510.25577",
    "paper_abstract": "Recent advances in speech foundation models (SFMs) have enabled the direct processing of spoken language from raw audio, bypassing intermediate textual representations. This capability allows SFMs to be exposed to, and potentially respond to, rich paralinguistic variations embedded in the input speech signal. One under-explored dimension of paralinguistic variation is voice quality, encompassing phonation types such as creaky and breathy voice. These phonation types are known to influence how listeners infer affective state, stance and social meaning in speech. Existing benchmarks for speech understanding largely rely on multiple-choice question answering (MCQA) formats, which are prone to failure and therefore unreliable in capturing the nuanced ways paralinguistic features influence model behaviour. In this paper, we probe SFMs through open-ended generation tasks and speech emotion recognition, evaluating whether model behaviours are consistent across different phonation inputs. We introduce a new parallel dataset featuring synthesized modifications to voice quality, designed to evaluate SFM responses to creaky and breathy voice. Our work provides the first examination of SFM sensitivity to these particular non-lexical aspects of speech perception.",
    "paper_abstract_zh": "近年来语音基础模型（SFMs）的发展使得其能够直接处理原始音频中的口语内容，无需经过中间文本表示。这种能力使SFMs能够接触到输入语音信号中丰富的副语言特征变化，并可能对其做出响应。副语言特征变化中一个尚未充分探索的维度是语音质量，包括声门化和气声等发声类型。这些发声类型已知会影响听者对言语中情感状态、立场和社会意义的推断。现有的语音理解评估基准大多依赖于多项选择题回答（MCQA）格式，这种格式容易失效，因此在捕捉副语言特征对模型行为的细微影响方面不可靠。本文通过开放式生成任务和语音情感识别来探究SFMs，评估模型行为在不同发声输入下是否具有一致性。我们引入了一个新的平行数据集，包含对语音质量的合成修改，用于评估SFM对声门化和气声的响应。我们的工作首次考察了SFM对这些特定非词汇性语音感知特征的敏感性。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ],
    "update_time": "2025-10-30",
    "paper_authors": "Harm Lameris, Shree Harsha Bokkahalli Satish, Joakim Gustafson, Éva Székely",
    "topic": [
      "Audio Representation Learning",
      "Speech Recognition",
      "Speech Synthesis"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Evaluating Emotion Recognition in Spoken Language Models on Emotionally Incongruent Speech",
    "paper_title_zh": "情感不一致语音中语音语言模型的情感识别评估",
    "paper_id": "2510.25054",
    "paper_abstract": "Advancements in spoken language processing have driven the development of spoken language models (SLMs), designed to achieve universal audio understanding by jointly learning text and audio representations for a wide range of tasks. Although promising results have been achieved, there is growing discussion regarding these models' generalization capabilities and the extent to which they truly integrate audio and text modalities in their internal representations. In this work, we evaluate four SLMs on the task of speech emotion recognition using a dataset of emotionally incongruent speech samples, a condition under which the semantic content of the spoken utterance conveys one emotion while speech expressiveness conveys another. Our results indicate that SLMs rely predominantly on textual semantics rather than speech emotion to perform the task, indicating that text-related representations largely dominate over acoustic representations. We release both the code and the Emotionally Incongruent Synthetic Speech dataset (EMIS) to the community.",
    "paper_abstract_zh": "语音处理技术的进步推动了语音语言模型（SLMs）的发展，这类模型通过联合学习文本和音频表示以实现广泛的音频理解任务。尽管已取得显著成果，但关于这些模型的泛化能力以及其内部表示是否真正融合了音频与文本模态的讨论日益增多。本研究通过使用情感不一致语音样本数据集，在语音情感识别任务上评估了四种SLMs。在该数据集中，口语内容传达的情感与语音表达的情感不一致。实验结果表明，SLMs主要依赖文本语义而非语音情感完成任务，说明文本相关表征在模型决策中占据主导地位。我们向社区公开了代码及情感不一致合成语音数据集（EMIS）。",
    "subjects": [
      "Computation and Language (cs.CL)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2025-10-30",
    "paper_authors": "Pedro Corrêa, João Lima, Victor Moreno, Paula Dornhofer Paro Costa",
    "topic": [
      "Audio Representation Learning",
      "Audio Classification"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Joint Analysis of Acoustic Scenes and Sound Events Based on Semi-Supervised Training of Sound Events With Partial Labels",
    "paper_title_zh": "基于声音事件部分标签的半监督训练的声学场景与声音事件联合分析",
    "paper_id": "2510.25075",
    "paper_abstract": "Annotating time boundaries of sound events is labor-intensive, limiting the scalability of strongly supervised learning in audio detection. To reduce annotation costs, weakly-supervised learning with only clip-level labels has been widely adopted. As an alternative, partial label learning offers a cost-effective approach, where a set of possible labels is provided instead of exact weak annotations. However, partial label learning for audio analysis remains largely unexplored. Motivated by the observation that acoustic scenes provide contextual information for constructing a set of possible sound events, we utilize acoustic scene information to construct partial labels of sound events. On the basis of this idea, in this paper, we propose a multitask learning framework that jointly performs acoustic scene classification and sound event detection with partial labels of sound events. While reducing annotation costs, weakly-supervised and partial label learning often suffer from decreased detection performance due to lacking the precise event set and their temporal annotations. To better balance between annotation cost and detection performance, we also explore a semi-supervised framework that leverages both strong and partial labels. Moreover, to refine partial labels and achieve better model training, we propose a label refinement method based on self-distillation for the proposed approach with partial labels.",
    "paper_abstract_zh": "标注声音事件的时间边界是劳动密集型的，这限制了强监督学习在音频检测中的可扩展性。为了减少标注成本，仅使用片段级标签的弱监督学习已被广泛采用。作为一种替代方案，部分标签学习提供了一种经济高效的方法，即提供一组可能的标签而非精确的弱标注。然而，音频分析中的部分标签学习在很大程度上仍未被探索。受声学场景为构建可能的声音事件集提供上下文信息的启发，我们利用声学场景信息来构建声音事件的部分标签。基于这一想法，本文提出了一个多任务学习框架，联合执行声学场景分类和声音事件检测（使用声音事件的部分标签）。在减少标注成本的同时，弱监督和部分标签学习通常由于缺乏精确的事件集及其时间标注而导致检测性能下降。为了更好地平衡标注成本和检测性能，我们还探索了一种结合强标签和部分标签的半监督框架。此外，为了优化部分标签并实现更好的模型训练，我们提出了一种基于自蒸馏的标签优化方法，用于处理部分标签的 proposed 方法。",
    "subjects": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2025-10-30",
    "paper_authors": "Keisuke Imoto",
    "topic": [
      "Audio Classification",
      "Audio Representation Learning"
    ],
    "category": [
      "Other"
    ]
  },
  {
    "paper_title": "SFMS-ALR: Script-First Multilingual Speech Synthesis with Adaptive Locale Resolution",
    "paper_title_zh": "SFMS-ALR：基于脚本优先的多语言语音合成自适应区域解析",
    "paper_id": "2510.25178",
    "paper_abstract": "Intra-sentence multilingual speech synthesis (code-switching TTS) remains a major challenge due to abrupt language shifts, varied scripts, and mismatched prosody between languages. Conventional TTS systems are typically monolingual and fail to produce natural, intelligible speech in mixed-language contexts. We introduce Script-First Multilingual Synthesis with Adaptive Locale Resolution (SFMS-ALR), an engine-agnostic framework for fluent, real-time code-switched speech generation. SFMS-ALR segments input text by Unicode script, applies adaptive language identification to determine each segment's language and locale, and normalizes prosody using sentiment-aware adjustments to preserve expressive continuity across languages. The algorithm generates a unified SSML representation with appropriate \"lang\" or \"voice\" spans and synthesizes the utterance in a single TTS request. Unlike end-to-end multilingual models, SFMS-ALR requires no retraining and integrates seamlessly with existing voices from Google, Apple, Amazon, and other providers. Comparative analysis with data-driven pipelines such as Unicom and Mask LID demonstrates SFMS-ALR's flexibility, interpretability, and immediate deployability. The framework establishes a modular baseline for high-quality, engine-independent multilingual TTS and outlines evaluation strategies for intelligibility, naturalness, and user preference.",
    "paper_abstract_zh": "句子内多语言语音合成（代码切换TTS）由于语言突变、脚本差异以及语言间韵律不匹配等问题，仍面临重大挑战。传统TTS系统通常为单语言架构，在混合语言场景中难以生成自然流畅的语音。我们提出了一种与引擎无关的脚本优先多语言语音合成自适应区域解析框架（SFMS-ALR），可实现流畅实时的代码切换语音生成。该框架通过Unicode脚本分割输入文本，采用自适应语言识别确定各段落的语言和区域属性，并通过情感感知的韵律归一化方法保持跨语言表达连贯性。算法生成包含适当'lang'或'voice'标签的统一SSML表示法，并通过单次TTS请求合成语音。与端到端多语言模型不同，SFMS-ALR无需重新训练，可无缝集成Google、Apple、Amazon等语音服务提供商的现有语音库。与Unicom和Mask LID等数据驱动管道的对比分析表明，SFMS-ALR具有灵活性、可解释性和即时部署性。该框架为高质量、引擎无关的多语言TTS建立了模块化基线，并提出了可懂度、自然度和用户偏好评估策略。",
    "subjects": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)",
      "Artificial Intelligence (cs.AI)"
    ],
    "update_time": "2025-10-30",
    "paper_authors": "Dharma Teja Donepudi",
    "topic": [
      "Speech Synthesis"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Controlling Contrastive Self-Supervised Learning with Knowledge-Driven Multiple Hypothesis: Application to Beat Tracking",
    "paper_title_zh": "利用知识驱动的多假设控制对比自监督学习：应用于节拍跟踪",
    "paper_id": "2510.25560",
    "paper_abstract": "Ambiguities in data and problem constraints can lead to diverse, equally plausible outcomes for a machine learning task. In beat and downbeat tracking, for instance, different listeners may adopt various rhythmic interpretations, none of which would necessarily be incorrect. To address this, we propose a contrastive self-supervised pre-training approach that leverages multiple hypotheses about possible positive samples in the data. Our model is trained to learn representations compatible with different such hypotheses, which are selected with a knowledge-based scoring function to retain the most plausible ones. When fine-tuned on labeled data, our model outperforms existing methods on standard benchmarks, showcasing the advantages of integrating domain knowledge with multi-hypothesis selection in music representation learning in particular.",
    "paper_abstract_zh": "数据和问题约束中的模糊性可能导致机器学习任务产生多样且同样合理的结果。例如，在节拍和强拍跟踪中，不同的听众可能会采用各种节奏解释，这些解释中没有一种是必然错误的。为解决这一问题，我们提出了一种对比自监督预训练方法，该方法利用数据中可能正样本的多种假设。我们的模型被训练为学习与不同假设兼容的表示，这些假设通过基于知识的评分函数选择，以保留最合理的假设。在有标签数据上进行微调时，我们的模型在标准基准上优于现有方法，展示了在音乐表示学习中整合领域知识与多假设选择的优势。",
    "subjects": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2025-10-30",
    "paper_authors": "Antonin Gagnere, Slim Essid, Geoffroy Peeters",
    "topic": [
      "Audio Representation Learning",
      "Music Information Retrieval"
    ],
    "category": [
      "Music"
    ]
  },
  {
    "paper_title": "A Parameter-Efficient Multi-Scale Convolutional Adapter for Synthetic Speech Detection",
    "paper_title_zh": "一种参数高效的多尺度卷积适配器用于合成语音检测",
    "paper_id": "2510.24852",
    "paper_abstract": "Recent synthetic speech detection models typically adapt a pre-trained SSL model via finetuning, which is computationally demanding. Parameter-Efficient Fine-Tuning (PEFT) offers an alternative. However, existing methods lack the specific inductive biases required to model the multi-scale temporal artifacts characteristic of spoofed audio. This paper introduces the Multi-Scale Convolutional Adapter (MultiConvAdapter), a parameter-efficient architecture designed to address this limitation. MultiConvAdapter integrates parallel convolutional modules within the SSL encoder, facilitating the simultaneous learning of discriminative features across multiple temporal resolutions, capturing both short-term artifacts and long-term distortions. With only $3.17$M trainable parameters ($1\\%$ of the SSL backbone), MultiConvAdapter substantially reduces the computational burden of adaptation. Evaluations on five public datasets, demonstrate that MultiConvAdapter achieves superior performance compared to full fine-tuning and established PEFT methods.",
    "paper_abstract_zh": "最近的合成语音检测模型通常通过微调预训练的自监督学习(SSL)模型来实现，这需要大量计算资源。参数高效微调(PEFT)提供了一种替代方案。然而，现有方法缺乏建模欺骗音频特征的多尺度时间伪影所需的特定归纳偏置。本文介绍了多尺度卷积适配器(MultiConvAdapter)，这是一种参数高效架构，旨在解决这一局限性。MultiConvAdapter在SSL编码器中集成了并行卷积模块，促进在多个时间分辨率上同时学习判别性特征，既能捕捉短期伪影，也能捕捉长期失真。仅需3.17M可训练参数(占SSL主干网络的1%)，MultiConvAdapter显著降低了适应的计算负担。在五个公共数据集上的评估表明，与全量微调和既定的PEFT方法相比，MultiConvAdapter实现了更优的性能。",
    "subjects": [
      "Sound (cs.SD)"
    ],
    "update_time": "2025-10-30",
    "paper_authors": "Yassine El Kheir, Fabian Ritter-Guttierez, Arnab Das, Tim Polzehl, Sebastian Möller",
    "topic": [
      "Audio Classification",
      "Speech Synthesis"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Studies for : A Human-AI Co-Creative Sound Artwork Using a Real-time Multi-channel Sound Generation Model",
    "paper_title_zh": "Studies for：基于实时多通道声音生成模型的人机协同声音艺术创作研究",
    "paper_id": "2510.25228",
    "paper_abstract": "This paper explores the integration of AI technologies into the artistic workflow through the creation of Studies for, a generative sound installation developed in collaboration with sound artist Evala (this https URL). The installation employs SpecMaskGIT, a lightweight yet high-quality sound generation AI model, to generate and playback eight-channel sound in real-time, creating an immersive auditory experience over the course of a three-month exhibition. The work is grounded in the concept of a \"new form of archive,\" which aims to preserve the artistic style of an artist while expanding beyond artists' past artworks by continued generation of new sound elements. This speculative approach to archival preservation is facilitated by training the AI model on a dataset consisting of over 200 hours of Evala's past sound artworks.\nBy addressing key requirements in the co-creation of art using AI, this study highlights the value of the following aspects: (1) the necessity of integrating artist feedback, (2) datasets derived from an artist's past works, and (3) ensuring the inclusion of unexpected, novel outputs. In Studies for, the model was designed to reflect the artist's artistic identity while generating new, previously unheard sounds, making it a fitting realization of the concept of \"a new form of archive.\" We propose a Human-AI co-creation framework for effectively incorporating sound generation AI models into the sound art creation process and suggest new possibilities for creating and archiving sound art that extend an artist's work beyond their physical existence. Demo page: this https URL",
    "paper_abstract_zh": "本文通过与声音艺术家Evala合作创作的生成式声音装置《Studies for》探索了人工智能技术在艺术创作流程中的整合应用。该装置采用SpecMaskGIT这一轻量级高质量声音生成AI模型，实现实时八通道声音生成与播放，在为期三个月的展览中营造沉浸式听觉体验。作品基于'新型档案'概念构建，旨在保存艺术家创作风格的同时，通过持续生成新的声音元素超越艺术家过往作品的范畴。这种档案保存的 speculative 方法通过将包含Evala超过200小时过往声音作品的数据集训练AI模型得以实现。本研究通过解决AI参与艺术共创的关键需求，凸显了以下价值：(1)艺术家反馈整合的必要性；(2)基于艺术家过往作品的数据集构建；(3)确保包含意外性、新颖性的输出结果。在《Studies for》中，模型既反映艺术家创作身份，又能生成前所未闻的声音，完美诠释了'新型档案'概念。我们提出了一种人机协同创作框架，有效将声音生成AI模型融入声音艺术创作流程，并建议了延伸艺术家物理存在之外的声音艺术创作与档案保存新可能。演示页面：this https URL",
    "subjects": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)"
    ],
    "update_time": "2025-10-30",
    "paper_authors": "Chihiro Nagashima, Akira Takahashi, Zhi Zhong, Shusuke Takahashi, Yuki Mitsufuji",
    "topic": [
      "Audio Representation Learning",
      "Music Generation"
    ],
    "category": [
      "Music"
    ]
  },
  {
    "paper_title": "Binaspect -- A Python Library for Binaural Audio Analysis, Visualization & Feature Generation",
    "paper_title_zh": "",
    "paper_id": "2510.25714",
    "paper_abstract": "We present Binaspect, an open-source Python library for binaural audio analysis, visualization, and feature generation. Binaspect generates interpretable \"azimuth maps\" by calculating modified interaural time and level difference spectrograms, and clustering those time-frequency (TF) bins into stable time-azimuth histogram representations. This allows multiple active sources to appear as distinct azimuthal clusters, while degradations manifest as broadened, diffused, or shifted distributions. Crucially, Binaspect operates blindly on audio, requiring no prior knowledge of head models. These visualizations enable researchers and engineers to observe how binaural cues are degraded by codec and renderer design choices, among other downstream processes. We demonstrate the tool on bitrate ladders, ambisonic rendering, and VBAP source positioning, where degradations are clearly revealed. In addition to their diagnostic value, the proposed representations can be exported as structured features suitable for training machine learning models in quality prediction, spatial audio classification, and other binaural tasks. Binaspect is released under an open-source license with full reproducibility scripts at this https URL.",
    "paper_abstract_zh": "",
    "subjects": [
      "Sound (cs.SD)"
    ],
    "update_time": "2025-10-30",
    "paper_authors": "Dan Barry, Davoud Shariat Panah, Alessandro Ragano, Jan Skoglund, Andrew Hines",
    "topic": [],
    "category": []
  },
  {
    "paper_title": "Efficient Vocal Source Separation Through Windowed Sink Attention",
    "paper_title_zh": "基于窗口化Sink Attention的高效人声分离",
    "paper_id": "2510.25745",
    "paper_abstract": "State-of-the-art vocal separation models like Mel-Band-Roformer rely on full temporal self-attention mechanisms, where each temporal frame interacts with every other frames. This incurs heavy computational costs that scales quadratically with input audio length, motivating chunking and windowing approaches. Through analysis of a pre-trained vocal separation model, we discovered that temporal attention patterns are highly localized. Building on this insight, we replaced full attention with windowed sink attention (WSA) with small temporal attention window and attention sinks. We show empirically that fine-tuning from the original checkpoint recovers 92% of the original SDR performance while reducing FLOPs by 44.5x. We release our code and checkpoints under MIT license at this https URL.",
    "paper_abstract_zh": "最先进的人声分离模型（如Mel-Band-Roformer）依赖于全时序自注意力机制，其中每个时间帧与所有其他帧进行交互。这导致了随输入音频长度二次方增长的高计算成本，促使采用分块和窗口化方法。通过对预训练的人声分离模型的分析，我们发现时序注意力模式具有高度局部化的特点。基于这一见解，我们用具有小时间注意力窗口和注意力接收器（attention sinks）的窗口化Sink Attention（WSA）替代了全注意力。实验证明，从原始检查点微调可恢复92%的原始SDR性能，同时将FLOPs减少44.5倍。我们在MIT许可证下发布了代码和检查点，链接为https URL。",
    "subjects": [
      "Sound (cs.SD)"
    ],
    "update_time": "2025-10-30",
    "paper_authors": "Christodoulos Benetatos, Yongyi Zang, Randal Leistikow",
    "topic": [
      "Speech Enhancement",
      "Audio Representation Learning"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "State Space and Self-Attention Collaborative Network with Feature Aggregation for DOA Estimation",
    "paper_title_zh": "具有特征聚合的状态空间和自注意力协同网络用于DOA估计",
    "paper_id": "2510.25193",
    "paper_abstract": "Accurate direction-of-arrival (DOA) estimation for sound sources is challenging due to the continuous changes in acoustic characteristics across time and frequency. In such scenarios, accurate localization relies on the ability to aggregate relevant features and model temporal dependencies effectively. In time series modeling, achieving a balance between model performance and computational efficiency remains a significant challenge. To address this, we propose FA-Stateformer, a state space and self-attention collaborative network with feature aggregation. The proposed network first employs a feature aggregation module to enhance informative features across both temporal and spectral dimensions. This is followed by a lightweight Conformer architecture inspired by the squeeze-and-excitation mechanism, where the feedforward layers are compressed to reduce redundancy and parameter overhead. Additionally, a temporal shift mechanism is incorporated to expand the receptive field of convolutional layers while maintaining a compact kernel size. To further enhance sequence modeling capabilities, a bidirectional Mamba module is introduced, enabling efficient state-space-based representation of temporal dependencies in both forward and backward directions. The remaining self-attention layers are combined with the Mamba blocks, forming a collaborative modeling framework that achieves a balance between representation capacity and computational efficiency. Extensive experiments demonstrate that FA-Stateformer achieves superior performance and efficiency compared to conventional architectures.",
    "paper_abstract_zh": "由于声学特性在时间和频率上的连续变化，声源到达方向(DOA)的准确估计具有挑战性。在这种情况下，准确的定位依赖于聚合相关特征和有效建模时间依赖性的能力。在时间序列建模中，在模型性能和计算效率之间取得平衡仍然是一个重大挑战。为此，我们提出了FA-Stateformer，一种具有特征聚合的状态空间和自注意力协同网络。该网络首先采用特征聚合模块来增强时间和频谱维度上的信息特征。随后，受挤压-激励机制启发的轻量级Conformer架构被采用，其中前馈层被压缩以减少冗余和参数开销。此外，还引入了时间移位机制，以扩展卷积层的感受野，同时保持紧凑的核大小。为了进一步增强序列建模能力，引入了双向Mamba模块，能够以基于状态空间的方式高效地表示前向和后向的时间依赖性。剩余的自注意力层与Mamba块相结合，形成一个协同建模框架，实现了表示能力和计算效率之间的平衡。大量实验表明，与传统架构相比，FA-Stateformer实现了卓越的性能和效率。",
    "subjects": [
      "Signal Processing (eess.SP)",
      "Sound (cs.SD)"
    ],
    "update_time": "2025-10-30",
    "paper_authors": "Qi You, Qinghua Huang, Yi-Cheng Lin",
    "topic": [
      "Audio Representation Learning"
    ],
    "category": [
      "Speech"
    ]
  }
]