[
  {
    "paper_title": "Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation",
    "paper_title_zh": "重新审视用于学习通用音频表示的音频语言预训练",
    "paper_id": "2511.16757",
    "paper_abstract": "Audio-language pretraining holds promise for general-purpose audio understanding, yet remains underexplored compared to its vision counterpart. While vision-language models like CLIP serve as widely adopted foundations, existing audio-language models primarily excel at retrieval tasks with limited adoption as general-purpose encoders. We identify three key barriers: limited large-scale audio-text corpora, insufficient caption diversity, and lack of systematic exploration and evaluation. To this end, we introduce CaptionStew, a 10.7M caption dataset aggregating diverse open-source audio-text corpora across multiple domains and captioning styles. Using this resource, we conduct the first comprehensive evaluation comparing contrastive and captioning objectives for audio representation learning across speech, music, and environmental sound tasks. Our results demonstrate that audio-language pretraining yields competitive, transferable representations. Through systematic data-scaling experiments, we reveal complementary objective strengths: contrastive learning achieves superior data efficiency at smaller scales, while captioning demonstrates better scalability on language-involved audio understanding tasks. We also find that common supervised initialization practices provide diminishing returns at scale, challenging current approaches. These findings establish audio-language pretraining as a viable pathway toward general-purpose audio representations, guiding future research. To accelerate progress, we release data preparation recipes, training protocols, and pretrained models, paving the way toward universal audio understanding.",
    "paper_abstract_zh": "音频语言预训练在通用音频理解方面具有潜力，但相比其视觉对应领域，研究仍然不足。虽然像CLIP这样的视觉语言模型被广泛采用作为基础，但现有的音频语言模型主要在检索任务上表现出色，而作为通用编码器的应用有限。我们确定了三个关键障碍：大规模音频文本语料库有限、字幕多样性不足，以及缺乏系统性的探索和评估。为此，我们引入了CaptionStew，这是一个包含1070万条字幕的数据集，汇集了多个领域和字幕风格的多样化开源音频文本语料库。利用这一资源，我们进行了首次全面评估，比较了对比学习和字幕生成目标在语音、音乐和环境声音任务中的音频表示学习效果。我们的结果表明，音频语言预训练能够产生具有竞争力的、可迁移的表示。通过系统的数据扩展实验，我们揭示了不同目标的互补优势：对比学习在较小规模下表现出更高的数据效率，而字幕生成在涉及语言理解的音频任务上显示出更好的可扩展性。我们还发现，常见的监督初始化实践在规模扩大时收益递减，挑战了当前的方法。这些发现确立了音频语言预训练作为通用音频表示的可行途径，为未来研究提供了指导。为了加速进展，我们发布了数据准备方案、训练协议和预训练模型，为通用音频理解铺平了道路。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)",
      "Artificial Intelligence (cs.AI)"
    ],
    "update_time": "2025-11-24",
    "paper_authors": "Wei-Cheng Tseng, Xuanru Zhou, Mingyue Huo, Yiwen Shao, Hao Zhang, Dong Yu",
    "topic": [
      "Audio Representation Learning",
      "Music Information Retrieval",
      "Speech Recognition"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM",
    "paper_title_zh": "使用集成多模态大语言模型的长上下文Q-Former进行机器人确认生成与动作规划",
    "paper_id": "2511.17335",
    "paper_abstract": "Human-robot collaboration towards a shared goal requires robots to understand human action and interaction with the surrounding environment. This paper focuses on human-robot interaction (HRI) based on human-robot dialogue that relies on the robot action confirmation and action step generation using multimodal scene understanding. The state-of-the-art approach uses multimodal transformers to generate robot action steps aligned with robot action confirmation from a single clip showing a task composed of multiple micro steps. Although actions towards a long-horizon task depend on each other throughout an entire video, the current approaches mainly focus on clip-level processing and do not leverage long-context information. This paper proposes a long-context Q-former incorporating left and right context dependency in full videos. Furthermore, this paper proposes a text-conditioning approach to feed text embeddings directly into the LLM decoder to mitigate the high abstraction of the information in text by Q-former. Experiments with the YouCook2 corpus show that the accuracy of confirmation generation is a major factor in the performance of action planning. Furthermore, we demonstrate that the long-context Q-former improves the confirmation and action planning by integrating VideoLLaMA3.",
    "paper_abstract_zh": "朝着共同目标进行的人机协作需要机器人理解人类动作及其与周围环境的互动。本文专注于基于人机对话的人机交互（HRI），该交互依赖于机器人动作确认和动作步骤生成，并使用多模态场景理解。最先进的方法使用多模态transformer从展示由多个微步骤组成的任务的单一片段中生成与机器人动作确认对齐的机器人动作步骤。然而，针对长期目标的动作在整个视频中相互依赖，而当前方法主要关注片段级处理，并未充分利用长上下文信息。本文提出了一种长上下文Q-Former，它整合了完整视频中的左右上下文依赖关系。此外，本文提出了一种文本条件化方法，将文本嵌入直接输入到LLM解码器中，以减轻Q-Former中文本信息的高度抽象性。在YouCook2语料库上的实验表明，确认生成的准确性是动作规划性能的主要因素。此外，我们证明了通过集成VideoLLaMA3，长上下文Q-Former能够改进确认和动作规划。",
    "subjects": [
      "Sound (cs.SD)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computation and Language (cs.CL)",
      "Robotics (cs.RO)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2025-11-24",
    "paper_authors": "Chiori Hori, Yoshiki Masuyama, Siddarth Jain, Radu Corcodel, Devesh Jha, Diego Romeres, Jonathan Le Roux",
    "topic": [
      "Other"
    ],
    "category": [
      "Other"
    ]
  },
  {
    "paper_title": "The Artist is Present: Traces of Artists Resigind and Spawning in Text-to-Audio AI",
    "paper_title_zh": "艺术家在场：文本到音频AI中艺术家痕迹的识别与生成",
    "paper_id": "2511.17404",
    "paper_abstract": "Text-to-audio (TTA) systems are rapidly transforming music creation and distribution, with platforms like Udio and Suno generating thousands of tracks daily and integrating into mainstream music platforms and ecosystems. These systems, trained on vast and largely undisclosed datasets, are fundamentally reshaping how music is produced, reproduced and consumed. This paper presents empirical evidence that artist-conditioned regions can be systematically microlocated through metatag-based prompt design, effectively enabling the spawning of artist-like content through strategic prompt engineering. Through systematic exploration of metatag-based prompt engineering techniques this research reveals how users can access the distinctive sonic signatures of specific artists, evidencing their inclusion in training datasets. Using descriptor constellations drawn from public music taxonomies, the paper demonstrates reproducible proximity to artists such as Bon Iver, Philip Glass, Panda Bear and William Basinski. The results indicate stable text-audio correspondences consistent with artist-specific training signals, enabling precise traversal of stylistic microlocations without explicitly naming artists. This capacity to summon artist-specific outputs shows that artists' creative works fuction as foundational material from which these systems generate new content, often without explicit consent or attribuition. Conceptually, the work clarifies how textual descriptors act as navigational cues in high-dimensional representation spaces; methodologically, it provides a replicable protocol for auditing stylistic inducibility. The findings raise immediate queestions for governance-attribution, consent and disclosure standards-and for creative practice, where induced stylistic proximity complicates boundaries between ownership, reproduction, imitation, creative agency and the ethics of algorithmic creation.",
    "paper_abstract_zh": "文本到音频（TTA）系统正在迅速改变音乐创作和分发方式，像Udio和Suno这样的平台每天生成数千首曲目，并整合到主流音乐平台和生态系统中。这些系统基于庞大且大多未公开的数据集进行训练，从根本上改变了音乐的制作、复制和消费方式。本文提供了实证证据，表明基于元标签的提示设计可以系统性地微定位艺术家条件区域，从而通过战略性提示工程有效地生成类艺术家内容。通过对基于元标签的提示工程技术的系统性探索，本研究揭示了用户如何获取特定艺术家的独特声音特征，证明这些艺术家已被纳入训练数据集。论文使用来自公共音乐分类学的描述符星座，展示了与Bon Iver、Philip Glass、Panda Bear和William Basinski等艺术家可重复的接近性。结果表明，与艺术家特定的训练信号一致的稳定的文本-音频对应关系，使用户能够在不明确提及艺术家的情况下精确遍历风格微定位。这种能够唤起艺术家特定输出的能力表明，艺术家的创作作品构成了这些系统生成新内容的基础材料，通常没有明确的同意或归属。从概念上讲，这项工作阐明了文本描述符如何作为高维表示空间中的导航线索；从方法上讲，它提供了一种可复制的协议，用于审计风格诱导性。研究结果立即提出了关于治理、归属、同意和披露标准的问题，以及关于创作实践的问题，因为诱导的风格接近性模糊了所有权、复制、模仿、创作能动性和算法创作伦理之间的界限。",
    "subjects": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2025-11-24",
    "paper_authors": "Guilherme Coelho",
    "topic": [
      "Music Generation",
      "Music Information Retrieval",
      "Audio Representation Learning"
    ],
    "category": [
      "Music"
    ]
  },
  {
    "paper_title": "AI in Music and Sound: Pedagogical Reflections, Post-Structuralist Approaches and Creative Outcomes in Seminar Practice",
    "paper_title_zh": "音乐与声音中的AI：教学反思、后结构主义方法与研讨实践中的创造性成果",
    "paper_id": "2511.17425",
    "paper_abstract": "This paper presents a pedagogical and conceptual account of the course AI in Music and Sound: Modalities, Tools and Creative Applications, offered within the Music Informatics and Media Art module of an this http URL. in Audio Communication. The course engaged students with a range of AI modalities such as symbolic composition, voice synthesis, timbre transfer, neural audio synthesis, and text-to-audio systems, combining theoretical reflection with practice-based experimentation. Its central pedagogical move is a paired-études design: each modality is approached first through its intended affordances and then through a deliberately reframed or \"misused\" exercise that surfaces representational limits and alternative behaviours. Framed by medium theory and post-structuralist inquiry, we treated AI as a transmodal conduit-a system that translates and perturbs musical signs across textual, symbolic, timbral and audio domains. Evidence from student work and reflection indicates growth in technical fluency, medium awareness, and critical literacy, alongside the cultivation of experimental method and process-oriented listening. The paper outlines the course architecture, assessment design, and representative projects, and distils a set of design patterns for AI-music pedagogy (eg., prompt-conditioned interplays and semantic destabilisation in text-to-audio; latent space materialism in timbre transfer). It concludes with pedagogical recommendations that integrate creative practice with medium awareness and with cultural-epistemic analysis of AI technologies, preparing students to participate in how AI is understood, developed, and deployed with creative communities.",
    "paper_abstract_zh": "本文介绍了课程《音乐与声音中的AI：模态、工具与创造性应用》的教学与概念性概述，该课程是音频传播硕士项目中音乐信息学与媒体艺术模块的一部分。课程引导学生接触多种AI模态，如符号作曲、语音合成、音色迁移、神经音频合成和文本到音频系统，结合理论反思与实践实验。其核心教学设计是“双练习”模式：首先通过预期功能探索每种模态，然后通过刻意重构或“误用”练习揭示其表征局限和替代行为。基于媒介理论和后结构主义探究，我们将AI视为跨模态通道——在文本、符号、音色和音频领域之间翻译和扰动音乐符号的系统。学生作品和反思的证据显示，学生在技术熟练度、媒介意识和批判性素养方面有所提升，同时培养了实验方法和过程导向的倾听能力。本文概述了课程架构、评估设计和代表性项目，并提炼出一套AI音乐教学的设计模式（例如，文本到音频中的提示条件交互和语义不稳定；音色迁移中的潜在空间唯物主义）。最后，本文提出了将创造性实践与媒介意识及AI技术的文化-认识论分析相结合的教学建议，帮助学生参与AI在创意社区中的理解、开发和部署。",
    "subjects": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2025-11-24",
    "paper_authors": "Guilherme Coelho",
    "topic": [
      "Music Generation",
      "Music Information Retrieval"
    ],
    "category": [
      "Music"
    ]
  },
  {
    "paper_title": "Semantic and Semiotic Interplays in Text-to-Audio AI: Exploring Cognitive Dynamics and Musical Interactions",
    "paper_title_zh": "文本转音频人工智能中的语义与符号学互动：探索认知动力学与音乐交互",
    "paper_id": "2511.17429",
    "paper_abstract": "This paper investigates the emerging text-to-audio paradigm in artificial intelligence (AI), examining its transformative implications for musical creation, interpretation, and cognition. I explore the complex semantic and semiotic interplays that occur when descriptive natural language prompts are translated into nuanced sound objects across the text-to-audio modality. Drawing from structuralist and post-structuralist perspectives, as well as cognitive theories of schema dynamics and metacognition, the paper explores how these AI systems reconfigure musical signification processes and navigate established cognitive frameworks. The research analyzes some of the cognitive dynamics at play in AI-mediated musicking, including processes of schema assimilation and accommodation, metacognitive reflection, and constructive perception. The paper argues that text-to-audio AI models function as quasi-objects of musical signification, simultaneously stabilizing and destabilizing conventional forms while fostering new modes of listening and aesthetic this http URL Udio as a primary case study, this study explores how these models navigate the liminal spaces between linguistic prompts and sonic outputs. This process not only generates novel musical expressions but also prompts listeners to engage in forms of critical and \"structurally-aware listening.\", encouraging a deeper understanding of music's structures, semiotic nuances, and the socio-cultural contexts that shape our musical cognition. The paper concludes by reflecting on the potential of text-to-audio AI models to serve as epistemic tools and quasi-objects, facilitating a significant shift in musical interactions and inviting users to develop a more nuanced comprehension of the cognitive and cultural foundations of music.",
    "paper_abstract_zh": "本文研究了人工智能（AI）中新兴的文本转音频范式，考察了其对音乐创作、阐释和认知的变革性影响。我探讨了当描述性自然语言提示在文本转音频模态中被转化为细腻的声音对象时发生的复杂语义和符号学互动。借鉴结构主义和后结构主义视角，以及图式动力学和元认知的认知理论，本文探讨了这些AI系统如何重构音乐意指过程并驾驭既定的认知框架。研究分析了AI中介音乐行为中起作用的一些认知动力学，包括图式同化与顺应过程、元认知反思和建构性感知。本文认为，文本转音频AI模型充当了音乐意指的“准对象”，在稳定和 destabilizing（动摇）传统形式的同时，培育了新的聆听和审美体验模式。以Udio为主要案例研究，本研究探讨了这些模型如何在语言提示和声音输出之间的阈限空间中导航。这一过程不仅产生了新颖的音乐表达，还促使听众参与到批判性和“结构感知聆听”的形式中，鼓励人们更深入地理解音乐的结构、符号细微差别以及塑造我们音乐认知的社会文化背景。文章最后反思了文本转音频AI模型作为认知工具和准对象的潜力，促进了音乐交互的重大转变，并邀请用户对音乐的认知和文化基础发展出更细致的理解。",
    "subjects": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2025-11-24",
    "paper_authors": "Guilherme Coelho",
    "topic": [
      "Music Generation"
    ],
    "category": [
      "Music"
    ]
  },
  {
    "paper_title": "Device-Guided Music Transfer",
    "paper_title_zh": "设备引导的音乐传输",
    "paper_id": "2511.17136",
    "paper_abstract": "Device-guided music transfer adapts playback across unseen devices for users who lack them. Existing methods mainly focus on modifying the timbre, rhythm, harmony, or instrumentation to mimic genres or artists, overlooking the diverse hardware properties of the playback device (i.e., speaker). Therefore, we propose DeMT, which processes a speaker's frequency response curve as a line graph using a vision-language model to extract device embeddings. These embeddings then condition a hybrid transformer via feature-wise linear modulation. Fine-tuned on a self-collected dataset, DeMT enables effective speaker-style transfer and robust few-shot adaptation for unseen devices, supporting applications like device-style augmentation and quality enhancement.",
    "paper_abstract_zh": "设备引导的音乐传输能够为缺乏特定设备的用户适配跨设备的播放体验。现有方法主要侧重于修改音色、节奏、和声或乐器编排来模仿特定流派或艺术家，却忽视了播放设备（即扬声器）的多样化硬件特性。因此，我们提出了DeMT，该方法将扬声器的频率响应曲线作为线图，利用视觉语言模型提取设备嵌入。这些嵌入随后通过特征-wise线性调制调节混合transformer模型。在自收集数据集上微调后，DeMT实现了有效的扬声器风格传输，并对未见设备具有强大的少样本适应能力，支持设备风格增强和质量提升等应用。",
    "subjects": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)"
    ],
    "update_time": "2025-11-24",
    "paper_authors": "Manh Pham Hung, Changshuo Hu, Ting Dang, Dong Ma",
    "topic": [
      "Audio Representation Learning",
      "Music Generation"
    ],
    "category": [
      "Music"
    ]
  },
  {
    "paper_title": "MusicAIR: A Multimodal AI Music Generation Framework Powered by an Algorithm-Driven Core",
    "paper_title_zh": "MusicAIR：一种由算法驱动核心支持的多模态AI音乐生成框架",
    "paper_id": "2511.17323",
    "paper_abstract": "Recent advances in generative AI have made music generation a prominent research focus. However, many neural-based models rely on large datasets, raising concerns about copyright infringement and high-performance costs. In contrast, we propose MusicAIR, an innovative multimodal AI music generation framework powered by a novel algorithm-driven symbolic music core, effectively mitigating copyright infringement risks. The music core algorithms connect critical lyrical and rhythmic information to automatically derive musical features, creating a complete, coherent melodic score solely from the lyrics. The MusicAIR framework facilitates music generation from lyrics, text, and images. The generated score adheres to established principles of music theory, lyrical structure, and rhythmic conventions. We developed Generate AI Music (GenAIM), a web tool using MusicAIR for lyric-to-song, text-to-music, and image-to-music generation. In our experiments, we evaluated AI-generated music scores produced by the system using both standard music metrics and innovative analysis that compares these compositions with original works. The system achieves an average key confidence of 85%, outperforming human composers at 79%, and aligns closely with established music theory standards, demonstrating its ability to generate diverse, human-like compositions. As a co-pilot tool, GenAIM can serve as a reliable music composition assistant and a possible educational composition tutor while simultaneously lowering the entry barrier for all aspiring musicians, which is innovative and significantly contributes to AI for music generation.",
    "paper_abstract_zh": "生成式AI的最新进展使音乐生成成为突出的研究焦点。然而，许多基于神经网络的模型依赖大型数据集，引发了关于版权侵权和高性能成本的担忧。相比之下，我们提出了MusicAIR，这是一种创新的多模态AI音乐生成框架，由新颖的算法驱动符号音乐核心支持，有效缓解了版权侵权风险。音乐核心算法将关键歌词和节奏信息连接起来，自动推导出音乐特征，仅从歌词中创建完整、连贯的乐谱。MusicAIR框架支持从歌词、文本和图像生成音乐。生成的乐谱遵循既定的音乐理论、歌词结构和节奏约定。我们开发了生成AI音乐（GenAIM），这是一个使用MusicAIR进行歌词转歌曲、文本转音乐和图像转音乐生成的网络工具。在我们的实验中，我们使用标准音乐指标和将这些作品与原创作品进行比较的创新分析来评估系统生成的AI音乐乐谱。系统平均达到85%的调性置信度，优于人类作曲家的79%，并严格遵循既定的音乐理论标准，展示了其生成多样化、类人作品的能力。作为副驾驶工具，GenAIM可以作为可靠的音乐创作助手和潜在的教育创作导师，同时降低所有有抱负音乐家的入门门槛，这在AI音乐生成领域具有创新性并做出了重大贡献。",
    "subjects": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Multimedia (cs.MM)"
    ],
    "update_time": "2025-11-24",
    "paper_authors": "Callie C. Liao, Duoduo Liao, Ellie L. Zhang",
    "topic": [
      "Music Generation",
      "Audio Representation Learning"
    ],
    "category": [
      "Music"
    ]
  },
  {
    "paper_title": "Is Phase Really Needed for Weakly-Supervised Dereverberation ?",
    "paper_title_zh": "弱监督去混响中相位是否真的必要？",
    "paper_id": "2511.17346",
    "paper_abstract": "In unsupervised or weakly-supervised approaches for speech dereverberation, the target clean (dry) signals are considered to be unknown during training. In that context, evaluating to what extent information can be retrieved from the sole knowledge of reverberant (wet) speech becomes critical. This work investigates the role of the reverberant (wet) phase in the time-frequency domain. Based on Statistical Wave Field Theory, we show that late reverberation perturbs phase components with white, uniformly distributed noise, except at low frequencies. Consequently, the wet phase carries limited useful information and is not essential for weakly supervised dereverberation. To validate this finding, we train dereverberation models under a recent weak supervision framework and demonstrate that performance can be significantly improved by excluding the reverberant phase from the loss function.",
    "paper_abstract_zh": "在语音去混响的无监督或弱监督方法中，目标干净（干燥）信号在训练期间被认为是未知的。在这种情况下，评估仅从混响（湿润）语音的知识中可以检索到什么程度的信息变得至关重要。这项工作研究了时频域中混响（湿润）相位的作用。基于统计波场理论，我们表明晚期混响会扰动相位分量，使其呈现白色、均匀分布的噪声，低频除外。因此，湿润相位携带的有用信息有限，对于弱监督去混响并非必需。为了验证这一发现，我们在最近的弱监督框架下训练去混响模型，并证明通过从损失函数中排除混响相位，性能可以显著提高。",
    "subjects": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Signal Processing (eess.SP)",
      "Classical Physics (physics.class-ph)",
      "Machine Learning (stat.ML)"
    ],
    "update_time": "2025-11-24",
    "paper_authors": "Marius Rodrigues, Louis Bahrman, Roland Badeau, Gaël Richard",
    "topic": [
      "Speech Enhancement",
      "Audio Representation Learning"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Enhancing Quranic Learning: A Multimodal Deep Learning Approach for Arabic Phoneme Recognition",
    "paper_title_zh": "增强古兰经学习：基于阿拉伯语音素识别的多模态深度学习方法",
    "paper_id": "2511.17477",
    "paper_abstract": "Recent advances in multimodal deep learning have greatly enhanced the capability of systems for speech analysis and pronunciation assessment. Accurate pronunciation detection remains a key challenge in Arabic, particularly in the context of Quranic recitation, where subtle phonetic differences can alter meaning. Addressing this challenge, the present study proposes a transformer-based multimodal framework for Arabic phoneme mispronunciation detection that combines acoustic and textual representations to achieve higher precision and robustness. The framework integrates UniSpeech-derived acoustic embeddings with BERT-based textual embeddings extracted from Whisper transcriptions, creating a unified representation that captures both phonetic detail and linguistic context. To determine the most effective integration strategy, early, intermediate, and late fusion methods were implemented and evaluated on two datasets containing 29 Arabic phonemes, including eight hafiz sounds, articulated by 11 native speakers. Additional speech samples collected from publicly available YouTube recordings were incorporated to enhance data diversity and generalization. Model performance was assessed using standard evaluation metrics: accuracy, precision, recall, and F1-score, allowing a detailed comparison of the fusion strategies. Experimental findings show that the UniSpeech-BERT multimodal configuration provides strong results and that fusion-based transformer architectures are effective for phoneme-level mispronunciation detection. The study contributes to the development of intelligent, speaker-independent, and multimodal Computer-Aided Language Learning (CALL) systems, offering a practical step toward technology-supported Quranic pronunciation training and broader speech-based educational applications.",
    "paper_abstract_zh": "多模态深度学习的最新进展极大地增强了语音分析和发音评估系统能力。在阿拉伯语中，准确的发音检测仍然是一个关键挑战，特别是在古兰经朗诵的背景下，细微的语音差异可能会改变含义。为应对这一挑战，本研究提出了一种基于transformer的多模态框架，用于阿拉伯语音素发音错误检测，该框架结合声学和文本表示，以实现更高的精度和鲁棒性。该框架将源自UniSpeech的声学嵌入与从Whisper转录中提取的基于BERT的文本嵌入相结合，创建了一个同时捕捉语音细节和语言上下文的统一表示。为了确定最有效的集成策略，在包含29个阿拉伯语音素（包括八个哈菲兹音素）的两种数据集上实现了并评估了早期、中期和晚期融合方法，这些音素由11名母语人士发音。此外，还纳入了从公开可用的YouTube录音中收集的额外语音样本，以提高数据多样性和泛化能力。使用标准评估指标（准确率、精确率、召回率和F1分数）评估模型性能，从而能够详细比较融合策略。实验结果表明，UniSpeech-BERT多模态配置提供了强有力的结果，并且基于融合的transformer架构对音素级别的发音错误检测有效。该研究有助于开发智能的、说话人无关的、多模态的计算机辅助语言学习（CALL）系统，为技术支持的古兰经发音培训和更广泛的基于语音的教育应用提供了实际步骤。",
    "subjects": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)"
    ],
    "update_time": "2025-11-24",
    "paper_authors": "Ayhan Kucukmanisa, Derya Gelmez, Sukru Selim Calik, Zeynep Hilal Kilimci",
    "topic": [
      "Speech Recognition",
      "Audio Representation Learning"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Better audio representations are more brain-like: linking model-brain alignment with performance in downstream auditory tasks",
    "paper_title_zh": "更好的音频表示更接近大脑：将模型-大脑对齐与下游听觉任务性能联系起来",
    "paper_id": "2511.16849",
    "paper_abstract": "Artificial neural networks (ANNs) are increasingly powerful models of brain computation, yet it remains unclear whether improving their task performance also makes their internal representations more similar to brain signals. To address this question in the auditory domain, we quantified the alignment between the internal representations of 36 different audio models and brain activity from two independent fMRI datasets. Using voxel-wise and component-wise regression, and representation similarity analysis (RSA), we found that recent self-supervised audio models with strong performance in diverse downstream tasks are better predictors of auditory cortex activity than older and more specialized models. To assess the quality of the audio representations, we evaluated these models in 6 auditory tasks from the HEAREval benchmark, spanning music, speech, and environmental sounds. This revealed strong positive Pearson correlations ($r>0.7$) between a model's overall task performance and its alignment with brain representations. Finally, we analyzed the evolution of the similarity between audio and brain representations during the pretraining of EnCodecMAE. We discovered that brain similarity increases progressively and emerges early during pretraining, despite the model not being explicitly optimized for this objective. This suggests that brain-like representations can be an emergent byproduct of learning to reconstruct missing information from naturalistic audio data.",
    "paper_abstract_zh": "人工神经网络(ANN)是日益强大的大脑计算模型，但提高它们的任务性能是否也会使它们的内部表示更接近大脑信号仍然不清楚。为了在听觉领域解决这个问题，我们量化了36种不同音频模型的内部表示与两个独立fMRI数据集的大脑活动之间的对齐程度。使用体素级和组件级回归以及表示相似性分析(RSA)，我们发现，在多样化的下游任务中具有强大性能的最新自监督音频模型，比旧的和更专业的模型能更好地预测听觉皮层活动。为了评估音频表示的质量，我们在HEAREval基准测试的6个听觉任务中评估了这些模型，涵盖了音乐、语音和环境声音。这揭示了模型的总体任务性能与其大脑表示对齐之间的强正Pearson相关性(r>0.7)。最后，我们分析了在EnCodecMAE预训练过程中音频和大脑表示相似性的演变。我们发现，尽管模型没有为此目标进行显式优化，但大脑相似性在预训练过程中逐渐增加，并在早期阶段出现。这表明，从自然音频数据中学习重建缺失信息可能是类大脑表示的一个涌现副产品。",
    "subjects": [
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)"
    ],
    "update_time": "2025-11-24",
    "paper_authors": "Leonardo Pepino, Pablo Riera, Juan Kamienkowski, Luciana Ferrer",
    "topic": [
      "Audio Representation Learning",
      "Audio Classification"
    ],
    "category": [
      "Other"
    ]
  },
  {
    "paper_title": "Investigating self-supervised representations for audio-visual deepfake detection",
    "paper_title_zh": "研究用于音频-视频深度伪造检测的自监督表示",
    "paper_id": "2511.17181",
    "paper_abstract": "Self-supervised representations excel at many vision and speech tasks, but their potential for audio-visual deepfake detection remains underexplored. Unlike prior work that uses these features in isolation or buried within complex architectures, we systematically evaluate them across modalities (audio, video, multimodal) and domains (lip movements, generic visual content). We assess three key dimensions: detection effectiveness, interpretability of encoded information, and cross-modal complementarity. We find that most self-supervised features capture deepfake-relevant information, and that this information is complementary. Moreover, models primarily attend to semantically meaningful regions rather than spurious artifacts. Yet none generalize reliably across datasets. This generalization failure likely stems from dataset characteristics, not from the features themselves latching onto superficial patterns. These results expose both the promise and fundamental challenges of self-supervised representations for deepfake detection: while they learn meaningful patterns, achieving robust cross-domain performance remains elusive.",
    "paper_abstract_zh": "自监督表示在许多视觉和语音任务中表现出色，但它们在音频-视频深度伪造检测方面的潜力尚未得到充分探索。与之前孤立使用这些特征或将它们嵌入复杂架构中的工作不同，我们跨模态（音频、视频、多模态）和领域（唇部动作、通用视觉内容）系统性地评估了它们。我们评估了三个关键维度：检测有效性、编码信息的可解释性和跨模态互补性。我们发现大多数自监督特征捕捉了与深度伪造相关的信息，并且这些信息是互补的。此外，模型主要关注语义上有意义的区域，而不是伪影。然而，没有一种方法能在数据集间可靠地泛化。这种泛化失败可能源于数据集特性，而非特征本身依附于表面模式。这些结果揭示了自监督表示在深度伪造检测方面的前景和基本挑战：虽然它们学习有意义的模式，但实现稳健的跨域性能仍然困难。",
    "subjects": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)"
    ],
    "update_time": "2025-11-24",
    "paper_authors": "Dragos-Alexandru Boldisor, Stefan Smeu, Dan Oneata, Elisabeta Oneata",
    "topic": [
      "Audio Representation Learning"
    ],
    "category": [
      "Image&Video"
    ]
  },
  {
    "paper_title": "A new kid on the block: Distributional semantics predicts the word-specific tone signatures of monosyllabic words in conversational Taiwan Mandarin",
    "paper_title_zh": "新秀登场：分布语义学预测会话台湾汉语单音节词的词特定音调特征",
    "paper_id": "2511.17337",
    "paper_abstract": "We present a corpus-based investigation of how the pitch contours of monosyllabic words are realized in spontaneous conversational Mandarin, focusing on the effects of words' meanings. We used the generalized additive model to decompose a given observed pitch contour into a set of component pitch contours that are tied to different control variables and semantic predictors. Even when variables such as word duration, gender, speaker identity, tonal context, vowel height, and utterance position are controlled for, the effect of word remains a strong predictor of tonal realization. We present evidence that this effect of word is a semantic effect: word sense is shown to be a better predictor than word, and heterographic homophones are shown to have different pitch contours. The strongest evidence for the importance of semantics is that the pitch contours of individual word tokens can be predicted from their contextualized embeddings with an accuracy that substantially exceeds a permutation baseline. For phonetics, distributional semantics is a new kid on the block. Although our findings challenge standard theories of Mandarin tone, they fit well within the theoretical framework of the Discriminative Lexicon Model.",
    "paper_abstract_zh": "我们基于语料库研究了单音节词在自然会话汉语中的音高轮廓实现，重点关注词义的影响。我们使用广义加性模型将观察到的音高轮廓分解为一组与不同控制变量和语义预测因子相关的分量音高轮廓。即使控制了词长、性别、说话人身份、声调上下文、元音高度和句子位置等变量，词本身仍然是声调实现的一个强预测因子。我们提供了证据表明这种词效应是一种语义效应：词义被证明是比词本身更好的预测因子，且同音异义词被证明具有不同的音高轮廓。对于语义重要性的最强证据是，单个词标记的音高轮廓可以从其上下文化嵌入中预测，其准确度显著超过置换基线。对于语音学而言，分布语义学是一个新秀。尽管我们的发现挑战了标准的汉语声调理论，但它们很好地符合了判别词典模型的理论框架。",
    "subjects": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)"
    ],
    "update_time": "2025-11-24",
    "paper_authors": "Xiaoyun Jin, Mirjam Ernestus, R. Harald Baayen",
    "topic": [
      "Audio Representation Learning",
      "Speech Recognition"
    ],
    "category": [
      "Speech"
    ]
  }
]