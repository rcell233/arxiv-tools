[
  {
    "paper_title": "LatPhon: Lightweight Multilingual G2P for Romance Languages and English",
    "paper_title_zh": "LatPhon：面向罗曼语族与英语的轻量级多语言字音转换模型",
    "paper_id": "2509.03300v1",
    "paper_abstract": "Grapheme-to-phoneme (G2P) conversion is a key front-end for text-to-speech (TTS), automatic speech recognition (ASR), speech-to-speech translation (S2ST) and alignment systems, especially across multiple Latin-script languages.We present LatPhon, a 7.5 M - parameter Transformer jointly trained on six such languages--English, Spanish, French, Italian, Portuguese, and Romanian. On the public ipa-dict corpus, it attains a mean phoneme error rate (PER) of 3.5%, outperforming the byte-level ByT5 baseline (5.4%) and approaching language-specific WFSTs (3.2%) while occupying 30 MB of memory, which makes on-device deployment feasible when needed. These results indicate that compact multilingual G2P can serve as a universal front-end for Latin-language speech pipelines.",
    "paper_abstract_zh": "字素到音素（G2P）转换是文本到语音（TTS）、自动语音识别（ASR）、语音到语音翻译（S2ST）及对齐系统的重要前端，尤其在多种拉丁字母语言中更为关键。我们提出 LatPhon，一个拥有 750 万参数的 Transformer 模型，联合训练于英语、西班牙语、法语、意大利语、葡萄牙语和罗马尼亚语六种语言。在公开 ipa-dict 语料上，其平均音素错误率（PER）为 3.5%，优于字节级 ByT5 基线（5.4%），并接近单语言 WFST 系统（3.2%），而模型仅占用 30 MB 内存，便于设备端部署。结果表明，紧凑的多语言 G2P 可作为拉丁语系语音管道的通用前端。",
    "primary_category": "cs.CL",
    "update_time": "2025-09-03",
    "paper_authors": "Luis Felipe Chary, Miguel Arjona Ramirez",
    "topic": [
      "Speech Recognition",
      "Speech Synthesis"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Improving Perceptual Audio Aesthetic Assessment via Triplet Loss and Self-Supervised Embeddings",
    "paper_title_zh": "基于三元组损失与自监督嵌入的感知音频美学评估改进方法",
    "paper_id": "2509.03292v1",
    "paper_abstract": "We present a system for automatic multi-axis perceptual quality prediction of generative audio, developed for Track 2 of the AudioMOS Challenge 2025. The task is to predict four Audio Aesthetic Scores--Production Quality, Production Complexity, Content Enjoyment, and Content Usefulness--for audio generated by text-to-speech (TTS), text-to-audio (TTA), and text-to-music (TTM) systems. A main challenge is the domain shift between natural training data and synthetic evaluation data. To address this, we combine BEATs, a pretrained transformer-based audio representation model, with a multi-branch long short-term memory (LSTM) predictor and use a triplet loss with buffer-based sampling to structure the embedding space by perceptual similarity. Our results show that this improves embedding discriminability and generalization, enabling domain-robust audio quality assessment without synthetic training data.",
    "paper_abstract_zh": "我们提出一套用于生成式音频多维度感知质量预测的自动系统，面向 2025 AudioMOS 挑战赛 Track 2。任务要求为文本到语音（TTS）、文本到音频（TTA）和文本到音乐（TTM）系统生成的音频预测四项音频美学分数：制作质量、制作复杂度、内容愉悦度与内容有用度。主要挑战在于自然训练数据与合成评估数据之间的域偏移。为此，我们将预训练的 Transformer 音频表征模型 BEATs 与多分支长短期记忆（LSTM）预测器结合，并采用带缓存采样的三元组损失，按感知相似度结构化嵌入空间。实验表明，该方法提升了嵌入的可区分性与泛化能力，无需合成训练数据即可实现域鲁棒的音频质量评估。",
    "primary_category": "eess.AS",
    "update_time": "2025-09-03",
    "paper_authors": "Dyah A. M. G. Wisnu, Ryandhimas E. Zezario, Stefano Rini, Hsin-Min Wang, Yu Tsao",
    "topic": [
      "Speech Recognition",
      "Speech Synthesis"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Exploring persuasive Interactions with generative social robots: An experimental framework",
    "paper_title_zh": "探索生成式社交机器人的说服交互：一个实验框架",
    "paper_id": "2509.03231v1",
    "paper_abstract": "Integrating generative AI such as large language models into social robots has improved their ability to engage in natural, human-like communication. This study presents a method to examine their persuasive capabilities. We designed an experimental framework focused on decision making and tested it in a pilot that varied robot appearance and self-knowledge. Using qualitative analysis, we evaluated interaction quality, persuasion effectiveness, and the robot's communicative strategies. Participants generally experienced the interaction positively, describing the robot as competent, friendly, and supportive, while noting practical limits such as delayed responses and occasional speech-recognition errors. Persuasiveness was highly context dependent and shaped by robot behavior: participants responded well to polite, reasoned suggestions and expressive gestures, but emphasized the need for more personalized, context-aware arguments and clearer social roles. These findings suggest that generative social robots can influence user decisions, but their effectiveness depends on communicative nuance and contextual relevance. We propose refinements to the framework to further study persuasive dynamics between robots and human users.",
    "paper_abstract_zh": "将大语言模型等生成式 AI 集成到社交机器人中，显著提升了其自然、类人交流能力。本研究提出一种检验其说服能力的方法。我们设计了以决策为核心的实验框架，并在试点中通过改变机器人外观与自我知识进行测试。借助定性分析，评估交互质量、说服效果及机器人的沟通策略。参与者普遍对交互体验持积极态度，认为机器人能干、友好且支持性强，但也指出响应延迟和偶尔语音识别错误等实际局限。说服效果高度依赖情境与机器人行为：参与者对礼貌、有理的建议及富有表现力的手势反应良好，但强调需更个性化、情境感知的论据与更清晰的社会角色。研究结果表明，生成式社交机器人可影响用户决策，但其有效性取决于沟通细节与情境相关性。我们提出对该框架的改进建议，以进一步研究机器人与用户间的说服动态。",
    "primary_category": "cs.RO",
    "update_time": "2025-09-03",
    "paper_authors": "Stephan Vonschallen, Larissa Julia Corina Finsler, Theresa Schmiedel, Friederike Eyssel",
    "topic": [
      "Speech Recognition"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Beyond Words: Interjection Classification for Improved Human-Computer Interaction",
    "paper_title_zh": "超越词汇：面向更自然人机交互的感叹词分类研究",
    "paper_id": "2509.03181v1",
    "paper_abstract": "In the realm of human-computer interaction, fostering a natural dialogue between humans and machines is paramount. A key, often overlooked, component of this dialogue is the use of interjections such as \"mmm\" and \"hmm\". Despite their frequent use to express agreement, hesitation, or requests for information, these interjections are typically dismissed as \"non-words\" by Automatic Speech Recognition (ASR) engines. Addressing this gap, we introduce a novel task dedicated to interjection classification, a pioneer in the field to our knowledge. This task is challenging due to the short duration of interjection signals and significant inter- and intra-speaker variability. In this work, we present and publish a dataset of interjection signals collected specifically for interjection classification. We employ this dataset to train and evaluate a baseline deep learning model. To enhance performance, we augment the training dataset using techniques such as tempo and pitch transformation, which significantly improve classification accuracy, making models more robust. The interjection dataset, a Python library for the augmentation pipeline, baseline model, and evaluation scripts, are available to the research community.",
    "paper_abstract_zh": "在人机交互领域，实现人与机器之间的自然对话至关重要。其中常被忽视的关键成分是人们使用的感叹词，如“嗯”“呃”。尽管这些词频繁用于表达同意、犹豫或请求信息，却常被自动语音识别（ASR）引擎视为“非词汇”而丢弃。为填补这一空白，我们首次提出专门的感叹词分类任务。该任务因信号持续时间短且存在显著的说话人内外差异而颇具挑战。本文发布并公开了专为感叹词分类采集的信号数据集，并用其训练与评估基线深度学习模型。为进一步提升性能，我们采用节奏与音高变换等技术扩充训练数据，显著提高分类准确率，使模型更加鲁棒。该感叹词数据集、增强流程的 Python 库、基线模型及评估脚本均已向研究社区开放。",
    "primary_category": "cs.HC",
    "update_time": "2025-09-03",
    "paper_authors": "Yaniv Goren, Yuval Cohen, Alexander Apartsin, Yehudit Aperstein",
    "topic": [
      "Speech Recognition"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "A Long Short-Term Memory (LSTM) Model for Business Sentiment Analysis Based on Recurrent Neural Network",
    "paper_title_zh": "基于长短期记忆循环神经网络的商业情感分析模型",
    "paper_id": "2509.03060v1",
    "paper_abstract": "Business sentiment analysis (BSA) is one of the significant and popular topics of natural language processing. It is one kind of sentiment analysis techniques for business purposes. Different categories of sentiment analysis techniques like lexicon-based techniques and different types of machine learning algorithms are applied for sentiment analysis on different languages like English, Hindi, Spanish, etc. In this paper, long short-term memory (LSTM) is applied for business sentiment analysis, where a recurrent neural network is used. An LSTM model is used in a modified approach to prevent the vanishing gradient problem rather than applying the conventional recurrent neural network (RNN). To apply the modified RNN model, product review dataset is used. In this experiment, 70\\% of the data is trained for the LSTM and the rest 30\\% of the data is used for testing. The result of this modified RNN model is compared with other conventional RNN models, and a comparison is made among the results. It is noted that the proposed model performs better than the other conventional RNN models. Here, the proposed model, i.e., the modified RNN model approach has achieved around 91.33\\% of accuracy. By applying this model, any business company or e-commerce business site can identify the feedback from their customers about different types of products that customers like or dislike. Based on the customer reviews, a business company or e-commerce platform can evaluate its marketing strategy.",
    "paper_abstract_zh": "商业情感分析（BSA）是自然语言处理中重要且热门的研究方向，属于面向商业场景的情感分析技术。已有研究将基于词典的方法及多种机器学习算法应用于英语、印地语、西班牙语等不同语言的情感分析。本文采用长短期记忆（LSTM）模型进行商业情感分析，以改进的循环神经网络（RNN）解决梯度消失问题。实验使用商品评论数据集，其中 70% 用于训练 LSTM，剩余 30% 用于测试。将改进后的 RNN 模型与传统 RNN 模型对比，结果表明所提模型性能更优，准确率达到约 91.33%。企业或电商平台可利用该模型识别客户对不同商品的喜好反馈，并据此评估营销策略。",
    "primary_category": "cs.CL",
    "update_time": "2025-09-03",
    "paper_authors": "Md. Jahidul Islam Razin, Md. Abdul Karim, M. F. Mridha, S M Rafiuddin, Tahira Alam",
    "topic": [
      "Speech Recognition"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "A Study on Zero-Shot Non-Intrusive Speech Intelligibility for Hearing Aids Using Large Language Models",
    "paper_title_zh": "基于大语言模型的零样本非侵入式助听器语音可懂度研究",
    "paper_id": "2509.03021v1",
    "paper_abstract": "This work focuses on zero-shot non-intrusive speech assessment for hearing aids (HA) using large language models (LLMs). Specifically, we introduce GPT-Whisper-HA, an extension of GPT-Whisper, a zero-shot non-intrusive speech assessment model based on LLMs. GPT-Whisper-HA is designed for speech assessment for HA, incorporating MSBG hearing loss and NAL-R simulations to process audio input based on each individual's audiogram, two automatic speech recognition (ASR) modules for audio-to-text representation, and GPT-4o to predict two corresponding scores, followed by score averaging for the final estimated score. Experimental results indicate that GPT-Whisper-HA achieves a 2.59% relative root mean square error (RMSE) improvement over GPT-Whisper, confirming the potential of LLMs for zero-shot speech assessment in predicting subjective intelligibility for HA users.",
    "paper_abstract_zh": "本研究聚焦于利用大语言模型（LLM）为助听器（HA）进行零样本非侵入式语音评估。具体而言，我们提出 GPT-Whisper-HA，这是对基于 LLM 的零样本非侵入式语音评估模型 GPT-Whisper 的扩展。GPT-Whisper-HA 专为助听器场景设计，集成 MSBG 听力损失与 NAL-R 仿真以根据个体听力图处理音频输入，采用两套自动语音识别（ASR）模块将音频转为文本，再由 GPT-4o 预测两组对应评分并取平均得到最终估计分数。实验结果表明，GPT-Whisper-HA 相比 GPT-Whisper 的相对均方根误差（RMSE）降低 2.59%，验证了 LLM 在零样本语音评估中预测助听器用户主观可懂度的潜力。",
    "primary_category": "eess.AS",
    "update_time": "2025-09-03",
    "paper_authors": "Ryandhimas E. Zezario, Dyah A. M. G. Wisnu, Hsin-Min Wang, Yu Tsao",
    "topic": [
      "Speech Recognition"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "NADI 2025: The First Multidialectal Arabic Speech Processing Shared Task",
    "paper_title_zh": "NADI 2025：首届多方言阿拉伯语语音处理共享任务",
    "paper_id": "2509.02038v2",
    "paper_abstract": "We present the findings of the sixth Nuanced Arabic Dialect Identification (NADI 2025) Shared Task, which focused on Arabic speech dialect processing across three subtasks: spoken dialect identification (Subtask 1), speech recognition (Subtask 2), and diacritic restoration for spoken dialects (Subtask 3). A total of 44 teams registered, and during the testing phase, 100 valid submissions were received from eight unique teams. The distribution was as follows: 34 submissions for Subtask 1 \"five teams{\\ae}, 47 submissions for Subtask 2 \"six teams\", and 19 submissions for Subtask 3 \"two teams\". The best-performing systems achieved 79.8% accuracy on Subtask 1, 35.68/12.20 WER/CER (overall average) on Subtask 2, and 55/13 WER/CER on Subtask 3. These results highlight the ongoing challenges of Arabic dialect speech processing, particularly in dialect identification, recognition, and diacritic restoration. We also summarize the methods adopted by participating teams and briefly outline directions for future editions of NADI.",
    "paper_abstract_zh": "本文介绍第六届 Nuanced Arabic Dialect Identification（NADI 2025）共享任务的成果，任务聚焦阿拉伯语方言语音处理，共设三个子任务：口语方言识别（子任务 1）、语音识别（子任务 2）及方言语音的变音符号恢复（子任务 3）。共有 44 支队伍注册，测试阶段收到 8 支独立团队的 100 份有效提交，分布为：子任务 1 34 份（5 队）、子任务 2 47 份（6 队）、子任务 3 19 份（2 队）。最佳系统在各子任务分别取得 79.8% 准确率、35.68/12.20 的 WER/CER（平均）以及 55/13 的 WER/CER。结果凸显阿拉伯方言语音处理在识别、转写与变音符号恢复上的持续挑战。我们同时总结了参赛队伍采用的方法，并简要展望 NADI 未来版本的方向。",
    "primary_category": "cs.CL",
    "update_time": "2025-09-03",
    "paper_authors": "Bashar Talafha, Hawau Olamide Toyin, Peter Sullivan, AbdelRahim Elmadany, Abdurrahman Juma, Amirbek Djanibekov, Chiyu Zhang, Hamad Alshehhi, Hanan Aldarmaki, Mustafa Jarrar, Nizar Habash, Muhammad Abdul-Mageed",
    "topic": [
      "Speech Recognition"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Multi-level SSL Feature Gating for Audio Deepfake Detection",
    "paper_title_zh": "用于音频深度伪造检测的多层自监督特征门控机制",
    "paper_id": "2509.03409v1",
    "paper_abstract": "Recent advancements in generative AI, particularly in speech synthesis, have enabled the generation of highly natural-sounding synthetic speech that closely mimics human voices. While these innovations hold promise for applications like assistive technologies, they also pose significant risks, including misuse for fraudulent activities, identity theft, and security threats. Current research on spoofing detection countermeasures remains limited by generalization to unseen deepfake attacks and languages. To address this, we propose a gating mechanism extracting relevant feature from the speech foundation XLS-R model as a front-end feature extractor. For downstream back-end classifier, we employ Multi-kernel gated Convolution (MultiConv) to capture both local and global speech artifacts. Additionally, we introduce Centered Kernel Alignment (CKA) as a similarity metric to enforce diversity in learned features across different MultiConv layers. By integrating CKA with our gating mechanism, we hypothesize that each component helps improving the learning of distinct synthetic speech patterns. Experimental results demonstrate that our approach achieves state-of-the-art performance on in-domain benchmarks while generalizing robustly to out-of-domain datasets, including multilingual speech samples. This underscores its potential as a versatile solution for detecting evolving speech deepfake threats.",
    "paper_abstract_zh": "生成式 AI 的最新进展，尤其是语音合成领域，已能生成高度自然、几乎无法区分的人类语音。尽管这些技术在辅助技术等场景前景广阔，但也带来欺诈、身份盗用和安全威胁等巨大风险。当前反欺骗研究在应对未见深度伪造攻击和跨语言泛化方面仍显不足。为此，我们提出一种门控机制，从语音基础模型 XLS-R 中提取相关特征作为前端。下游后端分类器采用多核门控卷积（MultiConv）捕捉局部与全局语音伪影。此外，我们引入中心核对齐（CKA）作为相似度度量，在不同 MultiConv 层间强制特征多样性。通过将 CKA 与门控机制结合，我们假设各组件可协同学习更丰富的合成语音模式。实验表明，该方法在域内基准上达到 SOTA 性能，并对多语言域外数据集表现出稳健泛化，凸显其应对不断演变的语音深度伪造威胁的通用潜力。",
    "primary_category": "cs.SD",
    "update_time": "2025-09-03",
    "paper_authors": "Hoan My Tran, Damien Lolive, Aghilas Sini, Arnaud Delhay, Pierre-François Marteau, David Guennec",
    "topic": [
      "Speech Synthesis"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "AIVA: An AI-based Virtual Companion for Emotion-aware Interaction",
    "paper_title_zh": "AIVA：基于 AI 的情感感知虚拟伴侣",
    "paper_id": "2509.03212v1",
    "paper_abstract": "Recent advances in Large Language Models (LLMs) have significantly improved natural language understanding and generation, enhancing Human-Computer Interaction (HCI). However, LLMs are limited to unimodal text processing and lack the ability to interpret emotional cues from non-verbal signals, hindering more immersive and empathetic interactions. This work explores integrating multimodal sentiment perception into LLMs to create emotion-aware agents. We propose \\ours, an AI-based virtual companion that captures multimodal sentiment cues, enabling emotionally aligned and animated HCI. \\ours introduces a Multimodal Sentiment Perception Network (MSPN) using a cross-modal fusion transformer and supervised contrastive learning to provide emotional cues. Additionally, we develop an emotion-aware prompt engineering strategy for generating empathetic responses and integrate a Text-to-Speech (TTS) system and animated avatar module for expressive interactions. \\ours provides a framework for emotion-aware agents with applications in companion robotics, social care, mental health, and human-centered AI.",
    "paper_abstract_zh": "大语言模型（LLM）的最新进展显著提升了自然语言理解与生成能力，改善了人机交互（HCI）。然而，LLM 仅支持单模态文本处理，无法解读非语言信号中的情感线索，限制了更沉浸式、更具共情的交互体验。本研究探索将多模态情感感知集成至 LLM，以构建情感智能体。我们提出 AIVA，一款 AI 虚拟伴侣，可捕捉多模态情感线索，实现情感对齐且生动的人机交互。AIVA 引入多模态情感感知网络（MSPN），采用跨模态融合 Transformer 与监督对比学习提供情感线索；此外，我们设计了情感感知提示工程策略以生成共情回复，并集成文本到语音（TTS）系统与动画头像模块实现富有表现力的交互。AIVA 为情感智能体提供框架，可应用于陪伴机器人、社会关怀、心理健康及人本 AI 等领域。",
    "primary_category": "cs.CV",
    "update_time": "2025-09-03",
    "paper_authors": "Chenxi Li",
    "topic": [
      "Speech Synthesis"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Binaural Target Speaker Extraction using HRTFs",
    "paper_title_zh": "利用 HRTF 的双耳目标说话人提取",
    "paper_id": "2507.19369v2",
    "paper_abstract": "In this work, we aim to imitate the human ability to selectively attend to a single speaker, even in the presence of multiple simultaneous talkers. To achieve this, we propose a novel approach for binaural target speaker extraction that leverages the listener's Head-Related Transfer Function (HRTF) to isolate the desired speaker. Notably, our method does not rely on speaker embeddings, making it speaker-independent and enabling strong generalization across multiple speech datasets and languages. We employ a fully complex-valued neural network that operates directly on the complex-valued Short-Time Fourier transform (STFT) of the mixed audio signals, and compare it to a Real-Imaginary (RI)-based neural network, demonstrating the advantages of the former. We first evaluate the method in an anechoic, noise-free scenario, achieving excellent extraction performance while preserving the binaural cues of the target signal. We then extend the evaluation to reverberant conditions. Our method proves robust, maintaining speech clarity and source directionality while simultaneously reducing reverberation. A comparative analysis with existing binaural Target Speaker Extraction (TSE) methods demonstrates that our approach attains performance on par with competing techniques in terms of noise reduction and perceptual quality, while offering a clear advantage in preserving binaural cues.Demo-page: https://bi-ctse-hrtf.github.io",
    "paper_abstract_zh": "本研究旨在模仿人类在多人同时说话场景中选择性关注单一说话人的能力。为此，我们提出一种新颖的双耳目标说话人提取方法，利用听者的头相关传输函数（HRTF）分离目标说话人。值得注意的是，该方法无需说话人嵌入，具备说话人无关特性，可在多语音数据集及语言间实现强泛化。我们采用完全复值神经网络，直接在混合音频信号的复值短时傅里叶变换（STFT）上操作，并与实-虚（RI）网络对比，展示前者的优势。首先在无混响、无噪声场景评估，取得优异提取性能并保留目标信号的双耳线索；随后扩展至混响条件。实验表明，该方法稳健地保持语音清晰度与声源方向性，同时抑制混响。与现有双耳目标说话人提取（TSE）方法对比，我们的方案在降噪与感知质量上达到可比性能，并在保留双耳线索方面具有明显优势。演示页面：https://bi-ctse-hrtf.github.io",
    "primary_category": "eess.AS",
    "update_time": "2025-09-03",
    "paper_authors": "Yoav Ellinson, Sharon Gannot",
    "topic": [
      "Speech Enhancement"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Open Data from LIGO, Virgo, and KAGRA through the First Part of the Fourth Observing Run",
    "paper_title_zh": "LIGO、Virgo 与 KAGRA 在第四次观测运行前半段的开放数据",
    "paper_id": "2508.18079v2",
    "paper_abstract": "LIGO, Virgo, and KAGRA form a network of gravitational-wave observatories. Data and analysis results from this network are made publicly available through the Gravitational Wave Open Science Center. This paper describes open data from this network, including the addition of data from the first part of the fourth observing run (O4a) and selected periods from the preceding engineering run, collected from May 2023 to January 2024. The public data set includes calibrated strain time series for each instrument, data from additional channels used for noise subtraction and detector characterization, and analysis data products from version 4.0 of the Gravitational-Wave Transient Catalog.",
    "paper_abstract_zh": "LIGO、Virgo 与 KAGRA 构成引力波观测网络，其数据与分析结果通过引力波开放科学中心向公众发布。本文介绍了该网络的开放数据，新增了 2023 年 5 月至 2024 年 1 月期间第四次观测运行前半段（O4a）以及前期工程运行部分时段的数据。公开数据集包含每台仪器的校准应变时间序列、用于噪声扣除与探测器表征的附加通道数据，以及引力波瞬变目录 4.0 版的分析数据产品。",
    "primary_category": "gr-qc",
    "update_time": "2025-09-03",
    "paper_authors": "The LIGO Scientific Collaboration, the Virgo Collaboration, the KAGRA Collaboration, A. G. Abac, I. Abouelfettouh, F. Acernese, K. Ackley, C. Adamcewicz, S. Adhicary, D. Adhikari, N. Adhikari, R. X. Adhikari, V. K. Adkins, S. Afroz, A. Agapito, D. Agarwal, M. Agathos, N. Aggarwal, S. Aggarwal, O. D. Aguiar, I. -L. Ahrend, L. Aiello, A. Ain, P. Ajith, T. Akutsu, S. Albanesi, W. Ali, S. Al-Kershi, C. Alléné, A. Allocca, S. Al-Shammari, P. A. Altin, S. Alvarez-Lopez, W. Amar, O. Amarasinghe, A. Amato, F. Amicucci, C. Amra, A. Ananyeva, S. B. Anderson, W. G. Anderson, M. Andia, M. Ando, M. Andrés-Carcasona, T. Andrić, J. Anglin, S. Ansoldi, J. M. Antelis, S. Antier, M. Aoumi, E. Z. Appavuravther, S. Appert, S. K. Apple, K. Arai, A. Araya, M. C. Araya, M. Arca Sedda, J. S. Areeda, N. Aritomi, F. Armato, S. Armstrong, N. Arnaud, M. Arogeti, S. M. Aronson, G. Ashton, Y. Aso, L. Asprea, M. Assiduo, S. Assis de Souza Melo, S. M. Aston, P. Astone, F. Attadio, F. Aubin, K. AultONeal, G. Avallone, E. A. Avila, S. Babak, C. Badger, S. Bae, S. Bagnasco, L. Baiotti, R. Bajpai, T. Baka, A. M. Baker, K. A. Baker, T. Baker, G. Baldi, N. Baldicchi, M. Ball, G. Ballardin, S. W. Ballmer, S. Banagiri, B. Banerjee, D. Bankar, T. M. Baptiste, P. Baral, M. Baratti, J. C. Barayoga, B. C. Barish, D. Barker, N. Barman, P. Barneo, F. Barone, B. Barr, L. Barsotti, M. Barsuglia, D. Barta, A. M. Bartoletti, M. A. Barton, I. Bartos, A. Basalaev, R. Bassiri, A. Basti, M. Bawaj, P. Baxi, J. C. Bayley, A. C. Baylor, P. A. Baynard II, M. Bazzan, V. M. Bedakihale, F. Beirnaert, M. Bejger, D. Belardinelli, A. S. Bell, D. S. Bellie, L. Bellizzi, W. Benoit, I. Bentara, J. D. Bentley, M. Ben Yaala, S. Bera, F. Bergamin, B. K. Berger, S. Bernuzzi, M. Beroiz, C. P. L. Berry, D. Bersanetti, T. Bertheas, A. Bertolini, J. Betzwieser, D. Beveridge, G. Bevilacqua, N. Bevins, R. Bhandare, R. Bhatt, D. Bhattacharjee, S. Bhattacharyya, S. Bhaumik, V. Biancalana, A. Bianchi, I. A. Bilenko, G. Billingsley, A. Binetti, S. Bini, C. Binu, S. Biot, O. Birnholtz, S. Biscoveanu, A. Bisht, M. Bitossi, M. -A. Bizouard, S. Blaber, J. K. Blackburn, L. A. Blagg, C. D. Blair, D. G. Blair, N. Bode, N. Boettner, G. Boileau, M. Boldrini, G. N. Bolingbroke, A. Bolliand, L. D. Bonavena, R. Bondarescu, F. Bondu, E. Bonilla, M. S. Bonilla, A. Bonino, R. Bonnand, A. Borchers, S. Borhanian, V. Boschi, S. Bose, V. Bossilkov, Y. Bothra, A. Boudon, L. Bourg, M. Boyle, A. Bozzi, C. Bradaschia, P. R. Brady, A. Branch, M. Branchesi, I. Braun, T. Briant, A. Brillet, M. Brinkmann, P. Brockill, E. Brockmueller, A. F. Brooks, B. C. Brown, D. D. Brown, M. L. Brozzetti, S. Brunett, G. Bruno, R. Bruntz, J. Bryant, Y. Bu, F. Bucci, J. Buchanan, O. Bulashenko, T. Bulik, H. J. Bulten, A. Buonanno, K. Burtnyk, R. Buscicchio, D. Buskulic, C. Buy, R. L. Byer, G. S. Cabourn Davies, R. Cabrita, V. Cáceres-Barbosa, L. Cadonati, G. Cagnoli, C. Cahillane, A. Calafat, T. A. Callister, E. Calloni, S. R. Callos, M. Canepa, G. Caneva Santoro, K. C. Cannon, H. Cao, L. A. Capistran, E. Capocasa, E. Capote, G. Capurri, G. Carapella, F. Carbognani, M. Carlassara, J. B. Carlin, T. K. Carlson, M. F. Carney, M. Carpinelli, G. Carrillo, J. J. Carter, G. Carullo, A. Casallas-Lagos, J. Casanueva Diaz, C. Casentini, S. Y. Castro-Lucas, S. Caudill, M. Cavaglià, R. Cavalieri, A. Ceja, G. Cella, P. Cerdá-Durán, E. Cesarini, N. Chabbra, W. Chaibi, A. Chakraborty, P. Chakraborty, S. Chakraborty, S. Chalathadka Subrahmanya, J. C. L. Chan, M. Chan, K. Chang, S. Chao, P. Charlton, E. Chassande-Mottin, C. Chatterjee, Debarati Chatterjee, Deep Chatterjee, M. Chaturvedi, S. Chaty, K. Chatziioannou, A. Chen, A. H. -Y. Chen, D. Chen, H. Chen, H. Y. Chen, S. Chen, Yanbei Chen, Yitian Chen, H. P. Cheng, P. Chessa, H. T. Cheung, S. Y. Cheung, F. Chiadini, G. Chiarini, A. Chiba, A. Chincarini, M. L. Chiofalo, A. Chiummo, C. Chou, S. Choudhary, N. Christensen, S. S. Y. Chua, G. Ciani, P. Ciecielag, M. Cieślar, M. Cifaldi, B. Cirok, F. Clara, J. A. Clark, T. A. Clarke, P. Clearwater, S. Clesse, F. Cleva, E. Coccia, E. Codazzo, P. -F. Cohadon, S. Colace, E. Colangeli, M. Colleoni, C. G. Collette, J. Collins, S. Colloms, A. Colombo, C. M. Compton, G. Connolly, L. Conti, T. R. Corbitt, I. Cordero-Carrión, S. Corezzi, N. J. Cornish, I. Coronado, A. Corsi, R. Cottingham, M. W. Coughlin, A. Couineaux, P. Couvares, D. M. Coward, R. Coyne, A. Cozzumbo, J. D. E. Creighton, T. D. Creighton, P. Cremonese, S. Crook, R. Crouch, J. Csizmazia, J. R. Cudell, T. J. Cullen, A. Cumming, E. Cuoco, M. Cusinato, L. V. Da Conceição, T. Dal Canton, S. Dal Pra, G. Dálya, B. D'Angelo, S. Danilishin, S. D'Antonio, K. Danzmann, K. E. Darroch, L. P. Dartez, R. Das, A. Dasgupta, V. Dattilo, A. Daumas, N. Davari, I. Dave, A. Davenport, M. Davier, T. F. Davies, D. Davis, L. Davis, M. C. Davis, P. Davis, E. J. Daw, M. Dax, J. De Bolle, M. Deenadayalan, J. Degallaix, M. De Laurentis, F. De Lillo, S. Della Torre, W. Del Pozzo, A. Demagny, F. De Marco, G. Demasi, F. De Matteis, N. Demos, T. Dent, A. Depasse, N. DePergola, R. De Pietri, R. De Rosa, C. De Rossi, M. Desai, R. DeSalvo, A. DeSimone, R. De Simone, A. Dhani, R. Diab, M. C. Díaz, M. Di Cesare, G. Dideron, T. Dietrich, L. Di Fiore, C. Di Fronzo, M. Di Giovanni, T. Di Girolamo, D. Diksha, J. Ding, S. Di Pace, I. Di Palma, D. Di Piero, F. Di Renzo, Divyajyoti, A. Dmitriev, J. P. Docherty, Z. Doctor, N. Doerksen, E. Dohmen, A. Doke, A. Domiciano De Souza, L. D'Onofrio, F. Donovan, K. L. Dooley, T. Dooney, S. Doravari, O. Dorosh, W. J. D. Doyle, M. Drago, J. C. Driggers, L. Dunn, U. Dupletsa, P. -A. Duverne, D. D'Urso, P. Dutta Roy, H. Duval, S. E. Dwyer, C. Eassa, M. Ebersold, T. Eckhardt, G. Eddolls, A. Effler, J. Eichholz, H. Einsle, M. Eisenmann, M. Emma, K. Endo, R. Enficiaud, L. Errico, R. Espinosa, M. Esposito, R. C. Essick, H. Estellés, T. Etzel, M. Evans, T. Evstafyeva, B. E. Ewing, J. M. Ezquiaga, F. Fabrizi, V. Fafone, S. Fairhurst, A. M. Farah, B. Farr, W. M. Farr, G. Favaro, M. Favata, M. Fays, M. Fazio, J. Feicht, M. M. Fejer, R. Felicetti, E. Fenyvesi, J. Fernandes, T. Fernandes, D. Fernando, S. Ferraiuolo, T. A. Ferreira, F. Fidecaro, P. Figura, A. Fiori, I. Fiori, M. Fishbach, R. P. Fisher, R. Fittipaldi, V. Fiumara, R. Flaminio, S. M. Fleischer, L. S. Fleming, E. Floden, H. Fong, J. A. Font, F. Fontinele-Nunes, C. Foo, B. Fornal, K. Franceschetti, F. Frappez, S. Frasca, F. Frasconi, J. P. Freed, Z. Frei, A. Freise, O. Freitas, R. Frey, W. Frischhertz, P. Fritschel, V. V. Frolov, G. G. Fronzé, M. Fuentes-Garcia, S. Fujii, T. Fujimori, P. Fulda, M. Fyffe, B. Gadre, J. R. Gair, S. Galaudage, V. Galdi, R. Gamba, A. Gamboa, S. Gamoji, D. Ganapathy, A. Ganguly, B. Garaventa, J. García-Bellido, C. García-Quirós, J. W. Gardner, K. A. Gardner, S. Garg, J. Gargiulo, X. Garrido, A. Garron, F. Garufi, P. A. Garver, C. Gasbarra, B. Gateley, F. Gautier, V. Gayathri, T. Gayer, G. Gemme, A. Gennai, V. Gennari, J. George, R. George, O. Gerberding, L. Gergely, Archisman Ghosh, Sayantan Ghosh, Shaon Ghosh, Shrobana Ghosh, Suprovo Ghosh, Tathagata Ghosh, J. A. Giaime, K. D. Giardina, D. R. Gibson, C. Gier, S. Gkaitatzis, J. Glanzer, F. Glotin, J. Godfrey, R. V. Godley, P. Godwin, A. S. Goettel, E. Goetz, J. Golomb, S. Gomez Lopez, B. Goncharov, G. González, P. Goodarzi, S. Goode, A. W. Goodwin-Jones, M. Gosselin, R. Gouaty, D. W. Gould, K. Govorkova, A. Grado, V. Graham, A. E. Granados, M. Granata, V. Granata, S. Gras, P. Grassia, J. Graves, C. Gray, R. Gray, G. Greco, A. C. Green, L. Green, S. M. Green, S. R. Green, C. Greenberg, A. M. Gretarsson, H. K. Griffin, D. Griffith, H. L. Griggs, G. Grignani, C. Grimaud, H. Grote, S. Grunewald, D. Guerra, D. Guetta, G. M. Guidi, A. R. Guimaraes, H. K. Gulati, F. Gulminelli, H. Guo, W. Guo, Y. Guo, Anuradha Gupta, I. Gupta, N. C. Gupta, S. K. Gupta, V. Gupta, N. Gupte, J. Gurs, N. Gutierrez, N. Guttman, F. Guzman, D. Haba, M. Haberland, S. Haino, E. D. Hall, E. Z. Hamilton, G. Hammond, M. Haney, J. Hanks, C. Hanna, M. D. Hannam, O. A. Hannuksela, A. G. Hanselman, H. Hansen, J. Hanson, S. Hanumasagar, R. Harada, A. R. Hardison, S. Harikumar, K. Haris, I. Harley-Trochimczyk, T. Harmark, J. Harms, G. M. Harry, I. W. Harry, J. Hart, B. Haskell, C. J. Haster, K. Haughian, H. Hayakawa, K. Hayama, M. C. Heintze, J. Heinze, J. Heinzel, H. Heitmann, F. Hellman, A. F. Helmling-Cornell, G. Hemming, O. Henderson-Sapir, M. Hendry, I. S. Heng, M. H. Hennig, C. Henshaw, M. Heurs, A. L. Hewitt, J. Heynen, J. Heyns, S. Higginbotham, S. Hild, S. Hill, Y. Himemoto, N. Hirata, C. Hirose, D. Hofman, B. E. Hogan, N. A. Holland, I. J. Hollows, D. E. Holz, L. Honet, D. J. Horton-Bailey, J. Hough, S. Hourihane, N. T. Howard, E. J. Howell, C. G. Hoy, C. A. Hrishikesh, P. Hsi, H. -F. Hsieh, H. -Y. Hsieh, C. Hsiung, S. -H. Hsu, W. -F. Hsu, Q. Hu, H. Y. Huang, Y. Huang, Y. T. Huang, A. D. Huddart, B. Hughey, V. Hui, S. Husa, R. Huxford, L. Iampieri, G. A. Iandolo, M. Ianni, G. Iannone, J. Iascau, K. Ide, R. Iden, A. Ierardi, S. Ikeda, H. Imafuku, Y. Inoue, G. Iorio, P. Iosif, M. H. Iqbal, J. Irwin, R. Ishikawa, M. Isi, K. S. Isleif, Y. Itoh, M. Iwaya, B. R. Iyer, C. Jacquet, P. -E. Jacquet, T. Jacquot, S. J. Jadhav, S. P. Jadhav, M. Jain, T. Jain, A. L. James, K. Jani, J. Janquart, N. N. Janthalur, S. Jaraba, P. Jaranowski, R. Jaume, W. Javed, A. Jennings, M. Jensen, W. Jia, J. Jiang, H. -B. Jin, G. R. Johns, N. A. Johnson, M. C. Johnston, R. Johnston, N. Johny, D. H. Jones, D. I. Jones, R. Jones, H. E. Jose, P. Joshi, S. K. Joshi, G. Joubert, J. Ju, L. Ju, K. Jung, J. Junker, V. Juste, H. B. Kabagoz, T. Kajita, I. Kaku, V. Kalogera, M. Kalomenopoulos, M. Kamiizumi, N. Kanda, S. Kandhasamy, G. Kang, N. C. Kannachel, J. B. Kanner, S. A. KantiMahanty, S. J. Kapadia, D. P. Kapasi, M. Karthikeyan, M. Kasprzack, H. Kato, T. Kato, E. Katsavounidis, W. Katzman, R. Kaushik, K. Kawabe, R. Kawamoto, D. Keitel, L. J. Kemperman, J. Kennington, F. A. Kerkow, R. Kesharwani, J. S. Key, R. Khadela, S. Khadka, S. S. Khadkikar, F. Y. Khalili, F. Khan, T. Khanam, M. Khursheed, N. M. Khusid, W. Kiendrebeogo, N. Kijbunchoo, C. Kim, J. C. Kim, K. Kim, M. H. Kim, S. Kim, Y. -M. Kim, C. Kimball, K. Kimes, M. Kinnear, J. S. Kissel, S. Klimenko, A. M. Knee, E. J. Knox, N. Knust, K. Kobayashi, S. M. Koehlenbeck, G. Koekoek, K. Kohri, K. Kokeyama, S. Koley, P. Kolitsidou, A. E. Koloniari, K. Komori, A. K. H. Kong, A. Kontos, L. M. Koponen, M. Korobko, X. Kou, A. Koushik, N. Kouvatsos, M. Kovalam, T. Koyama, D. B. Kozak, S. L. Kranzhoff, V. Kringel, N. V. Krishnendu, S. Kroker, A. Królak, K. Kruska, J. Kubisz, G. Kuehn, S. Kulkarni, A. Kulur Ramamohan, Achal Kumar, Anil Kumar, Praveen Kumar, Prayush Kumar, Rahul Kumar, Rakesh Kumar, J. Kume, K. Kuns, N. Kuntimaddi, S. Kuroyanagi, S. Kuwahara, K. Kwak, K. Kwan, S. Kwon, G. Lacaille, D. Laghi, A. H. Laity, E. Lalande, M. Lalleman, P. C. Lalremruati, M. Landry, B. B. Lane, R. N. Lang, J. Lange, R. Langgin, B. Lantz, I. La Rosa, J. Larsen, A. Lartaux-Vollard, P. D. Lasky, J. Lawrence, M. Laxen, C. Lazarte, A. Lazzarini, C. Lazzaro, P. Leaci, L. Leali, Y. K. Lecoeuche, H. M. Lee, H. W. Lee, J. Lee, K. Lee, R. -K. Lee, R. Lee, Sungho Lee, Sunjae Lee, Y. Lee, I. N. Legred, J. Lehmann, L. Lehner, M. Le Jean, A. Lemaître, M. Lenti, M. Leonardi, M. Lequime, N. Leroy, M. Lesovsky, N. Letendre, M. Lethuillier, Y. Levin, K. Leyde, A. K. Y. Li, K. L. Li, T. G. F. Li, X. Li, Y. Li, Z. Li, A. Lihos, E. T. Lin, F. Lin, L. C. -C. Lin, Y. -C. Lin, C. Lindsay, S. D. Linker, A. Liu, G. C. Liu, Jian Liu, F. Llamas Villarreal, J. Llobera-Querol, R. K. L. Lo, J. -P. Locquet, S. C. G. Loggins, M. R. Loizou, L. T. London, A. Longo, D. Lopez, M. Lopez Portilla, A. Lorenzo-Medina, V. Loriette, M. Lormand, G. Losurdo, E. Lotti, T. P. Lott IV, J. D. Lough, H. A. Loughlin, C. O. Lousto, N. Low, N. Lu, L. Lucchesi, H. Lück, D. Lumaca, A. P. Lundgren, A. W. Lussier, R. Macas, M. MacInnis, D. M. Macleod, I. A. O. MacMillan, A. Macquet, K. Maeda, S. Maenaut, S. S. Magare, R. M. Magee, E. Maggio, R. Maggiore, M. Magnozzi, M. Mahesh, M. Maini, S. Majhi, E. Majorana, C. N. Makarem, D. Malakar, J. A. Malaquias-Reis, U. Mali, S. Maliakal, A. Malik, L. Mallick, A. -K. Malz, N. Man, M. Mancarella, V. Mandic, V. Mangano, B. Mannix, G. L. Mansell, M. Manske, M. Mantovani, M. Mapelli, C. Marinelli, F. Marion, A. S. Markosyan, A. Markowitz, E. Maros, S. Marsat, F. Martelli, I. W. Martin, R. M. Martin, B. B. Martinez, D. A. Martinez, M. Martinez, V. Martinez, A. Martini, J. C. Martins, D. V. Martynov, E. J. Marx, L. Massaro, A. Masserot, M. Masso-Reid, S. Mastrogiovanni, T. Matcovich, M. Matiushechkina, L. Maurin, N. Mavalvala, N. Maxwell, G. McCarrol, R. McCarthy, D. E. McClelland, S. McCormick, L. McCuller, S. McEachin, C. McElhenny, G. I. McGhee, J. McGinn, K. B. M. McGowan, J. McIver, A. McLeod, I. McMahon, T. McRae, R. McTeague, D. Meacher, B. N. Meagher, R. Mechum, Q. Meijer, A. Melatos, C. S. Menoni, F. Mera, R. A. Mercer, L. Mereni, K. Merfeld, E. L. Merilh, J. R. Mérou, J. D. Merritt, M. Merzougui, C. Messick, B. Mestichelli, M. Meyer-Conde, F. Meylahn, A. Mhaske, A. Miani, H. Miao, C. Michel, Y. Michimura, H. Middleton, D. P. Mihaylov, S. J. Miller, M. Millhouse, E. Milotti, V. Milotti, Y. Minenkov, E. M. Minihan, Ll. M. Mir, L. Mirasola, M. Miravet-Tenés, C. -A. Miritescu, A. Mishra, C. Mishra, T. Mishra, A. L. Mitchell, J. G. Mitchell, S. Mitra, V. P. Mitrofanov, K. Mitsuhashi, R. Mittleman, O. Miyakawa, S. Miyoki, A. Miyoko, G. Mo, L. Mobilia, S. R. P. Mohapatra, S. R. Mohite, M. Molina-Ruiz, M. Mondin, M. Montani, C. J. Moore, D. Moraru, A. More, S. More, C. Moreno, E. A. Moreno, G. Moreno, A. Moreso Serra, S. Morisaki, Y. Moriwaki, G. Morras, A. Moscatello, M. Mould, B. Mours, C. M. Mow-Lowry, L. Muccillo, F. Muciaccia, D. Mukherjee, Samanwaya Mukherjee, Soma Mukherjee, Subroto Mukherjee, Suvodip Mukherjee, N. Mukund, A. Mullavey, H. Mullock, J. Mundi, C. L. Mungioli, M. Murakoshi, P. G. Murray, D. Nabari, S. L. Nadji, A. Nagar, N. Nagarajan, K. Nakagaki, K. Nakamura, H. Nakano, M. Nakano, D. Nanadoumgar-Lacroze, D. Nandi, V. Napolano, P. Narayan, I. Nardecchia, T. Narikawa, H. Narola, L. Naticchioni, R. K. Nayak, L. Negri, A. Nela, C. Nelle, A. Nelson, T. J. N. Nelson, M. Nery, A. Neunzert, S. Ng, L. Nguyen Quynh, S. A. Nichols, A. B. Nielsen, Y. Nishino, A. Nishizawa, S. Nissanke, W. Niu, F. Nocera, J. Noller, M. Norman, C. North, J. Novak, R. Nowicki, J. F. Nuño Siles, L. K. Nuttall, K. Obayashi, J. Oberling, J. O'Dell, E. Oelker, M. Oertel, G. Oganesyan, T. O'Hanlon, M. Ohashi, F. Ohme, R. Oliveri, R. Omer, B. O'Neal, M. Onishi, K. Oohara, B. O'Reilly, M. Orselli, R. O'Shaughnessy, S. O'Shea, S. Oshino, C. Osthelder, I. Ota, D. J. Ottaway, A. Ouzriat, H. Overmier, B. J. Owen, R. Ozaki, A. E. Pace, R. Pagano, M. A. Page, A. Pai, L. Paiella, A. Pal, S. Pal, M. A. Palaia, M. Pálfi, P. P. Palma, C. Palomba, P. Palud, H. Pan, J. Pan, K. C. Pan, P. K. Panda, Shiksha Pandey, Swadha Pandey, P. T. H. Pang, F. Pannarale, K. A. Pannone, B. C. Pant, F. H. Panther, M. Panzeri, F. Paoletti, A. Paolone, A. Papadopoulos, E. E. Papalexakis, L. Papalini, G. Papigkiotis, A. Paquis, A. Parisi, B. -J. Park, J. Park, W. Parker, G. Pascale, D. Pascucci, A. Pasqualetti, R. Passaquieti, L. Passenger, D. Passuello, O. Patane, A. V. Patel, D. Pathak, A. Patra, B. Patricelli, B. G. Patterson, K. Paul, S. Paul, E. Payne, T. Pearce, M. Pedraza, A. Pele, F. E. Peña Arellano, X. Peng, Y. Peng, S. Penn, M. D. Penuliar, A. Perego, Z. Pereira, C. Périgois, G. Perna, A. Perreca, J. Perret, S. Perriès, J. W. Perry, D. Pesios, S. Peters, S. Petracca, C. Petrillo, H. P. Pfeiffer, H. Pham, K. A. Pham, K. S. Phukon, H. Phurailatpam, M. Piarulli, L. Piccari, O. J. Piccinni, M. Pichot, M. Piendibene, F. Piergiovanni, L. Pierini, G. Pierra, V. Pierro, M. Pietrzak, M. Pillas, F. Pilo, L. Pinard, I. M. Pinto, M. Pinto, B. J. Piotrzkowski, M. Pirello, M. D. Pitkin, A. Placidi, E. Placidi, M. L. Planas, W. Plastino, C. Plunkett, R. Poggiani, E. Polini, J. Pomper, L. Pompili, J. Poon, E. Porcelli, E. K. Porter, C. Posnansky, R. Poulton, J. Powell, G. S. Prabhu, M. Pracchia, B. K. Pradhan, T. Pradier, A. K. Prajapati, K. Prasai, R. Prasanna, P. Prasia, G. Pratten, G. Principe, G. A. Prodi, P. Prosperi, P. Prosposito, A. C. Providence, A. Puecher, J. Pullin, P. Puppo, M. Pürrer, H. Qi, J. Qin, G. Quéméner, V. Quetschke, P. J. Quinonez, N. Qutob, R. Rading, I. Rainho, S. Raja, C. Rajan, B. Rajbhandari, K. E. Ramirez, F. A. Ramis Vidal, M. Ramos Arevalo, A. Ramos-Buades, S. Ranjan, K. Ransom, P. Rapagnani, B. Ratto, A. Ravichandran, A. Ray, V. Raymond, M. Razzano, J. Read, T. Regimbau, S. Reid, C. Reissel, D. H. Reitze, A. I. Renzini, A. Renzini, B. Revenu, A. Revilla Peña, R. Reyes, L. Ricca, F. Ricci, M. Ricci, A. Ricciardone, J. Rice, J. W. Richardson, M. L. Richardson, A. Rijal, K. Riles, H. K. Riley, S. Rinaldi, J. Rittmeyer, C. Robertson, F. Robinet, M. Robinson, A. Rocchi, L. Rolland, J. G. Rollins, A. E. Romano, R. Romano, A. Romero, I. M. Romero-Shaw, J. H. Romie, S. Ronchini, T. J. Roocke, L. Rosa, T. J. Rosauer, C. A. Rose, D. Rosińska, M. P. Ross, M. Rossello-Sastre, S. Rowan, S. K. Roy, S. Roy, D. Rozza, P. Ruggi, N. Ruhama, E. Ruiz Morales, K. Ruiz-Rocha, S. Sachdev, T. Sadecki, P. Saffarieh, S. Safi-Harb, M. R. Sah, S. Saha, T. Sainrat, S. Sajith Menon, K. Sakai, Y. Sakai, M. Sakellariadou, S. Sakon, O. S. Salafia, F. Salces-Carcoba, L. Salconi, M. Saleem, F. Salemi, M. Sallé, S. U. Salunkhe, S. Salvador, A. Salvarese, A. Samajdar, A. Sanchez, E. J. Sanchez, L. E. Sanchez, N. Sanchis-Gual, J. R. Sanders, E. M. Sänger, F. Santoliquido, F. Sarandrea, T. R. Saravanan, N. Sarin, P. Sarkar, A. Sasli, P. Sassi, B. Sassolas, B. S. Sathyaprakash, R. Sato, S. Sato, Yukino Sato, Yu Sato, O. Sauter, R. L. Savage, T. Sawada, H. L. Sawant, S. Sayah, V. Scacco, D. Schaetzl, M. Scheel, A. Schiebelbein, M. G. Schiworski, P. Schmidt, S. Schmidt, R. Schnabel, M. Schneewind, R. M. S. Schofield, K. Schouteden, B. W. Schulte, B. F. Schutz, E. Schwartz, M. Scialpi, J. Scott, S. M. Scott, R. M. Sedas, T. C. Seetharamu, M. Seglar-Arroyo, Y. Sekiguchi, D. Sellers, N. Sembo, A. S. Sengupta, E. G. Seo, J. W. Seo, V. Sequino, M. Serra, A. Sevrin, T. Shaffer, U. S. Shah, M. A. Shaikh, L. Shao, A. K. Sharma, Preeti Sharma, Prianka Sharma, Ritwik Sharma, S. Sharma Chaudhary, P. Shawhan, N. S. Shcheblanov, E. Sheridan, Z. -H. Shi, M. Shikauchi, R. Shimomura, H. Shinkai, S. Shirke, D. H. Shoemaker, D. M. Shoemaker, R. W. Short, S. ShyamSundar, A. Sider, H. Siegel, D. Sigg, L. Silenzi, L. Silvestri, M. Simmonds, L. P. Singer, Amitesh Singh, Anika Singh, D. Singh, N. Singh, S. Singh, A. M. Sintes, V. Sipala, V. Skliris, B. J. J. Slagmolen, D. A. Slater, T. J. Slaven-Blair, J. Smetana, J. R. Smith, L. Smith, R. J. E. Smith, W. J. Smith, S. Soares de Albuquerque Filho, M. Soares-Santos, K. Somiya, I. Song, S. Soni, V. Sordini, F. Sorrentino, H. Sotani, F. Spada, V. Spagnuolo, A. P. Spencer, P. Spinicelli, A. K. Srivastava, F. Stachurski, C. J. Stark, D. A. Steer, N. Steinle, J. Steinlechner, S. Steinlechner, N. Stergioulas, P. Stevens, M. StPierre, M. D. Strong, A. Strunk, A. L. Stuver, M. Suchenek, S. Sudhagar, Y. Sudo, N. Sueltmann, L. Suleiman, K. D. Sullivan, J. Sun, L. Sun, S. Sunil, J. Suresh, B. J. Sutton, P. J. Sutton, K. Suzuki, M. Suzuki, B. L. Swinkels, A. Syx, M. J. Szczepańczyk, P. Szewczyk, M. Tacca, H. Tagoshi, K. Takada, H. Takahashi, R. Takahashi, A. Takamori, S. Takano, H. Takeda, K. Takeshita, I. Takimoto Schmiegelow, M. Takou-Ayaoh, C. Talbot, M. Tamaki, N. Tamanini, D. Tanabe, K. Tanaka, S. J. Tanaka, S. Tanioka, D. B. Tanner, W. Tanner, L. Tao, R. D. Tapia, E. N. Tapia San Martín, C. Taranto, A. Taruya, J. D. Tasson, J. G. Tau, D. Tellez, R. Tenorio, H. Themann, A. Theodoropoulos, M. P. Thirugnanasambandam, L. M. Thomas, M. Thomas, P. Thomas, J. E. Thompson, S. R. Thondapu, K. A. Thorne, E. Thrane, J. Tissino, A. Tiwari, Pawan Tiwari, Praveer Tiwari, S. Tiwari, V. Tiwari, M. R. Todd, M. Toffano, A. M. Toivonen, K. Toland, A. E. Tolley, T. Tomaru, V. Tommasini, T. Tomura, H. Tong, C. Tong-Yu, A. Torres-Forné, C. I. Torrie, I. Tosta e Melo, E. Tournefier, M. Trad Nery, K. Tran, A. Trapananti, R. Travaglini, F. Travasso, G. Traylor, M. Trevor, M. C. Tringali, A. Tripathee, G. Troian, A. Trovato, L. Trozzo, R. J. Trudeau, T. Tsang, S. Tsuchida, L. Tsukada, K. Turbang, M. Turconi, C. Turski, H. Ubach, N. Uchikata, T. Uchiyama, R. P. Udall, T. Uehara, K. Ueno, V. Undheim, L. E. Uronen, T. Ushiba, M. Vacatello, H. Vahlbruch, N. Vaidya, G. Vajente, A. Vajpeyi, J. Valencia, M. Valentini, S. A. Vallejo-Peña, S. Vallero, V. Valsan, M. van Dael, E. Van den Bossche, J. F. J. van den Brand, C. Van Den Broeck, M. van der Sluys, A. Van de Walle, J. van Dongen, K. Vandra, M. VanDyke, H. van Haevermaet, J. V. van Heijningen, P. Van Hove, J. Vanier, M. VanKeuren, J. Vanosky, N. van Remortel, M. Vardaro, A. F. Vargas, V. Varma, A. N. Vazquez, A. Vecchio, G. Vedovato, J. Veitch, P. J. Veitch, S. Venikoudis, R. C. Venterea, P. Verdier, M. Vereecken, D. Verkindt, B. Verma, Y. Verma, S. M. Vermeulen, F. Vetrano, A. Veutro, A. Viceré, S. Vidyant, A. D. Viets, A. Vijaykumar, A. Vilkha, N. Villanueva Espinosa, V. Villa-Ortega, E. T. Vincent, J. -Y. Vinet, S. Viret, S. Vitale, H. Vocca, D. Voigt, E. R. G. von Reis, J. S. A. von Wrangel, W. E. Vossius, L. Vujeva, S. P. Vyatchanin, J. Wack, L. E. Wade, M. Wade, K. J. Wagner, L. Wallace, E. J. Wang, H. Wang, J. Z. Wang, W. H. Wang, Y. F. Wang, G. Waratkar, J. Warner, M. Was, T. Washimi, N. Y. Washington, D. Watarai, B. Weaver, S. A. Webster, N. L. Weickhardt, M. Weinert, A. J. Weinstein, R. Weiss, L. Wen, K. Wette, J. T. Whelan, B. F. Whiting, C. Whittle, E. G. Wickens, D. Wilken, A. T. Wilkin, B. M. Williams, D. Williams, M. J. Williams, N. S. Williams, J. L. Willis, B. Willke, M. Wils, L. Wilson, C. W. Winborn, J. Winterflood, C. C. Wipf, G. Woan, J. Woehler, N. E. Wolfe, H. T. Wong, I. C. F. Wong, K. Wong, T. Wouters, J. L. Wright, M. Wright, B. Wu, C. Wu, D. S. Wu, H. Wu, K. Wu, Q. Wu, Y. Wu, Z. Wu, E. Wuchner, D. M. Wysocki, V. A. Xu, Y. Xu, N. Yadav, H. Yamamoto, K. Yamamoto, T. S. Yamamoto, T. Yamamoto, R. Yamazaki, T. Yan, K. Z. Yang, Y. Yang, Z. Yarbrough, J. Yebana, S. -W. Yeh, A. B. Yelikar, X. Yin, J. Yokoyama, T. Yokozawa, S. Yuan, H. Yuzurihara, M. Zanolin, M. Zeeshan, T. Zelenova, J. -P. Zendri, M. Zeoli, M. Zerrad, M. Zevin, L. Zhang, N. Zhang, R. Zhang, T. Zhang, C. Zhao, Yue Zhao, Yuhang Zhao, Z. -C. Zhao, Y. Zheng, H. Zhong, H. Zhou, H. O. Zhu, Z. -H. Zhu, A. B. Zimmerman, L. Zimmermann, M. E. Zucker, J. Zweizig",
    "topic": [
      "Music Information Retrieval"
    ],
    "category": [
      "Music"
    ]
  },
  {
    "paper_title": "LuxDiT: Lighting Estimation with Video Diffusion Transformer",
    "paper_title_zh": "LuxDiT：基于视频扩散 Transformer 的光照估计",
    "paper_id": "2509.03680v1",
    "paper_abstract": "Estimating scene lighting from a single image or video remains a longstanding challenge in computer vision and graphics. Learning-based approaches are constrained by the scarcity of ground-truth HDR environment maps, which are expensive to capture and limited in diversity. While recent generative models offer strong priors for image synthesis, lighting estimation remains difficult due to its reliance on indirect visual cues, the need to infer global (non-local) context, and the recovery of high-dynamic-range outputs. We propose LuxDiT, a novel data-driven approach that fine-tunes a video diffusion transformer to generate HDR environment maps conditioned on visual input. Trained on a large synthetic dataset with diverse lighting conditions, our model learns to infer illumination from indirect visual cues and generalizes effectively to real-world scenes. To improve semantic alignment between the input and the predicted environment map, we introduce a low-rank adaptation finetuning strategy using a collected dataset of HDR panoramas. Our method produces accurate lighting predictions with realistic angular high-frequency details, outperforming existing state-of-the-art techniques in both quantitative and qualitative evaluations.",
    "paper_abstract_zh": "从单张图像或视频估计场景光照一直是计算机视觉与图形学的长期难题。基于学习的方法受限于真实 HDR 环境贴图稀缺，其采集昂贵且多样性不足。尽管最新生成模型为图像合成提供了强先验，光照估计仍因依赖间接视觉线索、需推断全局（非局部）上下文并恢复高动态范围输出而困难重重。我们提出 LuxDiT，一种新颖的数据驱动方法，通过微调视频扩散 Transformer，根据视觉输入生成 HDR 环境贴图。模型在包含多样光照条件的大规模合成数据集上训练，学会从间接视觉线索推断光照，并能泛化到真实场景。为提升输入与预测环境贴图间的语义对齐，我们引入基于收集的 HDR 全景数据集的低秩适配微调策略。实验表明，该方法生成的光照预测在角域高频细节方面更加真实，在定量与定性评估中均优于现有最先进方法。",
    "primary_category": "cs.GR",
    "update_time": "2025-09-03",
    "paper_authors": "Ruofan Liang, Kai He, Zan Gojcic, Igor Gilitschenski, Sanja Fidler, Nandita Vijaykumar, Zian Wang",
    "topic": [
      "Image Generation",
      "Video Generation"
    ],
    "category": [
      "Image"
    ]
  },
  {
    "paper_title": "Efficient Virtuoso: A Latent Diffusion Transformer Model for Goal-Conditioned Trajectory Planning",
    "paper_title_zh": "高效 Virtuoso：面向目标条件轨迹规划的潜在扩散 Transformer 模型",
    "paper_id": "2509.03658v1",
    "paper_abstract": "The ability to generate a diverse and plausible distribution of future trajectories is a critical capability for autonomous vehicle planning systems. While recent generative models have shown promise, achieving high fidelity, computational efficiency, and precise control remains a significant challenge. In this paper, we present the \\textbf{Efficient Virtuoso}, a conditional latent diffusion model for goal-conditioned trajectory planning. Our approach introduces a novel two-stage normalization pipeline that first scales trajectories to preserve their geometric aspect ratio and then normalizes the resulting PCA latent space to ensure a stable training target. The denoising process is performed efficiently in this low-dimensional latent space by a simple MLP denoiser, which is conditioned on a rich scene context fused by a powerful Transformer-based StateEncoder. We demonstrate that our method achieves state-of-the-art performance on the Waymo Open Motion Dataset, reaching a \\textbf{minADE of 0.25}. Furthermore, through a rigorous ablation study on goal representation, we provide a key insight: while a single endpoint goal can resolve strategic ambiguity, a richer, multi-step sparse route is essential for enabling the precise, high-fidelity tactical execution that mirrors nuanced human driving behavior.",
    "paper_abstract_zh": "生成多样且合理的未来轨迹分布是自动驾驶规划系统的关键能力。尽管近期生成模型已显潜力，但在高保真、计算效率与精确控制方面仍面临重大挑战。本文提出“高效 Virtuoso”，一种用于目标条件轨迹规划的条件潜在扩散模型。我们引入新颖的两阶段归一化流程：首先按比例缩放轨迹以保持几何宽高比，随后对所得 PCA 潜在空间进行归一化，确保训练目标稳定。去噪过程在该低维潜在空间中由简单 MLP 去噪器高效完成，并以强大的 Transformer 状态编码器融合丰富场景上下文为条件。在 Waymo Open Motion Dataset 上，我们的方法达到 0.25 的最小平均位移误差（minADE），实现最先进性能。通过对目标表征的严格消融实验，我们得出关键洞见：单点终点目标可解决战略歧义，而更丰富、多步稀疏路径对于实现精确、高保真的战术执行、复现细腻人类驾驶行为至关重要。",
    "primary_category": "cs.RO",
    "update_time": "2025-09-03",
    "paper_authors": "Antonio Guillen-Perez",
    "topic": [
      "Image Generation"
    ],
    "category": [
      "Image"
    ]
  },
  {
    "paper_title": "NoiseNoise is All You Need: rethinking the value of noise on seismic denoising via diffusion models",
    "paper_title_zh": "NoiseNoise 即所需：通过扩散模型重新思考噪声在地震去噪中的价值",
    "paper_id": "2509.03629v1",
    "paper_abstract": "We introduce SeisDiff-denoNIA, a novel diffusion-based seismic denoising framework that trains directly on field noise, eliminating the reliance on synthetic datasets. Unlike conventional denoising methods that require clean signal labels, our approach leverages field noise extracted prior to first arrivals as training targets, allowing the diffusion model to explicitly learn the true noise distribution. The model demonstrates robust performance on field DAS-VSP data contaminated by different noise types, and significantly outperforms traditional signal-based diffusion models under low SNR conditions in synthetic tests. The results suggest that explicitly modeling noise is not only viable but advantageous for seismic denoising tasks.",
    "paper_abstract_zh": "我们提出 SeisDiff-denoNIA，一种新颖的基于扩散的地震去噪框架，直接利用野外噪声训练，无需依赖合成数据集。与传统需干净信号标签的去噪方法不同，本方法利用初至前提取的野外噪声作为训练目标，使扩散模型显式学习真实噪声分布。模型在受多种噪声污染的野外 DAS-VSP 数据上表现稳健，并在低信噪比条件下的合成测试中显著优于传统基于信号的扩散模型。结果表明，显式建模噪声不仅可行，而且对地震去噪任务更具优势。",
    "primary_category": "physics.geo-ph",
    "update_time": "2025-09-03",
    "paper_authors": "Donglin Zhu, Peiyao Li, Ge Jin",
    "topic": [
      "Image Generation"
    ],
    "category": [
      "Image"
    ]
  },
  {
    "paper_title": "Joint Training of Image Generator and Detector for Road Defect Detection",
    "paper_title_zh": "道路缺陷检测中图像生成器与检测器的联合训练",
    "paper_id": "2509.03465v1",
    "paper_abstract": "Road defect detection is important for road authorities to reduce the vehicle damage caused by road defects. Considering the practical scenarios where the defect detectors are typically deployed on edge devices with limited memory and computational resource, we aim at performing road defect detection without using ensemble-based methods or test-time augmentation (TTA). To this end, we propose to Jointly Train the image Generator and Detector for road defect detection (dubbed as JTGD). We design the dual discriminators for the generative model to enforce both the synthesized defect patches and overall images to look plausible. The synthesized image quality is improved by our proposed CLIP-based Fr\\'echet Inception Distance loss. The generative model in JTGD is trained jointly with the detector to encourage the generative model to synthesize harder examples for the detector. Since harder synthesized images of better quality caused by the aforesaid design are used in the data augmentation, JTGD outperforms the state-of-the-art method in the RDD2022 road defect detection benchmark across various countries under the condition of no ensemble and TTA. JTGD only uses less than 20% of the number of parameters compared with the competing baseline, which makes it more suitable for deployment on edge devices in practice.",
    "paper_abstract_zh": "道路缺陷检测对道路管理部门减少车辆损坏至关重要。鉴于实际场景中缺陷检测器通常部署在内存与计算资源受限的边缘设备上，我们旨在在不使用集成方法或测试时增强（TTA）的条件下完成道路缺陷检测。为此，我们提出联合训练图像生成器与检测器进行道路缺陷检测（简称 JTGD）。我们为生成模型设计双重判别器，迫使合成的缺陷补丁与整体图像均显得真实。通过提出的基于 CLIP 的 Fréchet Inception Distance 损失，进一步提升合成图像质量。JTGD 中的生成器与检测器联合训练，鼓励生成器合成对检测器更具挑战性的样本。由于上述设计带来的更高质量、更具挑战性的合成图像被用于数据增强，JTGD 在无需集成与 TTA 的条件下，于 RDD2022 多国道路缺陷检测基准上超越现有最先进方法。JTGD 的参数量仅为竞争基线的不到 20%，更适合实际部署于边缘设备。",
    "primary_category": "cs.CV",
    "update_time": "2025-09-03",
    "paper_authors": "Kuan-Chuan Peng",
    "topic": [
      "Image Generation",
      "Multimodal Generation"
    ],
    "category": [
      "Image"
    ]
  },
  {
    "paper_title": "ANNIE: Be Careful of Your Robots",
    "paper_title_zh": "ANNIE：小心你的机器人",
    "paper_id": "2509.03383v1",
    "paper_abstract": "The integration of vision-language-action (VLA) models into embodied AI (EAI) robots is rapidly advancing their ability to perform complex, long-horizon tasks in humancentric environments. However, EAI systems introduce critical security risks: a compromised VLA model can directly translate adversarial perturbations on sensory input into unsafe physical actions. Traditional safety definitions and methodologies from the machine learning community are no longer sufficient. EAI systems raise new questions, such as what constitutes safety, how to measure it, and how to design effective attack and defense mechanisms in physically grounded, interactive settings. In this work, we present the first systematic study of adversarial safety attacks on embodied AI systems, grounded in ISO standards for human-robot interactions. We (1) formalize a principled taxonomy of safety violations (critical, dangerous, risky) based on physical constraints such as separation distance, velocity, and collision boundaries; (2) introduce ANNIEBench, a benchmark of nine safety-critical scenarios with 2,400 video-action sequences for evaluating embodied safety; and (3) ANNIE-Attack, a task-aware adversarial framework with an attack leader model that decomposes long-horizon goals into frame-level perturbations. Our evaluation across representative EAI models shows attack success rates exceeding 50% across all safety categories. We further demonstrate sparse and adaptive attack strategies and validate the real-world impact through physical robot experiments. These results expose a previously underexplored but highly consequential attack surface in embodied AI systems, highlighting the urgent need for security-driven defenses in the physical AI era. Code is available at https://github.com/RLCLab/Annie.",
    "paper_abstract_zh": "将视觉-语言-动作（VLA）模型集成到具身人工智能（EAI）机器人中，正迅速提升其在人类中心环境中执行复杂长周期任务的能力。然而，EAI 系统引入了关键的安全风险：一旦 VLA 模型被攻破，针对感官输入的对抗扰动可直接转化为不安全的物理行为。传统机器学习社区的安全定义与方法已不敷使用。EAI 系统提出了新问题：何为安全、如何度量安全，以及如何在具有物理基础且交互式的环境中设计有效的攻击与防御机制。本文首次基于 ISO 人机交互标准，对具身 AI 系统的对抗性安全攻击展开系统研究。我们（1）依据分离距离、速度、碰撞边界等物理约束，形式化了一套安全违规的分级原则（严重、危险、有风险）；（2）推出 ANNIEBench 基准，涵盖 9 个安全关键场景、2,400 段视频-动作序列，用于评估具身安全；（3）提出 ANNIE-Attack，一种任务感知的对抗框架，其攻击主模型将长周期目标分解为帧级扰动。在代表性 EAI 模型上的评估显示，所有安全类别的攻击成功率均超过 50%。我们进一步展示了稀疏且自适应的攻击策略，并通过实体机器人实验验证了现实影响。结果揭示了具身 AI 系统中一个此前被忽视却影响巨大的攻击面，凸显物理 AI 时代亟需以安全为核心的防御措施。代码开源：https://github.com/RLCLab/Annie。",
    "primary_category": "cs.AI",
    "update_time": "2025-09-03",
    "paper_authors": "Yiyang Huang, Zixuan Wang, Zishen Wan, Yapeng Tian, Haobo Xu, Yinhe Han, Yiming Gan",
    "topic": [
      "Image Generation"
    ],
    "category": [
      "Image"
    ]
  },
  {
    "paper_title": "On the MIA Vulnerability Gap Between Private GANs and Diffusion Models",
    "paper_title_zh": "私有 GAN 与扩散模型在成员推理脆弱性上的差距研究",
    "paper_id": "2509.03341v1",
    "paper_abstract": "Generative Adversarial Networks (GANs) and diffusion models have emerged as leading approaches for high-quality image synthesis. While both can be trained under differential privacy (DP) to protect sensitive data, their sensitivity to membership inference attacks (MIAs), a key threat to data confidentiality, remains poorly understood. In this work, we present the first unified theoretical and empirical analysis of the privacy risks faced by differentially private generative models. We begin by showing, through a stability-based analysis, that GANs exhibit fundamentally lower sensitivity to data perturbations than diffusion models, suggesting a structural advantage in resisting MIAs. We then validate this insight with a comprehensive empirical study using a standardized MIA pipeline to evaluate privacy leakage across datasets and privacy budgets. Our results consistently reveal a marked privacy robustness gap in favor of GANs, even in strong DP regimes, highlighting that model type alone can critically shape privacy leakage.",
    "paper_abstract_zh": "生成对抗网络（GAN）与扩散模型已成为高质量图像合成的两大主流方法。二者均可在差分隐私（DP）下训练以保护敏感数据，但它们对成员推理攻击（MIA）——威胁数据机密性的关键攻击——的敏感程度仍不清楚。本文首次对差分隐私生成模型所面临的隐私风险进行统一的理论与实证分析。我们通过基于稳定性的分析证明，GAN 对数据扰动的固有敏感度显著低于扩散模型，表明其在抵御 MIA 方面具有结构性优势。随后，我们使用标准化 MIA 流程，在多个数据集与隐私预算下对隐私泄露进行全面实证评估。结果一致显示，即使在强 DP 机制下，GAN 仍明显更具隐私鲁棒性，凸显模型类型本身对隐私泄露具有决定性影响。",
    "primary_category": "cs.LG",
    "update_time": "2025-09-03",
    "paper_authors": "Ilana Sebag, Jean-Yves Franceschi, Alain Rakotomamonjy, Alexandre Allauzen, Jamal Atif",
    "topic": [
      "Image Generation"
    ],
    "category": [
      "Image"
    ]
  },
  {
    "paper_title": "InfraDiffusion: zero-shot depth map restoration with diffusion models and prompted segmentation from sparse infrastructure point clouds",
    "paper_title_zh": "InfraDiffusion：基于扩散模型与提示分割的零样本深度图修复，用于稀疏基础设施点云",
    "paper_id": "2509.03324v1",
    "paper_abstract": "Point clouds are widely used for infrastructure monitoring by providing geometric information, where segmentation is required for downstream tasks such as defect detection. Existing research has automated semantic segmentation of structural components, while brick-level segmentation (identifying defects such as spalling and mortar loss) has been primarily conducted from RGB images. However, acquiring high-resolution images is impractical in low-light environments like masonry tunnels. Point clouds, though robust to dim lighting, are typically unstructured, sparse, and noisy, limiting fine-grained segmentation. We present InfraDiffusion, a zero-shot framework that projects masonry point clouds into depth maps using virtual cameras and restores them by adapting the Denoising Diffusion Null-space Model (DDNM). Without task-specific training, InfraDiffusion enhances visual clarity and geometric consistency of depth maps. Experiments on masonry bridge and tunnel point cloud datasets show significant improvements in brick-level segmentation using the Segment Anything Model (SAM), underscoring its potential for automated inspection of masonry assets. Our code and data is available at https://github.com/Jingyixiong/InfraDiffusion-official-implement.",
    "paper_abstract_zh": "点云因提供几何信息而被广泛用于基础设施监测，其中分割是缺陷检测等下游任务的前提。现有研究已实现对结构构件的自动语义分割，而砖块级分割（识别剥落、砂浆流失等缺陷）主要依赖 RGB 图像。然而，在砖石隧道等弱光环境中获取高分辨率图像并不现实。点云虽对弱光鲁棒，但通常无结构、稀疏且含噪，限制了细粒度分割。我们提出 InfraDiffusion，一个零样本框架：通过虚拟相机将砖石点云投影为深度图，并基于去噪扩散零空间模型（DDNM）进行修复。无需任务特定训练，InfraDiffusion 即可提升深度图的视觉清晰度与几何一致性。在砖石桥梁与隧道点云数据集上的实验表明，使用 Segment Anything 模型（SAM）进行砖块级分割的性能显著提升，展现了其在砖石资产自动巡检中的潜力。代码与数据开源：https://github.com/Jingyixiong/InfraDiffusion-official-implement。",
    "primary_category": "cs.CV",
    "update_time": "2025-09-03",
    "paper_authors": "Yixiong Jing, Cheng Zhang, Haibing Wu, Guangming Wang, Olaf Wysocki, Brian Sheil",
    "topic": [
      "Image Generation"
    ],
    "category": [
      "Image"
    ]
  },
  {
    "paper_title": "SynBT: High-quality Tumor Synthesis for Breast Tumor Segmentation by 3D Diffusion Model",
    "paper_title_zh": "SynBT：基于 3D 扩散模型的高质量乳腺肿瘤合成用于分割",
    "paper_id": "2509.03267v1",
    "paper_abstract": "Synthetic tumors in medical images offer controllable characteristics that facilitate the training of machine learning models, leading to an improved segmentation performance. However, the existing methods of tumor synthesis yield suboptimal performances when tumor occupies a large spatial volume, such as breast tumor segmentation in MRI with a large field-of-view (FOV), while commonly used tumor generation methods are based on small patches. In this paper, we propose a 3D medical diffusion model, called SynBT, to generate high-quality breast tumor (BT) in contrast-enhanced MRI images. The proposed model consists of a patch-to-volume autoencoder, which is able to compress the high-resolution MRIs into compact latent space, while preserving the resolution of volumes with large FOV. Using the obtained latent space feature vector, a mask-conditioned diffusion model is used to synthesize breast tumors within selected regions of breast tissue, resulting in realistic tumor appearances. We evaluated the proposed method for a tumor segmentation task, which demonstrated the proposed high-quality tumor synthesis method can facilitate the common segmentation models with performance improvement of 2-3% Dice Score on a large public dataset, and therefore provides benefits for tumor segmentation in MRI images.",
    "paper_abstract_zh": "医学图像中的合成肿瘤具有可控特性，有助于提升机器学习模型的分割性能。然而，当肿瘤占据较大空间体积（如大视野（FOV）乳腺 MRI 分割）时，现有肿瘤合成方法表现欠佳，而主流方法仅基于小块区域生成。本文提出一种 3D 医学扩散模型 SynBT，用于在对比增强 MRI 中生成高质量乳腺肿瘤（BT）。该模型包含一个块到体积的自编码器，可将高分辨率 MRI 压缩至紧凑潜空间，同时保留大 FOV 体数据的分辨率。利用所得潜空间特征向量，掩膜条件扩散模型在选定的乳腺组织区域内合成肿瘤，呈现逼真的肿瘤外观。我们在肿瘤分割任务上评估该方法，结果显示高质量肿瘤合成可将公开大尺度数据集上常用分割模型的 Dice 分数提升 2–3%，从而为 MRI 肿瘤分割带来实际益处。",
    "primary_category": "cs.CV",
    "update_time": "2025-09-03",
    "paper_authors": "Hongxu Yang, Edina Timko, Levente Lippenszky, Vanda Czipczer, Lehel Ferenczi",
    "topic": [
      "Image Generation"
    ],
    "category": [
      "Image"
    ]
  },
  {
    "paper_title": "Estudio de la eficiencia en la escalabilidad de GPUs para el entrenamiento de Inteligencia Artificial",
    "paper_title_zh": "GPU 可扩展性在人工智能训练中的效率研究",
    "paper_id": "2509.03263v1",
    "paper_abstract": "Training large-scale deep learning models has become a key challenge for the scientific community and industry. While the massive use of GPUs can significantly speed up training times, this approach has a negative impact on efficiency. In this article, we present a detailed analysis of the times reported by MLPerf Training v4.1 on four workloads: BERT, Llama2 LoRA, RetinaNet, and Stable Diffusion, showing that there are configurations that optimise the relationship between performance, GPU usage, and efficiency. The results point to a break-even point that allows training times to be reduced while maximising efficiency.",
    "paper_abstract_zh": "大规模深度学习模型的训练已成为学术界与工业界的关键挑战。虽然大规模使用 GPU 可显著缩短训练时间，但该做法对效率产生负面影响。本文详细分析了 MLPerf Training v4.1 在四种工作负载（BERT、Llama2 LoRA、RetinaNet 和 Stable Diffusion）上的耗时数据，指出存在能够优化性能、GPU 使用量与效率之间关系的配置。结果揭示了一个盈亏平衡点，可在缩短训练时间的同时最大化效率。",
    "primary_category": "cs.LG",
    "update_time": "2025-09-03",
    "paper_authors": "David Cortes, Carlos Juiz, Belen Bermejo",
    "topic": [
      "Image Generation"
    ],
    "category": [
      "Image"
    ]
  },
  {
    "paper_title": "Temporally-Aware Diffusion Model for Brain Progression Modelling with Bidirectional Temporal Regularisation",
    "paper_title_zh": "用于脑进展建模的双向时间正则化时间感知三维扩散模型",
    "paper_id": "2509.03141v1",
    "paper_abstract": "Generating realistic MRIs to accurately predict future changes in the structure of brain is an invaluable tool for clinicians in assessing clinical outcomes and analysing the disease progression at the patient level. However, current existing methods present some limitations: (i) some approaches fail to explicitly capture the relationship between structural changes and time intervals, especially when trained on age-imbalanced datasets; (ii) others rely only on scan interpolation, which lack clinical utility, as they generate intermediate images between timepoints rather than future pathological progression; and (iii) most approaches rely on 2D slice-based architectures, thereby disregarding full 3D anatomical context, which is essential for accurate longitudinal predictions. We propose a 3D Temporally-Aware Diffusion Model (TADM-3D), which accurately predicts brain progression on MRI volumes. To better model the relationship between time interval and brain changes, TADM-3D uses a pre-trained Brain-Age Estimator (BAE) that guides the diffusion model in the generation of MRIs that accurately reflect the expected age difference between baseline and generated follow-up scans. Additionally, to further improve the temporal awareness of TADM-3D, we propose the Back-In-Time Regularisation (BITR), by training TADM-3D to predict bidirectionally from the baseline to follow-up (forward), as well as from the follow-up to baseline (backward). Although predicting past scans has limited clinical applications, this regularisation helps the model generate temporally more accurate scans. We train and evaluate TADM-3D on the OASIS-3 dataset, and we validate the generalisation performance on an external test set from the NACC dataset. The code will be available upon acceptance.",
    "paper_abstract_zh": "生成能够准确预测未来脑部结构变化的逼真 MRI，是临床医生评估临床结局和个体层面分析疾病进展的宝贵工具。然而，现有方法存在以下局限：（i）部分方法未能显式刻画结构变化与时间间隔的关系，尤其在年龄分布不平衡的数据集上训练时；（ii）另一些仅依赖扫描插值，缺乏临床价值，因其仅生成两个时间点之间的中间图像，而非未来病理进展；（iii）大多数采用 2D 切片架构，忽视完整的三维解剖上下文，而这对纵向预测至关重要。我们提出三维时间感知扩散模型（TADM-3D），可在 MRI 体素上准确预测脑部进展。为更好地建模时间间隔与脑变化的关系，TADM-3D 引入预训练的脑年龄估计器（BAE），引导扩散模型生成能准确反映基线与生成随访扫描之间预期年龄差异的 MRI。此外，我们提出“回到过去”正则化（BITR），通过让 TADM-3D 双向训练——从基线预测随访（正向）以及从随访预测基线（反向）——进一步增强其时间感知能力。尽管预测过去扫描的临床意义有限，但该正则化帮助模型生成时间更准确的扫描。我们在 OASIS-3 数据集上训练并评估 TADM-3D，并在来自 NACC 的外部测试集上验证其泛化性能。代码将在论文接收后公开。",
    "primary_category": "cs.CV",
    "update_time": "2025-09-03",
    "paper_authors": "Mattia Litrico, Francesco Guarnera, Mario Valerio Giuffrida, Daniele Ravì, Sebastiano Battiato",
    "topic": [
      "Image Generation"
    ],
    "category": [
      "Image"
    ]
  },
  {
    "paper_title": "RecBase: Generative Foundation Model Pretraining for Zero-Shot Recommendation",
    "paper_title_zh": "RecBase：面向零样本推荐的生成式基础模型预训练",
    "paper_id": "2509.03131v1",
    "paper_abstract": "Recent advances in LLM-based recommendation have shown promise, yet their cross-domain generalization is hindered by a fundamental mismatch between language-centric pretraining and the recommendation task. Existing methods, relying on language-level knowledge, fail to capture dynamic, item-level user interests across domains. To bridge this gap, we propose RecBase, a domain-agnostic foundational model pretrained with a recommendation-oriented objective. RecBase leverages a large-scale, heterogeneous, cross-domain corpus with unified textual representations and feature mappings to enhance cross-domain generalization. To further align item semantics across domains, we introduce a unified item tokenizer that encodes items into hierarchical concept identifiers, enabling structured representation and efficient vocabulary sharing. The model is trained using an autoregressive objective to capture complex item-level sequential patterns. On eight real-world datasets, our 1.5B-parameter model matches or surpasses the performance of LLM baselines up to 7B parameters in zero-shot and cross-domain recommendation tasks.",
    "paper_abstract_zh": "近期基于大语言模型（LLM）的推荐系统取得进展，但其跨域泛化能力受限于以语言为中心的预训练与推荐任务之间的根本错位。现有方法依赖语言层面知识，难以捕捉跨域动态、细粒度的物品级用户兴趣。为弥合这一差距，我们提出 RecBase——一种领域无关的基础模型，采用面向推荐的预训练目标。RecBase 利用大规模、异构、跨域语料，通过统一的文本表示与特征映射增强跨域泛化。为进一步对齐跨域物品语义，我们设计统一物品分词器，将物品编码为分层概念标识符，实现结构化表示与高效词表共享。模型采用自回归目标训练，以捕捉复杂的物品级序列模式。在八个真实世界数据集上，我们 15 亿参数的模型在零样本与跨域推荐任务中匹配或超越参数量高达 70 亿的 LLM 基线。",
    "primary_category": "cs.IR",
    "update_time": "2025-09-03",
    "paper_authors": "Sashuai Zhou, Weinan Gan, Qijiong Liu, Ke Lei, Jieming Zhu, Hai Huang, Yan Xia, Ruiming Tang, Zhenhua Dong, Zhou Zhao",
    "topic": [
      "Image Generation"
    ],
    "category": [
      "Image"
    ]
  },
  {
    "paper_title": "High Cursive Complex Character Recognition using GAN External Classifier",
    "paper_title_zh": "基于 GAN 外部分类器的高连笔复杂字符识别",
    "paper_id": "2509.03062v1",
    "paper_abstract": "Handwritten characters can be trickier to classify due to their complex and cursive nature compared to simple and non-cursive characters. We present an external classifier along with a Generative Adversarial Network that can classify highly cursive and complex characters. The generator network produces fake handwritten character images, which are then used to augment the training data after adding adversarially perturbed noise and achieving a confidence score above a threshold with the discriminator network. The results show that the accuracy of convolutional neural networks decreases as character complexity increases, but our proposed model, ADA-GAN, remains more robust and effective for both cursive and complex characters.",
    "paper_abstract_zh": "相比简单非连笔字符，手写字符因其复杂且连笔的特性更难分类。我们提出一种结合生成对抗网络（GAN）的外部分类器，可对高度连笔且复杂的字符进行识别。生成器网络生成伪造手写字符图像，经对抗扰动噪声增强后，若通过判别器网络且置信度高于阈值，则用于扩充训练数据。实验表明，随着字符复杂度增加，卷积神经网络准确率下降，但我们提出的 ADA-GAN 模型对连笔与复杂字符均保持更强鲁棒性与有效性。",
    "primary_category": "cs.CV",
    "update_time": "2025-09-03",
    "paper_authors": "S M Rafiuddin",
    "topic": [
      "Image Generation"
    ],
    "category": [
      "Image"
    ]
  },
  {
    "paper_title": "DCDB: Dynamic Conditional Dual Diffusion Bridge for Ill-posed Multi-Tasks",
    "paper_title_zh": "DCDB：面向病态多任务的场景动态条件双扩散桥",
    "paper_id": "2509.03044v1",
    "paper_abstract": "Conditional diffusion models have made impressive progress in the field of image processing, but the characteristics of constructing data distribution pathways make it difficult to exploit the intrinsic correlation between tasks in multi-task scenarios, which is even worse in ill-posed tasks with a lack of training data. In addition, traditional static condition control makes it difficult for networks to learn in multi-task scenarios with its dynamically evolving characteristics. To address these challenges, we propose a dynamic conditional double diffusion bridge training paradigm to build a general framework for ill-posed multi-tasks. Firstly, this paradigm decouples the diffusion and condition generation processes, avoiding the dependence of the diffusion model on supervised data in ill-posed tasks. Secondly, generated by the same noise schedule, dynamic conditions are used to gradually adjust their statistical characteristics, naturally embed time-related information, and reduce the difficulty of network learning. We analyze the learning objectives of the network under different conditional forms in the single-step denoising process and compare the changes in its attention weights in the network, demonstrating the superiority of our dynamic conditions. Taking dehazing and visible-infrared fusion as typical ill-posed multi-task scenarios, we achieve the best performance in multiple indicators on public datasets. The code has been publicly released at: https://anonymous.4open.science/r/DCDB-D3C2.",
    "paper_abstract_zh": "条件扩散模型在图像处理领域取得显著进展，但其构建数据分布路径的特性使其在多任务场景中难以挖掘任务间内在关联，在训练数据匮乏的病态任务中尤为严重。此外，传统静态条件控制难以适应多任务场景动态演化的特点。为此，我们提出动态条件双扩散桥训练范式，为病态多任务构建通用框架。首先，该范式将扩散过程与条件生成过程解耦，避免扩散模型在病态任务中对监督数据的依赖；其次，基于相同噪声调度生成动态条件，逐步调整其统计特性，自然嵌入时间相关信息，降低网络学习难度。我们分析了单步去噪过程中不同条件形式下的网络学习目标，并比较其注意力权重变化，验证动态条件的优越性。以去雾与可见光-红外融合为典型病态多任务场景，我们在公开数据集的多项指标上取得最佳性能。代码已开源：https://anonymous.4open.science/r/DCDB-D3C2。",
    "primary_category": "cs.CV",
    "update_time": "2025-09-03",
    "paper_authors": "Chengjie Huang, Jiafeng Yan, Jing Li, Lu Bai",
    "topic": [
      "Image Generation"
    ],
    "category": [
      "Image"
    ]
  },
  {
    "paper_title": "Enhancing Robustness in Post-Processing Watermarking: An Ensemble Attack Network Using CNNs and Transformers",
    "paper_title_zh": "增强后处理水印鲁棒性：基于 CNN 与 Transformer 的集成攻击网络",
    "paper_id": "2509.03006v1",
    "paper_abstract": "Recent studies on deep watermarking have predominantly focused on in-processing watermarking, which integrates the watermarking process into image generation. However, post-processing watermarking, which embeds watermarks after image generation, offers more flexibility. It can be applied to outputs from any generative model (e.g. GANs, diffusion models) without needing access to the model's internal structure. It also allows users to embed unique watermarks into individual images. Therefore, this study focuses on post-processing watermarking and enhances its robustness by incorporating an ensemble attack network during training. We construct various versions of attack networks using CNN and Transformer in both spatial and frequency domains to investigate how each combination influences the robustness of the watermarking model. Our results demonstrate that combining a CNN-based attack network in the spatial domain with a Transformer-based attack network in the frequency domain yields the highest robustness in watermarking models. Extensive evaluation on the WAVES benchmark, using average bit accuracy as the metric, demonstrates that our ensemble attack network significantly enhances the robustness of baseline watermarking methods under various stress tests. In particular, for the Regeneration Attack defined in WAVES, our method improves StegaStamp by 18.743%. The code is released at:https://github.com/aiiu-lab/DeepRobustWatermark.",
    "paper_abstract_zh": "近期深度水印研究主要关注“处理中”水印，即将水印嵌入过程融入图像生成。然而，“后处理”水印在图像生成后再嵌入水印，具有更高灵活性：可应用于任何生成模型（如 GAN、扩散模型）的输出，无需访问模型内部结构，也允许用户为单张图像嵌入独特水印。因此，本研究聚焦后处理水印，通过在训练阶段引入集成攻击网络增强其鲁棒性。我们构建多种基于 CNN 与 Transformer 的空间域与频域攻击网络版本，探究不同组合对水印模型鲁棒性的影响。实验表明，空间域 CNN 攻击网络与频域 Transformer 攻击网络相结合，可获得最高鲁棒性。在 WAVES 基准上的广泛评估以平均比特准确率为指标，显示我们的集成攻击网络显著提升了基线水印方法在各种压力测试下的鲁棒性。特别地，对于 WAVES 定义的再生攻击，我们的方法将 StegaStamp 性能提升 18.743%。代码已开源：https://github.com/aiiu-lab/DeepRobustWatermark。",
    "primary_category": "cs.CV",
    "update_time": "2025-09-03",
    "paper_authors": "Tzuhsuan Huang, Cheng Yu Yeo, Tsai-Ling Huang, Hong-Han Shuai, Wen-Huang Cheng, Jun-Cheng Chen",
    "topic": [
      "Image Generation"
    ],
    "category": [
      "Image"
    ]
  },
  {
    "paper_title": "InstaDA: Augmenting Instance Segmentation Data with Dual-Agent System",
    "paper_title_zh": "InstaDA：基于双智能体系统的实例分割数据增强方法",
    "paper_id": "2509.02973v1",
    "paper_abstract": "Acquiring high-quality instance segmentation data is challenging due to the labor-intensive nature of the annotation process and significant class imbalances within datasets. Recent studies have utilized the integration of Copy-Paste and diffusion models to create more diverse datasets. However, these studies often lack deep collaboration between large language models (LLMs) and diffusion models, and underutilize the rich information within the existing training data. To address these limitations, we propose InstaDA, a novel, training-free Dual-Agent system designed to augment instance segmentation datasets. First, we introduce a Text-Agent (T-Agent) that enhances data diversity through collaboration between LLMs and diffusion models. This agent features a novel Prompt Rethink mechanism, which iteratively refines prompts based on the generated images. This process not only fosters collaboration but also increases image utilization and optimizes the prompts themselves. Additionally, we present an Image-Agent (I-Agent) aimed at enriching the overall data distribution. This agent augments the training set by generating new instances conditioned on the training images. To ensure practicality and efficiency, both agents operate as independent and automated workflows, enhancing usability. Experiments conducted on the LVIS 1.0 validation set indicate that InstaDA achieves significant improvements, with an increase of +4.0 in box average precision (AP) and +3.3 in mask AP compared to the baseline. Furthermore, it outperforms the leading model, DiverGen, by +0.3 in box AP and +0.1 in mask AP, with a notable +0.7 gain in box AP on common categories and mask AP gains of +0.2 on common categories and +0.5 on frequent categories.",
    "paper_abstract_zh": "由于标注过程劳动密集且数据集存在严重的类别不平衡，获取高质量的实例分割数据颇具挑战。近期研究尝试将复制-粘贴与扩散模型结合以生成更多样化的数据，但往往缺乏大语言模型（LLM）与扩散模型之间的深度协作，也未能充分利用现有训练数据中的丰富信息。为此，我们提出 InstaDA——一种无需训练的新型双智能体系统，用于实例分割数据增强。首先，我们引入文本智能体（T-Agent），通过 LLM 与扩散模型的协同提升数据多样性；该智能体提出“提示再思”机制，可根据生成图像迭代优化提示词，从而促进协作、提高图像利用率并优化提示本身。其次，我们设计图像智能体（I-Agent），以训练图像为条件生成新实例，丰富整体数据分布。两大智能体各自独立、自动运行，兼顾实用与高效。在 LVIS 1.0 验证集上的实验表明，InstaDA 相比基线将框 AP 提升 4.0，掩膜 AP 提升 3.3；相比当前最佳方法 DiverGen，框 AP 再提高 0.3，掩膜 AP 提高 0.1，其中常见类别框 AP 增益达 0.7，掩膜 AP 在常见与频繁类别上分别提升 0.2 与 0.5。",
    "primary_category": "cs.CV",
    "update_time": "2025-09-03",
    "paper_authors": "Xianbao Hou, Yonghao He, Zeyd Boukhers, John See, Hu Su, Wei Sui, Cong Yang",
    "topic": [
      "Image Generation"
    ],
    "category": [
      "Image"
    ]
  },
  {
    "paper_title": "Non-Linear and Meta-Stable Dynamics in Financial Markets: Evidence from High Frequency Crypto Currency Market Makers",
    "paper_title_zh": "金融市场的非线性与亚稳态动力学：来自高频加密货币做市商的证据",
    "paper_id": "2509.02941v1",
    "paper_abstract": "This work builds upon the long-standing conjecture that linear diffusion models are inadequate for complex market dynamics. Specifically, it provides experimental validation for the author's prior arguments that realistic market dynamics are governed by higher-order (cubic and higher) non-linearities in the drift. As the diffusion drift is given by the negative gradient of a potential function, this means that a non-linear drift translates into a non-quadratic potential. These arguments were based both on general theoretical grounds as well as a structured approach to modeling the price dynamics which incorporates money flows and their impact on market prices. Here, we find direct confirmation of this view by analyzing high-frequency crypto currency data at different time scales ranging from minutes to months. We find that markets can be characterized by either a single-well or a double-well potential, depending on the time period and sampling frequency, where a double-well potential may signal market uncertainty or stress.",
    "paper_abstract_zh": "本文延续“线性扩散模型难以刻画复杂市场动力学”的长期猜想，为作者此前提出的观点提供实验验证：真实市场动力学应由漂移项中的高阶（三次及以上）非线性主导。由于扩散漂移是势函数负梯度，非线性漂移对应非二次势。该观点既基于一般理论，也基于纳入资金流及其对价格影响的结构化建模。本文通过分析分钟到月等多尺度高频加密货币数据，直接证实了这一看法：市场可用单势阱或双势阱势函数刻画，具体取决于时段与采样频率；双势阱可能表征市场不确定性或压力。",
    "primary_category": "q-fin.ST",
    "update_time": "2025-09-03",
    "paper_authors": "Igor Halperin",
    "topic": [
      "Image Generation"
    ],
    "category": [
      "Image"
    ]
  },
  {
    "paper_title": "The Role of Far-side Magnetic Structures in Modeling 2024 Solar Eclipse",
    "paper_title_zh": "远侧磁场结构在 2024 年日食建模中的作用",
    "paper_id": "2509.02911v1",
    "paper_abstract": "The corona is a crucial region that connects the solar surface to the solar wind and serves as the primary site of solar activity. The 2024 total solar eclipse (TSE) provides a unique opportunity to investigate the large-scale coronal structure. Combined with TSE observations, we study the impact of the magnetic structure of the far-side active region, located in the eastern hemisphere of the Sun that has not yet rotated into the Earth Field-of-View (FoV), on a global Magnetohydrodynamic (MHD) simulation. To address the limitation of single-view measurements in the routine synoptic map, we correct the magnetic field in the far-side region by incorporating full-disk magnetograms measured several days after the TSE, allowing us to capture the temporal evolution of the photospheric magnetic field in near real-time. Simulation results demonstrate that the local magnetic field in the far-side active region can significantly influence the global coronal structure by altering the position of the heliospheric current sheet (HCS), and further affect the global distribution of plasma parameters, even in polar regions. A comparison of the simulation results with white-light (WL) TSE + LASCO C2 observations and in situ measurements by the Parker Solar Probe (PSP) reveals that the composite synoptic map improves the accuracy of coronal modeling. This work provides robust support for advancing our understanding of coronal evolution, as well as deepens the link between the photosphere and large-scale coronal structure. Furthermore, it establishes a theoretical foundation for the future development of multi-view, stereoscopic measurements of the photospheric magnetic field.",
    "paper_abstract_zh": "日冕是连接太阳表面与太阳风的关键区域，也是太阳活动的主要发生地。2024 年全食为研究大尺度日冕结构提供了独特机会。结合全食观测，我们探讨位于太阳东半球、尚未进入地球视场的远侧活动区磁场结构对全球磁流体动力学（MHD）模拟的影响。为克服日常综合日面图单视角测量的局限，我们在日食数日后引入全日面磁图，实时修正远侧磁场，捕捉光球磁场的演化。模拟表明，远侧活动区的局地磁场可通过改变日球电流片（HCS）位置显著影响全球日冕结构，并进一步改变包括极区在内的全球等离子体参数分布。与全食白光（WL）+LASCO C2 观测以及帕克太阳探针（PSP）原位测量对比显示，综合综合图提高了日冕建模精度。本工作为理解日冕演化提供了有力支撑，深化了光球与大尺度日冕结构的联系，并为未来多视角、立体测量光球磁场奠定了理论基础。",
    "primary_category": "astro-ph.SR",
    "update_time": "2025-09-03",
    "paper_authors": "Guanglu Shi, Jiahui Shan, Li Feng, Jun Chen, Weiqun Gan",
    "topic": [
      "Image Generation"
    ],
    "category": [
      "Image"
    ]
  },
  {
    "paper_title": "Sem-RaDiff: Diffusion-Based 3D Radar Semantic Perception in Cluttered Agricultural Environments",
    "paper_title_zh": "Sem-RaDiff：基于扩散的杂乱农业环境 3D 雷达语义感知",
    "paper_id": "2509.02283v2",
    "paper_abstract": "Accurate and robust environmental perception is crucial for robot autonomous navigation. While current methods typically adopt optical sensors (e.g., camera, LiDAR) as primary sensing modalities, their susceptibility to visual occlusion often leads to degraded performance or complete system failure. In this paper, we focus on agricultural scenarios where robots are exposed to the risk of onboard sensor contamination. Leveraging radar's strong penetration capability, we introduce a radar-based 3D environmental perception framework as a viable alternative. It comprises three core modules designed for dense and accurate semantic perception: 1) Parallel frame accumulation to enhance signal-to-noise ratio of radar raw data. 2) A diffusion model-based hierarchical learning framework that first filters radar sidelobe artifacts then generates fine-grained 3D semantic point clouds. 3) A specifically designed sparse 3D network optimized for processing large-scale radar raw data. We conducted extensive benchmark comparisons and experimental evaluations on a self-built dataset collected in real-world agricultural field scenes. Results demonstrate that our method achieves superior structural and semantic prediction performance compared to existing methods, while simultaneously reducing computational and memory costs by 51.3% and 27.5%, respectively. Furthermore, our approach achieves complete reconstruction and accurate classification of thin structures such as poles and wires-which existing methods struggle to perceive-highlighting its potential for dense and accurate 3D radar perception.",
    "paper_abstract_zh": "准确稳健的环境感知对机器人自主导航至关重要。现有方法主要依赖光学传感器（相机、激光雷达），但易受视觉遮挡影响，导致性能下降甚至系统失效。本文针对农业场景下机载传感器易被污染的痛点，利用雷达强穿透能力，提出一种基于雷达的 3D 环境感知框架。该框架包含三大核心模块，实现稠密、精确的语义感知：1) 并行帧累积，提升雷达原始数据信噪比；2) 基于扩散模型的分层学习框架，先滤除雷达旁瓣伪影，再生成精细 3D 语义点云；3) 专为大规模雷达原始数据设计的稀疏 3D 网络。我们在自建真实农田数据集上进行大量基准对比与实验评估，结果表明，相比现有方法，本方法在结构与语义预测性能上均更优，同时计算与内存开销分别降低 51.3% 与 27.5%。此外，本方法能完整重建并准确分类杆、线等细小结构，而现有方法难以感知，展现了其在稠密、精确 3D 雷达感知方面的潜力。",
    "primary_category": "cs.RO",
    "update_time": "2025-09-03",
    "paper_authors": "Ruibin Zhang, Fei Gao",
    "topic": [
      "Image Generation"
    ],
    "category": [
      "Image"
    ]
  },
  {
    "paper_title": "Discrete Noise Inversion for Next-scale Autoregressive Text-based Image Editing",
    "paper_title_zh": "用于下一代自回归文本驱动图像编辑的离散噪声反演",
    "paper_id": "2509.01984v2",
    "paper_abstract": "Visual autoregressive models (VAR) have recently emerged as a promising class of generative models, achieving performance comparable to diffusion models in text-to-image generation tasks. While conditional generation has been widely explored, the ability to perform prompt-guided image editing without additional training is equally critical, as it supports numerous practical real-world applications. This paper investigates the text-to-image editing capabilities of VAR by introducing Visual AutoRegressive Inverse Noise (VARIN), the first noise inversion-based editing technique designed explicitly for VAR models. VARIN leverages a novel pseudo-inverse function for argmax sampling, named Location-aware Argmax Inversion (LAI), to generate inverse Gumbel noises. These inverse noises enable precise reconstruction of the source image and facilitate targeted, controllable edits aligned with textual prompts. Extensive experiments demonstrate that VARIN effectively modifies source images according to specified prompts while significantly preserving the original background and structural details, thus validating its efficacy as a practical editing approach.",
    "paper_abstract_zh": "视觉自回归模型（VAR）近期成为一类颇具前景的生成模型，在文本到图像生成任务中已可媲美扩散模型。尽管条件生成已被广泛研究，无需额外训练即可实现提示引导图像编辑的能力同样关键，可支撑众多实际应用。本文通过提出视觉自回归逆噪声（VARIN）——首个专为 VAR 设计的基于噪声反演的编辑技术——探索 VAR 的文本到图像编辑能力。VARIN 利用一种新颖的针对 argmax 采样的伪逆函数，即位置感知 argmax 反演（LAI），生成逆 Gumbel 噪声，实现源图像的精确重建，并支持面向文本提示的定向、可控编辑。大量实验表明，VARIN 能按指定提示有效修改源图像，同时显著保留原始背景与结构细节，验证了其作为实用编辑方法的有效性。",
    "primary_category": "cs.CV",
    "update_time": "2025-09-03",
    "paper_authors": "Quan Dao, Xiaoxiao He, Ligong Han, Ngan Hoai Nguyen, Amin Heyrani Nobar, Faez Ahmed, Han Zhang, Viet Anh Nguyen, Dimitris Metaxas",
    "topic": [
      "Image Generation"
    ],
    "category": [
      "Image"
    ]
  },
  {
    "paper_title": "CompSlider: Compositional Slider for Disentangled Multiple-Attribute Image Generation",
    "paper_title_zh": "CompSlider：用于多属性解耦图像生成的组合式滑块",
    "paper_id": "2509.01028v2",
    "paper_abstract": "In text-to-image (T2I) generation, achieving fine-grained control over attributes - such as age or smile - remains challenging, even with detailed text prompts. Slider-based methods offer a solution for precise control of image attributes. Existing approaches typically train individual adapter for each attribute separately, overlooking the entanglement among multiple attributes. As a result, interference occurs among different attributes, preventing precise control of multiple attributes together. To address this challenge, we aim to disentangle multiple attributes in slider-based generation to enbale more reliable and independent attribute manipulation. Our approach, CompSlider, can generate a conditional prior for the T2I foundation model to control multiple attributes simultaneously. Furthermore, we introduce novel disentanglement and structure losses to compose multiple attribute changes while maintaining structural consistency within the image. Since CompSlider operates in the latent space of the conditional prior and does not require retraining the foundation model, it reduces the computational burden for both training and inference. We evaluate our approach on a variety of image attributes and highlight its generality by extending to video generation.",
    "paper_abstract_zh": "在文生图（T2I）生成中，即使使用详细的文本提示，对年龄、微笑等属性进行细粒度控制仍然困难。基于滑块的方法为精确控制图像属性提供了解决方案。现有方法通常为每个属性单独训练适配器，忽视了多属性之间的耦合，导致不同属性间相互干扰，无法同时精确控制多个属性。为此，我们提出 CompSlider，旨在在基于滑块的生成中解耦多个属性，实现更可靠且独立的属性操控。CompSlider 可为 T2I 基础模型生成条件先验，以同时控制多个属性。此外，我们引入新颖的解耦损失与结构损失，在保持图像结构一致性的前提下组合多种属性变化。由于 CompSlider 仅在条件先验的潜空间操作，无需重新训练基础模型，显著降低了训练与推理的计算负担。我们在多种图像属性上评估了该方法，并通过扩展到视频生成展示了其通用性。",
    "primary_category": "cs.CV",
    "update_time": "2025-09-03",
    "paper_authors": "Zixin Zhu, Kevin Duarte, Mamshad Nayeem Rizve, Chengyuan Xu, Ratheesh Kalarot, Junsong Yuan",
    "topic": [
      "Image Generation",
      "Video Generation"
    ],
    "category": [
      "Image"
    ]
  },
  {
    "paper_title": "HyPV-LEAD: Proactive Early-Warning of Cryptocurrency Anomalies through Data-Driven Structural-Temporal Modeling",
    "paper_title_zh": "HyPV-LEAD：通过数据驱动的结构-时间建模主动预警加密货币异常",
    "paper_id": "2509.03260v1",
    "paper_abstract": "Abnormal cryptocurrency transactions - such as mixing services, fraudulent transfers, and pump-and-dump operations -- pose escalating risks to financial integrity but remain notoriously difficult to detect due to class imbalance, temporal volatility, and complex network dependencies. Existing approaches are predominantly model-centric and post hoc, flagging anomalies only after they occur and thus offering limited preventive value. This paper introduces HyPV-LEAD (Hyperbolic Peak-Valley Lead-time Enabled Anomaly Detection), a data-driven early-warning framework that explicitly incorporates lead time into anomaly detection. Unlike prior methods, HyPV-LEAD integrates three innovations: (1) window-horizon modeling to guarantee actionable lead-time alerts, (2) Peak-Valley (PV) sampling to mitigate class imbalance while preserving temporal continuity, and (3) hyperbolic embedding to capture the hierarchical and scale-free properties of blockchain transaction networks. Empirical evaluation on large-scale Bitcoin transaction data demonstrates that HyPV-LEAD consistently outperforms state-of-the-art baselines, achieving a PR-AUC of 0.9624 with significant gains in precision and recall. Ablation studies further confirm that each component - PV sampling, hyperbolic embedding, and structural-temporal modeling - provides complementary benefits, with the full framework delivering the highest performance. By shifting anomaly detection from reactive classification to proactive early-warning, HyPV-LEAD establishes a robust foundation for real-time risk management, anti-money laundering (AML) compliance, and financial security in dynamic blockchain environments.",
    "paper_abstract_zh": "混币服务、欺诈转账、拉盘砸盘等异常加密货币交易对金融完整性构成日益严重的威胁，但由于类别不平衡、时间波动性和复杂的网络依赖关系，其检测极为困难。现有方法多为以模型为中心的事后检测，只能在异常发生后标记，预防价值有限。本文提出 HyPV-LEAD（双曲峰谷提前期异常检测），一种数据驱动的早期预警框架，将提前期显式纳入异常检测。与既往方法不同，HyPV-LEAD 融合三项创新：(1) 窗口-视界建模，确保可操作的提前期警报；(2) 峰谷（PV）采样，缓解类别失衡同时保持时间连续性；(3) 双曲嵌入，捕捉区块链交易网络的层级和无标度特性。在大型比特币交易数据上的实证评估表明，HyPV-LEAD 持续优于最新基线，PR-AUC 达 0.9624，精确率与召回率均显著提升。消融实验进一步证实，PV 采样、双曲嵌入和结构-时间建模各组件互补，完整框架性能最高。通过将异常检测从被动分类转变为主动预警，HyPV-LEAD 为动态区块链环境中的实时风险管理、反洗钱合规与金融安全奠定了坚实基础。",
    "primary_category": "cs.LG",
    "update_time": "2025-09-03",
    "paper_authors": "Minjung Park, Gyuyeon Na, Soyoun Kim, Sunyoung Moon, HyeonJeong Cha, Sangmi Chai",
    "topic": [
      "Video Generation"
    ],
    "category": [
      "Image"
    ]
  },
  {
    "paper_title": "See No Evil: Adversarial Attacks Against Linguistic-Visual Association in Referring Multi-Object Tracking Systems",
    "paper_title_zh": "视而不见：面向指称多目标跟踪系统的语言-视觉关联对抗攻击",
    "paper_id": "2509.02028v2",
    "paper_abstract": "Language-vision understanding has driven the development of advanced perception systems, most notably the emerging paradigm of Referring Multi-Object Tracking (RMOT). By leveraging natural-language queries, RMOT systems can selectively track objects that satisfy a given semantic description, guided through Transformer-based spatial-temporal reasoning modules. End-to-End (E2E) RMOT models further unify feature extraction, temporal memory, and spatial reasoning within a Transformer backbone, enabling long-range spatial-temporal modeling over fused textual-visual representations. Despite these advances, the reliability and robustness of RMOT remain underexplored. In this paper, we examine the security implications of RMOT systems from a design-logic perspective, identifying adversarial vulnerabilities that compromise both the linguistic-visual referring and track-object matching components. Additionally, we uncover a novel vulnerability in advanced RMOT models employing FIFO-based memory, whereby targeted and consistent attacks on their spatial-temporal reasoning introduce errors that persist within the history buffer over multiple subsequent frames. We present VEIL, a novel adversarial framework designed to disrupt the unified referring-matching mechanisms of RMOT models. We show that carefully crafted digital and physical perturbations can corrupt the tracking logic reliability, inducing track ID switches and terminations. We conduct comprehensive evaluations using the Refer-KITTI dataset to validate the effectiveness of VEIL and demonstrate the urgent need for security-aware RMOT designs for critical large-scale applications.",
    "paper_abstract_zh": "语言-视觉理解推动了先进感知系统的发展，其中最具代表性的新兴范式是指称多目标跟踪（RMOT）。RMOT 系统利用自然语言查询，通过基于 Transformer 的时空推理模块，选择性地跟踪满足语义描述的目标。端到端（E2E）RMOT 模型进一步在 Transformer 主干内统一特征提取、时序记忆与空间推理，实现对融合文本-视觉表征的长程时空建模。尽管取得这些进展，RMOT 的可靠性与鲁棒性仍缺乏深入探索。本文从设计逻辑视角审视 RMOT 系统的安全含义，发现危及语言-视觉指称与轨迹-目标匹配组件的对抗漏洞。此外，我们揭示了采用 FIFO 记忆的高级 RMOT 模型中存在的新漏洞：针对其时空推理的有针对性且持续的攻击会在历史缓冲区中引入错误，并在后续多帧中持续累积。我们提出 VEIL，一种新型对抗框架，旨在破坏 RMOT 模型统一的指称-匹配机制。实验表明，精心设计的数字与物理扰动可破坏跟踪逻辑可靠性，诱发轨迹 ID 切换与终止。我们在 Refer-KITTI 数据集上开展全面评估，验证 VEIL 的有效性，并凸显面向大规模关键应用的 RMOT 安全感知设计的迫切需求。",
    "primary_category": "cs.CV",
    "update_time": "2025-09-03",
    "paper_authors": "Halima Bouzidi, Haoyu Liu, Mohammad Abdullah Al Faruque",
    "topic": [
      "Video Generation"
    ],
    "category": [
      "Image"
    ]
  },
  {
    "paper_title": "ContraGS: Codebook-Condensed and Trainable Gaussian Splatting for Fast, Memory-Efficient Reconstruction",
    "paper_title_zh": "ContraGS：基于码本压缩的可训练高斯溅射，实现快速、内存高效的重建",
    "paper_id": "2509.03775v1",
    "paper_abstract": "3D Gaussian Splatting (3DGS) is a state-of-art technique to model real-world scenes with high quality and real-time rendering. Typically, a higher quality representation can be achieved by using a large number of 3D Gaussians. However, using large 3D Gaussian counts significantly increases the GPU device memory for storing model parameters. A large model thus requires powerful GPUs with high memory capacities for training and has slower training/rendering latencies due to the inefficiencies of memory access and data movement. In this work, we introduce ContraGS, a method to enable training directly on compressed 3DGS representations without reducing the Gaussian Counts, and thus with a little loss in model quality. ContraGS leverages codebooks to compactly store a set of Gaussian parameter vectors throughout the training process, thereby significantly reducing memory consumption. While codebooks have been demonstrated to be highly effective at compressing fully trained 3DGS models, directly training using codebook representations is an unsolved challenge. ContraGS solves the problem of learning non-differentiable parameters in codebook-compressed representations by posing parameter estimation as a Bayesian inference problem. To this end, ContraGS provides a framework that effectively uses MCMC sampling to sample over a posterior distribution of these compressed representations. With ContraGS, we demonstrate that ContraGS significantly reduces the peak memory during training (on average 3.49X) and accelerated training and rendering (1.36X and 1.88X on average, respectively), while retraining close to state-of-art quality.",
    "paper_abstract_zh": "3D 高斯溅射（3DGS）是建模真实场景的最新技术，兼具高质量与实时渲染能力。通常，使用更多 3D 高斯可获得更高质量的表征，但会显著增加存储模型参数的 GPU 显存。大模型需要高显存强力 GPU 进行训练，且因内存访问与数据搬运低效导致训练/渲染延迟更高。本文提出 ContraGS，可直接在压缩后的 3DGS 表征上训练，而无需减少高斯数量，从而几乎不损失模型质量。ContraGS 利用码本在整个训练过程中紧凑存储一组高斯参数向量，显著降低内存消耗。尽管码本已被证明对训练后的 3DGS 模型压缩极为有效，但直接使用码本表征训练仍是未解难题。ContraGS 将码本压缩表征中的不可微参数学习问题转化为贝叶斯推断问题，为此提供一套框架，利用 MCMC 采样对压缩表征的后验分布进行有效采样。实验表明，ContraGS 在训练阶段平均减少 3.49 倍峰值内存，并分别将训练与渲染速度平均提升 1.36 倍与 1.88 倍，同时保持接近最先进的质量。",
    "primary_category": "cs.GR",
    "update_time": "2025-09-03",
    "paper_authors": "Sankeerth Durvasula, Sharanshangar Muhunthan, Zain Moustafa, Richard Chen, Ruofan Liang, Yushi Guan, Nilesh Ahuja, Nilesh Jain, Selvakumar Panneer, Nandita Vijaykumar",
    "topic": [
      "3D Generation"
    ],
    "category": [
      "Image"
    ]
  },
  {
    "paper_title": "GS-TG: 3D Gaussian Splatting Accelerator with Tile Grouping for Reducing Redundant Sorting while Preserving Rasterization Efficiency",
    "paper_title_zh": "GS-TG：基于瓦片分组的 3D 高斯溅射加速器，在保持光栅化效率的同时减少冗余排序",
    "paper_id": "2509.00911v2",
    "paper_abstract": "3D Gaussian Splatting (3D-GS) has emerged as a promising alternative to neural radiance fields (NeRF) as it offers high speed as well as high image quality in novel view synthesis. Despite these advancements, 3D-GS still struggles to meet the frames per second (FPS) demands of real-time applications. In this paper, we introduce GS-TG, a tile-grouping-based accelerator that enhances 3D-GS rendering speed by reducing redundant sorting operations and preserving rasterization efficiency. GS-TG addresses a critical trade-off issue in 3D-GS rendering: increasing the tile size effectively reduces redundant sorting operations, but it concurrently increases unnecessary rasterization computations. So, during sorting of the proposed approach, GS-TG groups small tiles (for making large tiles) to share sorting operations across tiles within each group, significantly reducing redundant computations. During rasterization, a bitmask assigned to each Gaussian identifies relevant small tiles, to enable efficient sharing of sorting results. Consequently, GS-TG enables sorting to be performed as if a large tile size is used by grouping tiles during the sorting stage, while allowing rasterization to proceed with the original small tiles by using bitmasks in the rasterization stage. GS-TG is a lossless method requiring no retraining or fine-tuning and it can be seamlessly integrated with previous 3D-GS optimization techniques. Experimental results show that GS-TG achieves an average speed-up of 1.54 times over state-of-the-art 3D-GS accelerators.",
    "paper_abstract_zh": "3D 高斯溅射（3D-GS）已成为神经辐射场（NeRF）的有力替代方案，在新视角合成中兼具高速度与高质量。尽管取得进展，3D-GS 仍难以满足实时应用对帧率（FPS）的需求。本文提出 GS-TG，一种基于瓦片分组的加速器，通过减少冗余排序操作并保持光栅化效率来提升 3D-GS 渲染速度。GS-TG 解决了 3D-GS 渲染中的关键权衡：增大瓦片尺寸可有效减少冗余排序，但同时会增加不必要的片元计算。为此，GS-TG 在排序阶段将多个小瓦片组合成大瓦片，使组内瓦片共享排序操作，显著减少冗余计算；在光栅化阶段，为每个高斯分配位掩码以标识相关小瓦片，从而高效复用排序结果。因此，GS-TG 在排序阶段通过瓦片分组实现“大瓦片”效果，而在光栅化阶段借助位掩码仍以原始小瓦片运行，兼顾两者优势。GS-TG 无损、无需重训练或微调，可无缝集成现有 3D-GS 优化技术。实验结果表明，GS-TG 相比最新 3D-GS 加速器平均实现 1.54 倍加速。",
    "primary_category": "cs.AR",
    "update_time": "2025-09-03",
    "paper_authors": "Joongho Jo, Jongsun Park",
    "topic": [
      "3D Generation"
    ],
    "category": [
      "Image"
    ]
  },
  {
    "paper_title": "Singular Value Few-shot Adaptation of Vision-Language Models",
    "paper_title_zh": "视觉-语言模型的奇异值少样本自适应方法",
    "paper_id": "2509.03740v1",
    "paper_abstract": "Vision-language models (VLMs) like CLIP have shown impressive zero-shot and few-shot learning capabilities across diverse applications. However, adapting these models to new fine-grained domains remains difficult due to reliance on prompt engineering and the high cost of full model fine-tuning. Existing adaptation approaches rely on augmented components, such as prompt tokens and adapter modules, which could limit adaptation quality, destabilize the model, and compromise the rich knowledge learned during pretraining. In this work, we present \\textbf{CLIP-SVD}, a novel \\textit{multi-modal} and \\textit{parameter-efficient} adaptation technique that leverages Singular Value Decomposition (SVD) to modify the internal parameter space of CLIP without injecting additional modules. Specifically, we fine-tune only the singular values of the CLIP parameter matrices to rescale the basis vectors for domain adaptation while retaining the pretrained model. This design enables enhanced adaptation performance using only \\textbf{0.04\\%} of the model's total parameters and better preservation of its generalization ability. CLIP-SVD achieves state-of-the-art classification results on 11 natural and 10 biomedical datasets, outperforming previous methods in both accuracy and generalization under few-shot settings. Additionally, we leverage a natural language-based approach to analyze the effectiveness and dynamics of the CLIP adaptation to allow interpretability of CLIP-SVD. The code is publicly available at https://github.com/HealthX-Lab/CLIP-SVD.",
    "paper_abstract_zh": "像 CLIP 这样的视觉-语言模型（VLM）在众多任务中展现出卓越的零样本与少样本学习能力。然而，由于依赖提示工程且全参数微调代价高昂，将这些模型适配到新的细粒度领域仍然困难。现有方法通过引入提示词或适配模块等附加组件进行适配，可能限制适配质量、破坏模型稳定性并削弱预训练阶段获得的丰富知识。本文提出 CLIP-SVD，一种新颖的多模态、参数高效自适应技术，利用奇异值分解（SVD）直接修改 CLIP 的内部参数空间，无需注入任何额外模块。具体而言，我们仅对 CLIP 参数矩阵的奇异值进行微调，在保留预训练权重的同时重新缩放基向量以实现领域自适应。该方法仅用模型总参数的 0.04% 即可显著提升适配性能，并更好地保持泛化能力。CLIP-SVD 在 11 个自然场景和 10 个生物医学数据集上取得最先进的分类结果，在少样本设定下的准确率与泛化性均优于已有方法。此外，我们采用基于自然语言的分析手段，对 CLIP 自适应过程的有效性与动态变化进行解释。代码已开源：https://github.com/HealthX-Lab/CLIP-SVD。",
    "primary_category": "cs.CV",
    "update_time": "2025-09-03",
    "paper_authors": "Taha Koleilat, Hassan Rivaz, Yiming Xiao",
    "topic": [
      "Multimodal Generation"
    ],
    "category": [
      "Image"
    ]
  },
  {
    "paper_title": "The Optimiser Hidden in Plain Sight: Training with the Loss Landscape's Induced Metric",
    "paper_title_zh": "隐藏在眼前的优化器：利用损失景观诱导度量进行训练",
    "paper_id": "2509.03594v1",
    "paper_abstract": "We present a class of novel optimisers for training neural networks that makes use of the Riemannian metric naturally induced when the loss landscape is embedded in higher-dimensional space. This is the same metric that underlies common visualisations of loss landscapes. By taking this geometric perspective literally and using the induced metric, we develop a new optimiser and compare it to existing methods, namely: SGD, Adam, AdamW, and Muon, across a range of tasks and architectures. Empirically, we conclude that this new class of optimisers is highly effective in low dimensional examples, and provides slight improvement over state-of-the-art methods for training neural networks. These new optimisers have theoretically desirable properties. In particular, the effective learning rate is automatically decreased in regions of high curvature acting as a smoothed out form of gradient clipping. Similarly, one variant of these optimisers can also be viewed as inducing an effective scheduled learning rate and decoupled weight decay is the natural choice from our geometric perspective. The basic method can be used to modify any existing preconditioning method. The new optimiser has a computational complexity comparable to that of Adam.",
    "paper_abstract_zh": "我们提出了一类用于神经网络训练的新型优化器，其利用损失景观嵌入高维空间时自然诱导的黎曼度量——该度量正是常见损失景观可视化背后的几何基础。通过严格采纳这一几何视角并使用诱导度量，我们开发出一种新优化器，并在多种任务与架构下与 SGD、Adam、AdamW 及 Muon 等现有方法进行对比。实验表明，该优化器在低维示例中表现极为出色，在高维神经网络训练上也能对当前最优方法带来小幅提升。新优化器具备理论上的优良性质：在高曲率区域，有效学习率会自动降低，起到平滑版梯度裁剪的作用；其某一变体还可视为引入了一种有效的学习率调度，而从几何视角出发，解耦权重衰减成为自然选择。基本方法可用于改造任何现有预条件技术，且计算复杂度与 Adam 相当。",
    "primary_category": "cs.LG",
    "update_time": "2025-09-03",
    "paper_authors": "Thomas R. Harvey",
    "topic": [
      "Multimodal Generation"
    ],
    "category": [
      "Image"
    ]
  },
  {
    "paper_title": "Can the Waymo Open Motion Dataset Support Realistic Behavioral Modeling? A Validation Study with Naturalistic Trajectories",
    "paper_title_zh": "Waymo 开放运动数据集能否支持真实行为建模？基于自然驾驶轨迹的验证研究",
    "paper_id": "2509.03515v1",
    "paper_abstract": "The Waymo Open Motion Dataset (WOMD) has become a popular resource for data-driven modeling of autonomous vehicles (AVs) behavior. However, its validity for behavioral analysis remains uncertain due to proprietary post-processing, the absence of error quantification, and the segmentation of trajectories into 20-second clips. This study examines whether WOMD accurately captures the dynamics and interactions observed in real-world AV operations. Leveraging an independently collected naturalistic dataset from Level 4 AV operations in Phoenix, Arizona (PHX), we perform comparative analyses across three representative urban driving scenarios: discharging at signalized intersections, car-following, and lane-changing behaviors. For the discharging analysis, headways are manually extracted from aerial video to ensure negligible measurement error. For the car-following and lane-changing cases, we apply the Simulation-Extrapolation (SIMEX) method to account for empirically estimated error in the PHX data and use Dynamic Time Warping (DTW) distances to quantify behavioral differences. Results across all scenarios consistently show that behavior in PHX falls outside the behavioral envelope of WOMD. Notably, WOMD underrepresents short headways and abrupt decelerations. These findings suggest that behavioral models calibrated solely on WOMD may systematically underestimate the variability, risk, and complexity of naturalistic driving. Caution is therefore warranted when using WOMD for behavior modeling without proper validation against independently collected data.",
    "paper_abstract_zh": "Waymo 开放运动数据集（WOMD）已成为自动驾驶车辆（AV）数据驱动行为建模的热门资源。然而，由于专有后处理、缺乏误差量化以及将轨迹切分为 20 秒片段，其在行为分析中的有效性仍不确定。本研究检验 WOMD 能否准确反映真实 AV 运行中的动态与交互。利用在亚利桑那州凤凰城独立采集的 L4 级 AV 自然驾驶数据集（PHX），我们针对三种典型城市场景开展对比：信号交叉口放行、跟驰与换道。放行分析中，通过航拍视频手工提取车头时距以确保测量误差可忽略；跟驰与换道场景则采用 Simulation-Extrapolation（SIMEX）方法校正 PHX 数据中的经验估计误差，并用动态时间规整（DTW）距离量化行为差异。所有场景结果一致表明，PHX 中的行为落在 WOMD 行为包络之外，尤其 WOMD 明显低估短车头时距与急减速。这些发现提示，仅基于 WOMD 校准的行为模型可能系统性低估自然驾驶的变异性、风险与复杂性。因此，在未与独立采集数据充分验证的情况下，使用 WOMD 进行行为建模需谨慎。",
    "primary_category": "cs.RO",
    "update_time": "2025-09-03",
    "paper_authors": "Yanlin Zhang, Sungyong Chung, Nachuan Li, Dana Monzer, Hani S. Mahmassani, Samer H. Hamdar, Alireza Talebpour",
    "topic": [
      "Multimodal Generation"
    ],
    "category": [
      "Image"
    ]
  },
  {
    "paper_title": "OneCAT: Decoder-Only Auto-Regressive Model for Unified Understanding and Generation",
    "paper_title_zh": "OneCAT：用于统一理解与生成的纯解码器自回归模型",
    "paper_id": "2509.03498v1",
    "paper_abstract": "We introduce OneCAT, a unified multimodal model that seamlessly integrates understanding, generation, and editing within a novel, pure decoder-only transformer architecture. Our framework uniquely eliminates the need for external components such as Vision Transformers (ViT) or vision tokenizer during inference, leading to significant efficiency gains, especially for high-resolution inputs. This is achieved through a modality-specific Mixture-of-Experts (MoE) structure trained with a single autoregressive (AR) objective, which also natively supports dynamic resolutions. Furthermore, we pioneer a multi-scale visual autoregressive mechanism within the Large Language Model (LLM) that drastically reduces decoding steps compared to diffusion-based methods while maintaining state-of-the-art performance. Our findings demonstrate the powerful potential of pure autoregressive modeling as a sufficient and elegant foundation for unified multimodal intelligence. As a result, OneCAT sets a new performance standard, outperforming existing open-source unified multimodal models across benchmarks for multimodal generation, editing, and understanding.",
    "paper_abstract_zh": "我们提出 OneCAT，一种统一的多模态模型，在全新的纯解码器自回归 Transformer 架构中无缝集成理解、生成与编辑功能。该框架独特地省去了推理阶段对外部视觉 Transformer（ViT）或视觉分词器的需求，从而在高分辨率输入下获得显著效率提升。这一成果通过模态特定的混合专家（MoE）结构实现，并采用单一自回归（AR）目标训练，原生支持动态分辨率。此外，我们在大语言模型（LLM）内部首创多尺度视觉自回归机制，与基于扩散的方法相比大幅缩减解码步数，同时保持最先进的性能。实验结果彰显了纯自回归建模作为统一多模态智能之充分且优雅基础的强大潜力。由此，OneCAT 在多项多模态生成、编辑与理解基准上全面超越现有开源统一多模态模型，树立了新的性能标杆。",
    "primary_category": "cs.CV",
    "update_time": "2025-09-03",
    "paper_authors": "Han Li, Xinyu Peng, Yaoming Wang, Zelin Peng, Xin Chen, Rongxiang Weng, Jingang Wang, Xunliang Cai, Wenrui Dai, Hongkai Xiong",
    "topic": [
      "Multimodal Generation"
    ],
    "category": [
      "Image"
    ]
  },
  {
    "paper_title": "PointAD+: Learning Hierarchical Representations for Zero-shot 3D Anomaly Detection",
    "paper_title_zh": "PointAD+：用于零样本 3D 异常检测的层次化表征学习",
    "paper_id": "2509.03277v1",
    "paper_abstract": "In this paper, we aim to transfer CLIP's robust 2D generalization capabilities to identify 3D anomalies across unseen objects of highly diverse class semantics. To this end, we propose a unified framework to comprehensively detect and segment 3D anomalies by leveraging both point- and pixel-level information. We first design PointAD, which leverages point-pixel correspondence to represent 3D anomalies through their associated rendering pixel representations. This approach is referred to as implicit 3D representation, as it focuses solely on rendering pixel anomalies but neglects the inherent spatial relationships within point clouds. Then, we propose PointAD+ to further broaden the interpretation of 3D anomalies by introducing explicit 3D representation, emphasizing spatial abnormality to uncover abnormal spatial relationships. Hence, we propose G-aggregation to involve geometry information to enable the aggregated point representations spatially aware. To simultaneously capture rendering and spatial abnormality, PointAD+ proposes hierarchical representation learning, incorporating implicit and explicit anomaly semantics into hierarchical text prompts: rendering prompts for the rendering layer and geometry prompts for the geometry layer. A cross-hierarchy contrastive alignment is further introduced to promote the interaction between the rendering and geometry layers, facilitating mutual anomaly learning. Finally, PointAD+ integrates anomaly semantics from both layers to capture the generalized anomaly semantics. During the test, PointAD+ can integrate RGB information in a plug-and-play manner and further improve its detection performance. Extensive experiments demonstrate the superiority of PointAD+ in ZS 3D anomaly detection across unseen objects with highly diverse class semantics, achieving a holistic understanding of abnormality.",
    "paper_abstract_zh": "本文旨在将 CLIP 强大的 2D 泛化能力迁移到高度多样化语义类别的未见物体，实现 3D 异常检测。为此，我们提出一个统一框架，综合利用点级与像素级信息，全面检测并分割 3D 异常。首先设计 PointAD，通过点-像素对应关系，将 3D 异常用其渲染像素表征表示，称为隐式 3D 表征；然而该方法仅关注渲染像素异常，忽略了点云内在的空间关系。继而提出 PointAD+，引入显式 3D 表征以强调空间异常，揭示异常空间关系。为此，我们提出 G-aggregation，融入几何信息使聚合后的点表征具备空间感知能力。为同时捕捉渲染与空间异常，PointAD+ 提出层次化表征学习，将隐式与显式异常语义分别注入层次化文本提示：渲染层提示与几何层提示。进一步引入跨层次对比对齐，促进渲染层与几何层之间的交互，实现异常信息的相互学习。最终，PointAD+ 融合两层异常语义，捕获通用异常表征。测试阶段，PointAD+ 可即插即用式地融合 RGB 信息，进一步提升检测性能。大量实验表明，PointAD+ 在具有高度多样化语义类别的未见物体零样本 3D 异常检测任务上表现卓越，实现了对异常的整体理解。",
    "primary_category": "cs.CV",
    "update_time": "2025-09-03",
    "paper_authors": "Qihang Zhou, Shibo He, Jiangtao Yan, Wenchao Meng, Jiming Chen",
    "topic": [
      "Multimodal Generation"
    ],
    "category": [
      "Image"
    ]
  },
  {
    "paper_title": "Background Matters Too: A Language-Enhanced Adversarial Framework for Person Re-Identification",
    "paper_title_zh": "背景同样重要：面向行人重识别的语言增强对抗框架",
    "paper_id": "2509.03032v1",
    "paper_abstract": "Person re-identification faces two core challenges: precisely locating the foreground target while suppressing background noise and extracting fine-grained features from the target region. Numerous visual-only approaches address these issues by partitioning an image and applying attention modules, yet they rely on costly manual annotations and struggle with complex occlusions. Recent multimodal methods, motivated by CLIP, introduce semantic cues to guide visual understanding. However, they focus solely on foreground information, but overlook the potential value of background cues. Inspired by human perception, we argue that background semantics are as important as the foreground semantics in ReID, as humans tend to eliminate background distractions while focusing on target appearance. Therefore, this paper proposes an end-to-end framework that jointly models foreground and background information within a dual-branch cross-modal feature extraction pipeline. To help the network distinguish between the two domains, we propose an intra-semantic alignment and inter-semantic adversarial learning strategy. Specifically, we align visual and textual features that share the same semantics across domains, while simultaneously penalizing similarity between foreground and background features to enhance the network's discriminative power. This strategy drives the model to actively suppress noisy background regions and enhance attention toward identity-relevant foreground cues. Comprehensive experiments on two holistic and two occluded ReID benchmarks demonstrate the effectiveness and generality of the proposed method, with results that match or surpass those of current state-of-the-art approaches.",
    "paper_abstract_zh": "行人重识别面临两大核心挑战：精确定位前景目标并抑制背景噪声，以及从目标区域提取细粒度特征。大量纯视觉方法通过图像分区与注意力模块解决这些问题，但依赖昂贵的人工标注且难以应对复杂遮挡。近期受 CLIP 启发的多模态方法引入语义线索引导视觉理解，却仅关注前景信息，忽视背景线索的潜在价值。受人类感知启发，本文认为背景语义与前景语义在 ReID 中同等重要——人类在聚焦目标外观时会主动排除背景干扰。为此，我们提出一种端到端框架，在双分支跨模态特征提取管道中联合建模前景与背景信息。为帮助网络区分两者，我们提出“域内语义对齐+域间语义对抗”学习策略：一方面对齐跨域同义视觉-文本特征，另一方面惩罚前景-背景特征相似性，以增强网络判别力。该策略驱动模型主动抑制噪声背景区域，并强化对身份相关前景线索的关注。在两类整体与两类遮挡 ReID 基准上的全面实验验证了所提方法的有效性与泛化性，结果达到或超越当前最优水平。",
    "primary_category": "cs.CV",
    "update_time": "2025-09-03",
    "paper_authors": "Kaicong Huang, Talha Azfar, Jack M. Reilly, Thomas Guggisberg, Ruimin Ke",
    "topic": [
      "Multimodal Generation"
    ],
    "category": [
      "Image"
    ]
  },
  {
    "paper_title": "Hues and Cues: Human vs. CLIP",
    "paper_title_zh": "色调与提示：人类 vs CLIP",
    "paper_id": "2509.02305v2",
    "paper_abstract": "Playing games is inherently human, and a lot of games are created to challenge different human characteristics. However, these tasks are often left out when evaluating the human-like nature of artificial models. The objective of this work is proposing a new approach to evaluate artificial models via board games. To this effect, we test the color perception and color naming capabilities of CLIP by playing the board game Hues & Cues and assess its alignment with humans. Our experiments show that CLIP is generally well aligned with human observers, but our approach brings to light certain cultural biases and inconsistencies when dealing with different abstraction levels that are hard to identify with other testing strategies. Our findings indicate that assessing models with different tasks like board games can make certain deficiencies in the models stand out in ways that are difficult to test with the commonly used benchmarks.",
    "paper_abstract_zh": "游戏是人类的天性，许多游戏旨在挑战不同的人类特质。然而，在评估人工智能模型的“类人”程度时，这些任务常被忽略。本文提出一种通过桌游评估 AI 模型的新思路：让 CLIP 玩桌游《Hues & Cues》，测试其颜色感知与命名能力，并与人类对齐度进行比较。实验表明，CLIP 总体上与人类观察者高度一致，但我们的方法揭示了其在不同抽象层级下存在的文化偏见与不一致性，这些问题难以通过常规测试策略发现。研究结果表明，利用桌游等多样化任务评估模型，可使其缺陷以传统基准难以捕捉的方式暴露出来。",
    "primary_category": "cs.CV",
    "update_time": "2025-09-03",
    "paper_authors": "Nuria Alabau-Bosque, Jorge Vila-Tomás, Paula Daudén-Oliver, Pablo Hernández-Cámara, Jose Manuel Jaén-Lorites, Valero Laparra, Jesús Malo",
    "topic": [
      "Multimodal Generation"
    ],
    "category": [
      "Image"
    ]
  },
  {
    "paper_title": "Spotlighter: Revisiting Prompt Tuning from a Representative Mining View",
    "paper_title_zh": "Spotlighter：从代表性挖掘视角重审提示微调",
    "paper_id": "2509.00905v2",
    "paper_abstract": "CLIP's success has demonstrated that prompt tuning can achieve robust cross-modal semantic alignment for tasks ranging from open-domain recognition to fine-grained classification. However, redundant or weakly relevant feature components introduce noise and incur unnecessary computational costs. In this work, we propose Spotlighter, a lightweight token-selection framework that simultaneously enhances accuracy and efficiency in prompt tuning. Spotlighter evaluates each visual token's activation from both sample-wise and semantic-wise perspectives and retains only the top-scoring tokens for downstream prediction. A class-specific semantic memory bank of learned prototypes refines this selection, ensuring semantic representativeness and compensating for discarded features. To further prioritize informative signals, we introduce a two-level ranking mechanism that dynamically weights token--prototype interactions. Across 11 few-shot benchmarks, Spotlighter outperforms CLIP by up to 11.19\\% in harmonic mean accuracy and achieves up to 0.8K additional FPS, with only 21 extra parameters. These results establish Spotlighter as an effective and scalable baseline for prompt tuning. Code for our method will be available at https://github.com/greatest-gourmet/Spotlighter.",
    "paper_abstract_zh": "CLIP 的成功表明，提示微调可在从开放域识别到细粒度分类的多种任务中实现稳健的跨模态语义对齐。然而，冗余或弱相关特征分量会引入噪声并带来不必要的计算开销。本文提出 Spotlighter，一种轻量级 Token 选择框架，在提升提示微调准确率的同时显著提高效率。Spotlighter 从样本级与语义级双视角评估每个视觉 Token 的激活值，仅保留得分最高的 Token 用于下游预测。类别特定的语义记忆库通过已学原型精炼选择过程，确保语义代表性并补偿被丢弃的特征。为进一步突出有效信号，我们引入两级排序机制，动态加权 Token-原型交互。在 11 个小样本基准上，Spotlighter 相比 CLIP 的调和平均准确率最高提升 11.19%，额外带来 0.8K FPS 的推理加速，仅需 21 个新增参数。这些结果确立了 Spotlighter 作为提示微调高效且可扩展的新基线。代码将开源至 https://github.com/greatest-gourmet/Spotlighter。",
    "primary_category": "cs.CV",
    "update_time": "2025-09-03",
    "paper_authors": "Yutong Gao, Maoyuan Shao, Xinyang Huang, Chuang Zhu, Lijuan Sun, Yu Weng, Xuan Liu, Guoshun Nan",
    "topic": [
      "Multimodal Generation"
    ],
    "category": [
      "Image"
    ]
  },
  {
    "paper_title": "Comparison of End-to-end Speech Assessment Models for the NOCASA 2025 Challenge",
    "paper_title_zh": "NOCASA 2025 挑战赛端到端语音评估模型对比",
    "paper_id": "2509.03256v1",
    "paper_abstract": "This paper presents an analysis of three end-to-end models developed for the NOCASA 2025 Challenge, aimed at automatic word-level pronunciation assessment for children learning Norwegian as a second language. Our models include an encoder-decoder Siamese architecture (E2E-R), a prefix-tuned direct classification model leveraging pretrained wav2vec2.0 representations, and a novel model integrating alignment-free goodness-of-pronunciation (GOP) features computed via CTC. We introduce a weighted ordinal cross-entropy loss tailored for optimizing metrics such as unweighted average recall and mean absolute error. Among the explored methods, our GOP-CTC-based model achieved the highest performance, substantially surpassing challenge baselines and attaining top leaderboard scores.",
    "paper_abstract_zh": "本文分析了为 NOCASA 2025 挑战赛开发的三种端到端模型，旨在为以挪威语为第二语言的儿童提供自动词级发音评估。所提模型包括：编码器-解码器孪生架构（E2E-R）、利用预训练 wav2vec2.0 表示的前缀微调直接分类模型，以及一种无需对齐即可通过 CTC 计算发音质量（GOP）特征的新模型。我们设计了加权序数交叉熵损失，以优化未加权平均召回率和平均绝对误差等指标。在多种方法中，基于 GOP-CTC 的模型表现最佳，大幅超越挑战赛基线，并取得排行榜最高分。",
    "primary_category": "cs.CL",
    "update_time": "2025-09-03",
    "paper_authors": "Aleksei Žavoronkov, Tanel Alumäe",
    "topic": [],
    "category": [
      "Other"
    ]
  },
  {
    "paper_title": "Non-Intrusive Intelligibility Prediction for Hearing Aids: Recent Advances, Trends, and Challenges",
    "paper_title_zh": "助听器非侵入式可懂度预测：最新进展、趋势与挑战",
    "paper_id": "2509.03017v1",
    "paper_abstract": "This paper provides an overview of recent progress in non-intrusive speech intelligibility prediction for hearing aids (HA). We summarize developments in robust acoustic feature extraction, hearing loss modeling, and the use of emerging architectures for long-sequence processing. Listener-specific adaptation strategies and domain generalization approaches that aim to improve robustness in unseen acoustic environments are also discussed. Remaining challenges, such as the need for large-scale, diverse datasets and reliable cross-profile generalization, are acknowledged. Our goal is to offer a perspective on current trends, ongoing challenges, and possible future directions toward practical and reliable HA-oriented intelligibility prediction systems.",
    "paper_abstract_zh": "本文综述了助听器（HA）非侵入式语音可懂度预测的最新进展，总结了鲁棒声学特征提取、听力损失建模以及面向长序列处理的新兴架构的发展。文章还讨论了听者特定自适应策略与领域泛化方法，旨在提升模型在未知声学环境下的鲁棒性。同时指出仍存在的挑战，如大规模多样化数据集的需求以及跨听力曲线可靠泛化等。我们旨在为当前趋势、持续挑战及未来可能的研究方向提供视角，以推动实用且可靠的面向助听器的可懂度预测系统的发展。",
    "primary_category": "eess.AS",
    "update_time": "2025-09-03",
    "paper_authors": "Ryandhimas E. Zezario",
    "topic": [],
    "category": [
      "Other"
    ]
  },
  {
    "paper_title": "Mitigating Data Imbalance in Automated Speaking Assessment",
    "paper_title_zh": "缓解自动口语评估中的数据不平衡问题",
    "paper_id": "2509.03010v1",
    "paper_abstract": "Automated Speaking Assessment (ASA) plays a crucial role in evaluating second-language (L2) learners proficiency. However, ASA models often suffer from class imbalance, leading to biased predictions. To address this, we introduce a novel objective for training ASA models, dubbed the Balancing Logit Variation (BLV) loss, which perturbs model predictions to improve feature representation for minority classes without modifying the dataset. Evaluations on the ICNALE benchmark dataset show that integrating the BLV loss into a celebrated text-based (BERT) model significantly enhances classification accuracy and fairness, making automated speech evaluation more robust for diverse learners.",
    "paper_abstract_zh": "自动口语评估（ASA）在衡量第二语言（L2）学习者水平方面至关重要。然而，ASA 模型常因类别不平衡而产生有偏预测。为此，我们提出一种新的训练目标——平衡 Logit 扰动（BLV）损失，它通过对模型预测施加扰动来增强少数类的特征表示，而无需改动原始数据集。在 ICNALE 基准数据集上的实验表明，将 BLV 损失集成到经典的基于文本的 BERT 模型后，分类准确率与公平性均显著提升，使自动化口语评估对多样化学习者更加鲁棒。",
    "primary_category": "cs.CL",
    "update_time": "2025-09-03",
    "paper_authors": "Fong-Chun Tsai, Kuan-Tang Huang, Bi-Cheng Yan, Tien-Hong Lo, Berlin Chen",
    "topic": [],
    "category": [
      "Other"
    ]
  }
]