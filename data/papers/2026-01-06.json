[
  {
    "paper_title": "Speak the Art: A Direct Speech to Image Generation Framework",
    "paper_title_zh": "Speak the Art: 一种直接语音到图像生成框架",
    "paper_id": "2601.00827",
    "paper_abstract": "Direct speech-to-image generation has recently shown promising results. However, compared to text-to-image generation, there is still a large gap to enclose. Current approaches use two stages to tackle this task: speech encoding network and image generative adversarial network (GAN). The speech encoding networks in these approaches produce embeddings that do not capture sufficient linguistic information to semantically represent the input speech. GANs suffer from issues such as non-convergence, mode collapse, and diminished gradient, which result in unstable model parameters, limited sample diversity, and ineffective generator learning, respectively. To address these weaknesses, we introduce a framework called \\textbf{Speak the Art (STA)} which consists of a speech encoding network and a VQ-Diffusion network conditioned on speech embeddings. To improve speech embeddings, the speech encoding network is supervised by a large pre-trained image-text model during training. Replacing GANs with diffusion leads to more stable training and the generation of diverse images. Additionally, we investigate the feasibility of extending our framework to be multilingual. As a proof of concept, we trained our framework with two languages: English and Arabic. Finally, we show that our results surpass state-of-the-art models by a large margin.",
    "paper_abstract_zh": "直接语音到图像生成最近显示出有前景的结果。然而，与文本到图像生成相比，仍然存在很大差距。当前方法采用两个阶段来处理此任务：语音编码网络和图像生成对抗网络（GAN）。这些方法中的语音编码网络生成的嵌入无法捕获足够的语言信息来语义上表示输入语音。GANs存在不收敛、模式崩溃和梯度消失等问题，分别导致模型参数不稳定、样本多样性有限和生成器学习无效。为了解决这些弱点，我们引入了一个名为Speak the Art (STA)的框架，该框架包含一个语音编码网络和一个基于语音嵌入条件的VQ-Diffusion网络。为了改进语音嵌入，语音编码网络在训练过程中由大型预训练图像-文本模型监督。用扩散模型替代GANs可以带来更稳定的训练和多样化的图像生成。此外，我们还研究了将我们的框架扩展为多语言的可行性。作为概念验证，我们使用两种语言训练了我们的框架：英语和阿拉伯语。最后，我们的结果显示大幅超越了最先进的模型。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)",
      "Artificial Intelligence (cs.AI)",
      "Multimedia (cs.MM)"
    ],
    "update_time": "2026-01-06",
    "paper_authors": "Mariam Saeed, Manar Amr, Farida Adel, Nada Hassan, Nour Walid, Eman Mohamed, Mohamed Hussein, Marwan Torki",
    "topic": [
      "Audio Representation Learning",
      "Image Generation"
    ],
    "category": [
      "Image&Video"
    ]
  },
  {
    "paper_title": "Improving Code-Switching Speech Recognition with TTS Data Augmentation",
    "paper_title_zh": "使用TTS数据增强提高代码切换语音识别",
    "paper_id": "2601.00935",
    "paper_abstract": "Automatic speech recognition (ASR) for conversational code-switching speech remains challenging due to the scarcity of realistic, high-quality labeled speech data. This paper explores multilingual text-to-speech (TTS) models as an effective data augmentation technique to address this shortage. Specifically, we fine-tune the multilingual CosyVoice2 TTS model on the SEAME dataset to generate synthetic conversational Chinese-English code-switching speech, significantly increasing the quantity and speaker diversity of available training data. Our experiments demonstrate that augmenting real speech with synthetic speech reduces the mixed error rate (MER) from 12.1 percent to 10.1 percent on DevMan and from 17.8 percent to 16.0 percent on DevSGE, indicating consistent performance gains. These results confirm that multilingual TTS is an effective and practical tool for enhancing ASR robustness in low-resource conversational code-switching scenarios.",
    "paper_abstract_zh": "对话式代码切换语音的自动语音识别(ASR)仍然具有挑战性，原因是缺乏真实、高质量的有标签语音数据。本文探索了多语言文本到语音(TTS)模型作为一种有效的数据增强技术来解决这个问题。具体来说，我们在SEAME数据集上微调多语言CosyVoice2 TTS模型，以生成合成对话式中英文代码切换语音，显著增加了可用训练数据的数量和说话人多样性。我们的实验表明，将合成语音与真实语音相结合，在DevMan上将混合错误率(MER)从12.1%降低到10.1%，在DevSGE上从17.8%降低到16.0%，表明性能持续提升。这些结果证实，多语言TTS是增强低资源对话式代码切换场景中ASR鲁棒性的有效且实用的工具。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)",
      "Artificial Intelligence (cs.AI)"
    ],
    "update_time": "2026-01-06",
    "paper_authors": "Yue Heng Yeo, Yuchen Hu, Shreyas Gopal, Yizhou Peng, Hexin Liu, Eng Siong Chng",
    "topic": [
      "Speech Recognition",
      "Speech Synthesis"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Bayesian Negative Binomial Regression of Afrobeats Chart Persistence",
    "paper_title_zh": "非洲节奏音乐榜单持久性的贝叶斯负二项回归",
    "paper_id": "2601.01391",
    "paper_abstract": "Afrobeats songs compete for attention on streaming platforms, where chart visibility can influence both revenue and cultural impact. This paper examines whether collaborations help songs remain on the charts longer, using daily Nigeria Spotify Top 200 data from 2024. Each track is summarized by the number of days it appears in the Top 200 during the year and its total annual streams in Nigeria. A Bayesian negative binomial regression is applied, with days on chart as the outcome and collaboration status (solo versus multi-artist) and log total streams as predictors. This approach is well suited for overdispersed count data and allows the effect of collaboration to be interpreted while controlling for overall popularity. Posterior inference is conducted using Markov chain Monte Carlo, and results are assessed using rate ratios, posterior probabilities, and predictive checks. The findings indicate that, after accounting for total streams, collaboration tracks tend to spend slightly fewer days on the chart than comparable solo tracks.",
    "paper_abstract_zh": "非洲节奏音乐歌曲在流媒体平台上争夺关注，榜单可见性可以影响收入和文化影响力。本文研究了合作是否有助于歌曲在榜单上停留更长时间，使用2024年尼日利亚Spotify Top 200的每日数据。每首歌曲通过其在一年内进入Top 200的天数和尼日利亚的年度总播放次数来总结。应用贝叶斯负二项回归，以在榜单上的天数为结果变量，合作状态（独唱与多艺术家）和总播放次数的对数为预测变量。这种方法非常适合过度离散的计数数据，并在控制整体流行度的同时允许解释合作的影响。使用马尔可夫链蒙特 Carlo进行后验推断，并通过比率比、后验概率和预测检查评估结果。研究结果表明，在考虑总播放次数后，合作歌曲往往比可比的独唱歌曲在榜单上的停留时间略短。",
    "subjects": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2026-01-06",
    "paper_authors": "Ian Jacob Cabansag, Paul Ntegeka",
    "topic": [
      "Music Information Retrieval"
    ],
    "category": [
      "Music"
    ]
  },
  {
    "paper_title": "MORE: Multi-Objective Adversarial Attacks on Speech Recognition",
    "paper_title_zh": "MORE：针对语音识别的多目标对抗攻击",
    "paper_id": "2601.01852",
    "paper_abstract": "The emergence of large-scale automatic speech recognition (ASR) models such as Whisper has greatly expanded their adoption across diverse real-world applications. Ensuring robustness against even minor input perturbations is therefore critical for maintaining reliable performance in real-time environments. While prior work has mainly examined accuracy degradation under adversarial attacks, robustness with respect to efficiency remains largely unexplored. This narrow focus provides only a partial understanding of ASR model vulnerabilities. To address this gap, we conduct a comprehensive study of ASR robustness under multiple attack scenarios. We introduce MORE, a multi-objective repetitive doubling encouragement attack, which jointly degrades recognition accuracy and inference efficiency through a hierarchical staged repulsion-anchoring mechanism. Specifically, we reformulate multi-objective adversarial optimization into a hierarchical framework that sequentially achieves the dual objectives. To further amplify effectiveness, we propose a novel repetitive encouragement doubling objective (REDO) that induces duplicative text generation by maintaining accuracy degradation and periodically doubling the predicted sequence length. Overall, MORE compels ASR models to produce incorrect transcriptions at a substantially higher computational cost, triggered by a single adversarial input. Experiments show that MORE consistently yields significantly longer transcriptions while maintaining high word error rates compared to existing baselines, underscoring its effectiveness in multi-objective adversarial attack.",
    "paper_abstract_zh": "大规模自动语音识别(ASR)模型（如Whisper）的出现极大地扩展了它们在多样化现实世界应用中的采用。因此，确保对轻微输入扰动的鲁棒性对于在实时环境中保持可靠性能至关重要。虽然先前的工作主要研究了对抗攻击下的准确率下降，但关于效率方面的鲁棒性在很大程度上仍未被探索。这种狭隘的关注点仅提供了对ASR模型漏洞的部分理解。为了解决这一差距，我们对多种攻击场景下的ASR鲁棒性进行了全面研究。我们引入了MORE，一种多目标重复加倍鼓励攻击，它通过分层阶段排斥-锚定机制联合降低识别准确性和推理效率。具体而言，我们将多目标对抗优化重新表述为一个分层框架，该框架按顺序实现双重目标。为了进一步增强效果，我们提出了一种新颖的重复鼓励加倍目标(REDO)，它通过保持准确率下降并定期加倍预测序列长度来诱导重复文本生成。总体而言，MORE迫使ASR模型在单个对抗输入的触发下，以显著更高的计算成本产生错误的转录。实验表明，与现有基线相比，MORE在保持高词错误率的同时 consistently 产生显著更长的转录，突显了其在多目标对抗攻击中的有效性。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "update_time": "2026-01-06",
    "paper_authors": "Xiaoxue Gao, Zexin Li, Yiming Chen, Nancy F. Chen",
    "topic": [
      "Speech Recognition"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Towards Prosodically Informed Mizo TTS without Explicit Tone Markings",
    "paper_title_zh": "在没有明确声调标记的情况下，面向韵律信息的米佐语TTS系统",
    "paper_id": "2601.02073",
    "paper_abstract": "This paper reports on the development of a text-to-speech (TTS) system for Mizo, a low-resource, tonal, and Tibeto-Burman language spoken primarily in the Indian state of Mizoram. The TTS was built with only 5.18 hours of data; however, in terms of subjective and objective evaluations, the outputs were considered perceptually acceptable and intelligible. A baseline model using Tacotron2 was built, and then, with the same data, another TTS model was built with VITS. In both subjective and objective evaluations, the VITS model outperformed the Tacotron2 model. In terms of tone synthesis, the VITS model showed significantly lower tone errors than the Tacotron2 model. The paper demonstrates that a non-autoregressive, end-to-end framework can achieve synthesis of acceptable perceptual quality and intelligibility.",
    "paper_abstract_zh": "本文报告了米佐语(text-to-speech, TTS)系统的开发，米佐语是一种低资源、声调性的藏缅语族语言，主要在印度米佐拉姆邦使用。该TTS系统仅使用了5.18小时的数据构建；然而，在主观和客观评估方面，其输出被认为在感知上可接受且可理解。研究人员构建了一个使用Tacotron2的基线模型，然后使用相同的数据构建了另一个使用VITS的TTS模型。在主观和客观评估中，VITS模型均优于Tacotron2模型。在声调合成方面，VITS模型的声调错误显著低于Tacotron2模型。本文证明，非自回归、端到端的框架可以实现具有可接受感知质量和可理解性的合成。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2026-01-06",
    "paper_authors": "Abhijit Mohanta, Remruatpuii, Priyankoo Sarmah, Rohit Sinha, Wendy Lalhminghlui",
    "topic": [
      "Speech Synthesis"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "On the Role of Spatial Features in Foundation-Model-Based Speaker Diarization",
    "paper_title_zh": "基于基础模型的说话人分离中空间特征的作用",
    "paper_id": "2601.02231",
    "paper_abstract": "Recent advances in speaker diarization exploit large pretrained foundation models, such as WavLM, to achieve state-of-the-art performance on multiple datasets. Systems like DiariZen leverage these rich single-channel representations, but are limited to single-channel audio, preventing the use of spatial cues available in multi-channel recordings. This work analyzes the impact of incorporating spatial information into a state-of-the-art single-channel diarization system by evaluating several strategies for conditioning the model on multi-channel spatial features. Experiments on meeting-style datasets indicate that spatial information can improve diarization performance, but the overall improvement is smaller than expected for the proposed system, suggesting that the features aggregated over all WavLM layers already capture much of the information needed for accurate speaker discrimination, also in overlapping speech regions. These findings provide insight into the potential and limitations of using spatial cues to enhance foundation model-based diarization.",
    "paper_abstract_zh": "最近的说话人分离进展利用了大型预训练基础模型（如WavLM），在多个数据集上实现了最先进的性能。像DiariZen这样的系统利用了这些丰富的单通道表示，但仅限于单通道音频，无法使用多通道录音中可用的空间线索。本文通过评估几种将模型条件化为多通道空间特征的方法，分析了将空间信息纳入最先进单通道分离系统的影响。在会议风格数据集上的实验表明，空间信息可以提高分离性能，但所提出系统的整体改进小于预期，这表明在所有WavLM层上聚合的特征已经捕获了准确说话人区分所需的大部分信息，即使在重叠语音区域也是如此。这些发现为使用空间线索增强基于基础模型的分离的潜力和局限性提供了见解。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2026-01-06",
    "paper_authors": "Marc Deegen, Tobias Gburrek, Tobias Cord-Landwehr, Thilo von Neumann, Jiangyu Han, Lukáš Burget, Reinhold Haeb-Umbach",
    "topic": [
      "Audio Representation Learning",
      "Speech Recognition"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Index-ASR Technical Report",
    "paper_title_zh": "Index-ASR 技术报告",
    "paper_id": "2601.00890",
    "paper_abstract": "Automatic speech recognition (ASR) has witnessed remarkable progress in recent years, largely driven by the emergence of LLM-based ASR paradigm. Despite their strong performance on a variety of open-source benchmarks, existing LLM-based ASR systems still suffer from two critical limitations. First, they are prone to hallucination errors, often generating excessively long and repetitive outputs that are not well grounded in the acoustic input. Second, they provide limited support for flexible and fine-grained contextual customization. To address these challenges, we propose Index-ASR, a large-scale LLM-based ASR system designed to simultaneously enhance robustness and support customizable hotword recognition. The core idea of Index-ASR lies in the integration of LLM and large-scale training data enriched with background noise and contextual information. Experimental results show that our Index-ASR achieves strong performance on both open-source benchmarks and in-house test sets, highlighting its robustness and practicality for real-world ASR applications.",
    "paper_abstract_zh": "近年来，自动语音识别（ASR）取得了显著进展，这主要得益于基于大语言模型（LLM）的ASR范式的出现。尽管现有基于LLM的ASR系统在各种开源基准测试上表现出色，但仍存在两个关键局限性。首先，它们容易出现幻觉错误，经常生成过长且重复的输出，而这些输出并未很好地基于声学输入。其次，它们对灵活且细粒度的上下文定制支持有限。为解决这些挑战，我们提出了Index-ASR，这是一个大规模基于LLM的ASR系统，旨在同时提高鲁棒性并支持可定制的热词识别。Index-ASR的核心思想在于整合LLM和大规模训练数据，这些数据富含背景噪声和上下文信息。实验结果表明，我们的Index-ASR在开源基准测试和内部测试集上都取得了强大的性能，突显了其在现实世界ASR应用中的鲁棒性和实用性。",
    "subjects": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2026-01-06",
    "paper_authors": "Zheshu Song, Lu Wang, Wei Deng, Zhuo Yang, Yong Wu, Bin Xia",
    "topic": [
      "Speech Recognition"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "IO-RAE: Information-Obfuscation Reversible Adversarial Example for Audio Privacy Protection",
    "paper_title_zh": "IO-RAE：用于音频隐私保护的信息混淆可逆对抗样本",
    "paper_id": "2601.01239",
    "paper_abstract": "The rapid advancements in artificial intelligence have significantly accelerated the adoption of speech recognition technology, leading to its widespread integration across various applications. However, this surge in usage also highlights a critical issue: audio data is highly vulnerable to unauthorized exposure and analysis, posing significant privacy risks for businesses and individuals. This paper introduces an Information-Obfuscation Reversible Adversarial Example (IO-RAE) framework, the pioneering method designed to safeguard audio privacy using reversible adversarial examples. IO-RAE leverages large language models to generate misleading yet contextually coherent content, effectively preventing unauthorized eavesdropping by humans and Automatic Speech Recognition (ASR) systems. Additionally, we propose the Cumulative Signal Attack technique, which mitigates high-frequency noise and enhances attack efficacy by targeting low-frequency signals. Our approach ensures the protection of audio data without degrading its quality or our ability. Experimental evaluations demonstrate the superiority of our method, achieving a targeted misguidance rate of 96.5% and a remarkable 100% untargeted misguidance rate in obfuscating target keywords across multiple ASR models, including a commercial black-box system from Google. Furthermore, the quality of the recovered audio, measured by the Perceptual Evaluation of Speech Quality score, reached 4.45, comparable to high-quality original recordings. Notably, the recovered audio processed by ASR systems exhibited an error rate of 0%, indicating nearly lossless recovery. These results highlight the practical applicability and effectiveness of our IO-RAE framework in protecting sensitive audio privacy.",
    "paper_abstract_zh": "人工智能的快速发展显著加速了语音识别技术的采用，导致其在各种应用中得到广泛集成。然而，这种使用激增也凸显了一个关键问题：音频数据极易受到未授权的暴露和分析，给企业和个人带来重大的隐私风险。本文介绍了一种信息混淆可逆对抗样本（IO-RAE）框架，这是首个旨在使用可逆对抗样本来保护音频隐私的方法。IO-RAE利用大型语言模型生成具有误导性但上下文连贯的内容，有效防止人类和自动语音识别（ASR）系统的未授权窃听。此外，我们提出了累积信号攻击技术，通过针对低频信号来减轻高频噪声并提高攻击效果。我们的方法确保在保护音频数据的同时不会降低其质量或我们的能力。实验评估证明了我们方法的优越性，在多个ASR模型（包括谷歌的商业黑盒系统）中混淆目标关键词时，实现了96.5%的定向误导率和100%的非定向误导率。此外，通过语音质量感知评估（PESQ）分数测量的恢复音频质量达到4.45，与高质量原始录音相当。值得注意的是，经过ASR系统处理的恢复音频显示出0%的错误率，表明几乎无损恢复。这些结果突显了我们的IO-RAE框架在保护敏感音频隐私方面的实际应用性和有效性。",
    "subjects": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)",
      "Multimedia (cs.MM)",
      "Cryptography and Security (cs.CR)"
    ],
    "update_time": "2026-01-06",
    "paper_authors": "Jiajie Zhu, Xia Du, Xiaoyuan Liu, Jizhe Zhou, Qizhen Xu, Zheng Lin, Chi-Man Pun",
    "topic": [
      "Speech Recognition"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Diffusion Timbre Transfer Via Mutual Information Guided Inpainting",
    "paper_title_zh": "基于互信息引导修复的音色迁移扩散模型",
    "paper_id": "2601.01294",
    "paper_abstract": "We study timbre transfer as an inference-time editing problem for music audio. Starting from a strong pre-trained latent diffusion model, we introduce a lightweight procedure that requires no additional training: (i) a dimension-wise noise injection that targets latent channels most informative of instrument identity, and (ii) an early-step clamping mechanism that re-imposes the input's melodic and rhythmic structure during reverse diffusion. The method operates directly on audio latents and is compatible with text/audio conditioning (e.g., CLAP). We discuss design choices,analyze trade-offs between timbral change and structural preservation, and show that simple inference-time controls can meaningfully steer pre-trained models for style-transfer use cases.",
    "paper_abstract_zh": "我们将音色迁移研究作为音乐音频的推理时编辑问题。从一个强大的预训练潜在扩散模型出发，我们引入了一个轻量级程序，无需额外训练：(i) 一种针对最能体现乐器身份的潜在通道的逐维度噪声注入，以及(ii) 一种在反向扩散过程中重新施加输入旋律和节奏结构的早期步长钳制机制。该方法直接在音频潜在空间上操作，并与文本/音频条件（如CLAP）兼容。我们讨论了设计选择，分析了音色变化与结构保存之间的权衡，并表明简单的推理时控制可以有效地引导预训练模型用于风格迁移用例。",
    "subjects": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)",
      "Artificial Intelligence (cs.AI)"
    ],
    "update_time": "2026-01-06",
    "paper_authors": "Ching Ho Lee, Javier Nistal, Stefan Lattner, Marco Pasini, George Fazekas",
    "topic": [
      "Music Generation",
      "Audio Representation Learning"
    ],
    "category": [
      "Music"
    ]
  },
  {
    "paper_title": "UltraEval-Audio: A Unified Framework for Comprehensive Evaluation of Audio Foundation Models",
    "paper_title_zh": "UltraEval-Audio: 音频基础模型的综合评估统一框架",
    "paper_id": "2601.01373",
    "paper_abstract": "The development of audio foundation models has accelerated rapidly since the emergence of GPT-4o. However, the lack of comprehensive evaluation has become a critical bottleneck for further progress in the field, particularly in audio generation. Current audio evaluation faces three major challenges: (1) audio evaluation lacks a unified framework, with datasets and code scattered across various sources, hindering fair and efficient cross-model comparison;(2) audio codecs, as a key component of audio foundation models, lack a widely accepted and holistic evaluation methodology; (3) existing speech benchmarks are heavily reliant on English, making it challenging to objectively assess models' performance on Chinese. To address the first issue, we introduce UltraEval-Audio, a unified evaluation framework for audio foundation models, specifically designed for both audio understanding and generation tasks. UltraEval-Audio features a modular architecture, supporting 10 languages and 14 core task categories, while seamlessly integrating 24 mainstream models and 36 authoritative benchmarks. To enhance research efficiency, the framework provides a one-command evaluation feature, accompanied by real-time public leaderboards. For the second challenge, UltraEval-Audio adopts a novel comprehensive evaluation scheme for audio codecs, evaluating performance across three key dimensions: semantic accuracy, timbre fidelity, and acoustic quality. To address the third issue, we propose two new Chinese benchmarks, SpeechCMMLU and SpeechHSK, designed to assess Chinese knowledge proficiency and language fluency. We wish that UltraEval-Audio will provide both academia and industry with a transparent, efficient, and fair platform for comparison of audio models. Our code, benchmarks, and leaderboards are available at this https URL.",
    "paper_abstract_zh": "自GPT-4o出现以来，音频基础模型的发展迅速加速。然而，缺乏全面评估已成为该领域进一步发展的关键瓶颈，特别是在音频生成方面。当前音频评估面临三大挑战：(1) 音频评估缺乏统一框架，数据集和代码分散在各种来源中，阻碍了公平高效的跨模型比较；(2) 作为音频基础模型关键组件的音频编解码器，缺乏广泛接受和全面的评估方法；(3) 现有的语音基准测试严重依赖英语，使得难以客观评估模型在中文上的性能。为解决第一个问题，我们引入了UltraEval-Audio，这是一个专为音频理解和生成任务设计的音频基础模型统一评估框架。UltraEval-Audio采用模块化架构，支持10种语言和14个核心任务类别，同时无缝集成24个主流模型和36个权威基准测试。为提高研究效率，该框架提供一键评估功能，并配有实时公开排行榜。针对第二个挑战，UltraEval-Audio采用了一种新颖的音频编解码器综合评估方案，从语义准确性、音色保真度和声学质量三个关键维度评估性能。为解决第三个问题，我们提出了两个新的中文基准测试SpeechCMMLU和SpeechHSK，用于评估中文知识熟练度和语言流利度。我们希望UltraEval-Audio能为学术界和产业界提供一个透明、高效、公平的音频模型比较平台。我们的代码、基准测试和排行榜可通过此https URL获取。",
    "subjects": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)",
      "Artificial Intelligence (cs.AI)"
    ],
    "update_time": "2026-01-06",
    "paper_authors": "Qundong Shi, Jie Zhou, Biyuan Lin, Junbo Cui, Guoyang Zeng, Yixuan Zhou, Ziyang Wang, Xin Liu, Zhen Luo, Yudong Wang, Zhiyuan Liu",
    "topic": [
      "Audio Representation Learning",
      "Audio Codec",
      "Speech Recognition",
      "Speech Synthesis"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "SAFE-QAQ: End-to-End Slow-Thinking Audio-Text Fraud Detection via Reinforcement Learning",
    "paper_title_zh": "SAFE-QAQ：基于强化学习的端到端慢思考音频文本欺诈检测",
    "paper_id": "2601.01392",
    "paper_abstract": "Existing fraud detection methods predominantly rely on transcribed text, suffering from ASR errors and missing crucial acoustic cues like vocal tone and environmental context. This limits their effectiveness against complex deceptive strategies. To address these challenges, we first propose \\textbf{SAFE-QAQ}, an end-to-end comprehensive framework for audio-based slow-thinking fraud detection. First, the SAFE-QAQ framework eliminates the impact of transcription errors on detection performance. Secondly, we propose rule-based slow-thinking reward mechanisms that systematically guide the system to identify fraud-indicative patterns by accurately capturing fine-grained audio details, through hierarchical reasoning processes. Besides, our framework introduces a dynamic risk assessment framework during live calls, enabling early detection and prevention of fraud. Experiments on the TeleAntiFraud-Bench demonstrate that SAFE-QAQ achieves dramatic improvements over existing methods in multiple key dimensions, including accuracy, inference efficiency, and real-time processing capabilities. Currently deployed and analyzing over 70,000 calls daily, SAFE-QAQ effectively automates complex fraud detection, reducing human workload and financial losses. Code: this https URL.",
    "paper_abstract_zh": "现有的欺诈检测方法主要依赖转录文本，这导致ASR错误并缺失了重要的声学线索，如语音语调和环境上下文。这限制了它们应对复杂欺骗策略的有效性。为解决这些挑战，我们首先提出了SAFE-QAQ，这是一个基于音频的慢思考欺诈检测的端到端综合框架。首先，SAFE-QAQ框架消除了转录错误对检测性能的影响。其次，我们提出了基于规则的慢思考奖励机制，通过分层推理过程准确捕捉细粒度音频细节，系统性地指导识别欺诈指示模式。此外，我们的框架在实时通话中引入了动态风险评估框架，能够及早检测和预防欺诈。在TeleAntiFraud-Bench上的实验表明，SAFE-QAQ在准确性、推理效率和实时处理能力等多个关键维度上显著优于现有方法。目前，SAFE-QAQ已部署并每天分析超过70,000次通话，有效自动化了复杂欺诈检测，减少了人工工作量和财务损失。代码：this https URL。",
    "subjects": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)",
      "Computation and Language (cs.CL)"
    ],
    "update_time": "2026-01-06",
    "paper_authors": "Peidong Wang, Zhiming Ma, Xin Dai, Yongkang Liu, Shi Feng, Xiaocui Yang, Wenxing Hu, Zhihao Wang, Mingjun Pan, Li Yuan, Daling Wang",
    "topic": [
      "Audio Representation Learning",
      "Audio Classification"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "OV-InstructTTS: Towards Open-Vocabulary Instruct Text-to-Speech",
    "paper_title_zh": "OV-InstructTTS: 朝向开放词汇指令文本转语音",
    "paper_id": "2601.01459",
    "paper_abstract": "Instruct Text-to-Speech (InstructTTS) leverages natural language descriptions as style prompts to guide speech synthesis. However, existing InstructTTS methods mainly rely on a direct combination of audio-related labels or their diverse rephrasings, making it difficult to handle flexible, high-level instructions. Such rigid control is insufficient for users such as content creators who wish to steer generation with descriptive instructions. To address these constraints, we introduce OV-InstructTTS, a new paradigm for open-vocabulary InstructTTS. We propose a comprehensive solution comprising a newly curated dataset, OV-Speech, and a novel reasoning-driven framework. The OV-Speech dataset pairs speech with open-vocabulary instructions, each augmented with a reasoning process that connects high-level instructions to acoustic features. The reasoning-driven framework infers emotional, acoustic, and paralinguistic information from open-vocabulary instructions before synthesizing speech. Evaluations show that this reasoning-driven approach significantly improves instruction-following fidelity and speech expressiveness. We believe this work can inspire the next user-friendly InstructTTS systems with stronger generalization and real-world applicability. The dataset and demos are publicly available on our project page.",
    "paper_abstract_zh": "指令文本转语音（InstructTTS）利用自然语言描述作为风格提示来指导语音合成。然而，现有的InstructTTS方法主要依赖于音频相关标签的直接组合或其多样化的重新表述，这使得处理灵活的高级指令变得困难。这种严格的控制对于希望使用描述性指令引导生成的内容创作者等用户来说是不够的。为了解决这些限制，我们引入了OV-InstructTTS，这是一种开放词汇InstructTTS的新范式。我们提出了一个全面的解决方案，包括一个新策划的数据集OV-Speech和一个新颖的推理驱动框架。OV-Speech数据集将语音与开放词汇指令配对，每个指令都附有一个推理过程，该过程将高级指令与声学特征联系起来。推理驱动框架在合成语音之前，从开放词汇指令中推断情感、语言副特征等信息。评估表明，这种推理驱动的方法显著提高了指令遵循的保真度和语音表现力。我们相信这项工作可以启发下一代具有更强泛化能力和实际应用性的用户友好型InstructTTS系统。数据集和演示在我们的项目页面上公开提供。",
    "subjects": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2026-01-06",
    "paper_authors": "Yong Ren, Jiangyan Yi, Jianhua Tao, Haiyang Sun, Zhengqi Wen, Hao Gu, Le Xu, Ye Bai",
    "topic": [
      "Speech Synthesis"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Bridging the gap: A comparative exploration of Speech-LLM and end-to-end architecture for multilingual conversational ASR",
    "paper_title_zh": "弥合差距：Speech-LLM与端到端架构在多语言对话ASR中的比较探索",
    "paper_id": "2601.01461",
    "paper_abstract": "The INTERSPEECH 2025 Challenge on Multilingual Conversational Speech Language Models (MLC-SLM) promotes multilingual conversational ASR with large language models (LLMs). Our previous SHNU-mASR system adopted a competitive parallel-speech-encoder architecture that integrated Whisper and mHuBERT with an LLM. However, it faced two challenges: simple feature concatenation may not fully exploit complementary information, and the performance gap between LLM-based ASR and end-to-end(E2E) encoder-decoder ASR remained unexplored. In this work, we present an enhanced LLM-based ASR framework that combines fine-tuned Whisper and mHuBERT encoders with an LLM to enrich speech representations. We first evaluate E2E Whisper models with LoRA and full fine-tuning on the MLC-SLM ASR task, and then propose cross-attention-based fusion mechanisms for the parallel-speech-encoder. On the official evaluation set of the MLC-SLM Challenge, our system achieves a CER/WER of 10.69%, ranking on par with the top-ranked Track 1 systems, even though it uses only 1,500 hours of baseline training data compared with their large-scale training sets. Nonetheless, we find that our final LLM-based ASR still does not match the performance of a fine-tuned E2E Whisper model, providing valuable empirical guidance for future Speech-LLM design. Our code is publicly available at this https URL.",
    "paper_abstract_zh": "INTERSPEECH 2025多语言对话语音语言模型(MLC-SLM)挑战赛旨在通过大型语言模型(LLMs)促进多语言对话自动语音识别(ASR)。我们之前的SHNU-mASR系统采用了一种具有竞争力的并行语音编码器架构，将Whisper和mHuBERT与LLM集成。然而，它面临两个挑战：简单的特征连接可能无法充分利用互补信息，并且基于LLM的ASR与端到端(E2E)编码器-解码器ASR之间的性能差距仍未被探索。在这项工作中，我们提出了一个增强的基于LLM的ASR框架，结合了微调的Whisper和mHuBERT编码器与LLM，以丰富语音表示。我们首先在MLC-SLM ASR任务上使用LoRA和完全微调评估E2E Whisper模型，然后提出了基于交叉注意力的并行语音编码器融合机制。在MLC-SLM挑战赛的官方评估集上，我们的系统实现了10.69%的CER/WER，与排名第一的Track 1系统并列，尽管与它们的大规模训练集相比，我们仅使用了1500小时的基线训练数据。尽管如此，我们发现我们最终的基于LLM的ASR仍然无法与微调的E2E Whisper模型相匹配，为未来的Speech-LLM设计提供了宝贵的经验指导。我们的代码已在提供的URL上公开。",
    "subjects": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)",
      "Computation and Language (cs.CL)"
    ],
    "update_time": "2026-01-06",
    "paper_authors": "Yuxiang Mei, Dongxing Xu, Jiaen Liang, Yanhua Long",
    "topic": [
      "Speech Recognition",
      "Audio Representation Learning"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "MOSS Transcribe Diarize: Accurate Transcription with Speaker Diarization",
    "paper_title_zh": "MOSS Transcribe Diarize: 带说话人分离的准确转录",
    "paper_id": "2601.01554",
    "paper_abstract": "Speaker-Attributed, Time-Stamped Transcription (SATS) aims to transcribe what is said and to precisely determine the timing of each speaker, which is particularly valuable for meeting transcription. Existing SATS systems rarely adopt an end-to-end formulation and are further constrained by limited context windows, weak long-range speaker memory, and the inability to output timestamps. To address these limitations, we present MOSS Transcribe Diarize, a unified multimodal large language model that jointly performs Speaker-Attributed, Time-Stamped Transcription in an end-to-end paradigm. Trained on extensive real wild data and equipped with a 128k context window for up to 90-minute inputs, MOSS Transcribe Diarize scales well and generalizes robustly. Across comprehensive evaluations, it outperforms state-of-the-art commercial systems on multiple public and in-house benchmarks.",
    "paper_abstract_zh": "说话人属性时间戳转录(SATS)旨在转录所说的内容并精确确定每个说话人的时间点，这对会议转录特别有价值。现有的SATS系统很少采用端到端的公式，并且受到有限上下文窗口、弱长程说话人记忆和无法输出时间戳的限制。为解决这些局限性，我们提出了MOSS Transcribe Diarize，这是一个统一的 multimodal 大型语言模型，以端到端范式联合执行说话人属性时间戳转录。该模型在大量真实野外数据上进行训练，并配备了128k上下文窗口，可处理长达90分钟的输入，MOSS Transcribe Diarize扩展性好且泛化能力强。在全面评估中，它在多个公共和内部基准测试上优于最先进的商业系统。",
    "subjects": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)",
      "Artificial Intelligence (cs.AI)"
    ],
    "update_time": "2026-01-06",
    "paper_authors": "Donghua Yu, Zhengyuan Lin, Chen Yang, Yiyang Zhang, Zhaoye Fei, Hanfu Chen, Jingqi Chen, Ke Chen, Qinyuan Cheng, Liwei Fan, Yi Jiang, Jie Zhu, Muchen Li, Shimin Li, Wenxuan Wang, Yang Wang, Zhe Xu, Yitian Gong, Yuqian Zhang",
    "topic": [
      "Speech Recognition",
      "Audio Representation Learning"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "MM-Sonate: Multimodal Controllable Audio-Video Generation with Zero-Shot Voice Cloning",
    "paper_title_zh": "MM-Sonate：基于零样本语音克隆的多模态可控音频-视频生成",
    "paper_id": "2601.01568",
    "paper_abstract": "Joint audio-video generation aims to synthesize synchronized multisensory content, yet current unified models struggle with fine-grained acoustic control, particularly for identity-preserving speech. Existing approaches either suffer from temporal misalignment due to cascaded generation or lack the capability to perform zero-shot voice cloning within a joint synthesis framework. In this work, we present MM-Sonate, a multimodal flow-matching framework that unifies controllable audio-video joint generation with zero-shot voice cloning capabilities. Unlike prior works that rely on coarse semantic descriptions, MM-Sonate utilizes a unified instruction-phoneme input to enforce strict linguistic and temporal alignment. To enable zero-shot voice cloning, we introduce a timbre injection mechanism that effectively decouples speaker identity from linguistic content. Furthermore, addressing the limitations of standard classifier-free guidance in multimodal settings, we propose a noise-based negative conditioning strategy that utilizes natural noise priors to significantly enhance acoustic fidelity. Empirical evaluations demonstrate that MM-Sonate establishes new state-of-the-art performance in joint generation benchmarks, significantly outperforming baselines in lip synchronization and speech intelligibility, while achieving voice cloning fidelity comparable to specialized Text-to-Speech systems.",
    "paper_abstract_zh": "联合音频-视频生成旨在合成同步的多感官内容，但当前统一模型难以实现细粒度的声学控制，特别是在保持身份特征的语音生成方面。现有方法要么因级联生成导致时间对齐问题，要么在联合合成框架内缺乏零样本语音克隆能力。在这项工作中，我们提出了MM-Sonate，一个多模态流匹配框架，统一了可控的音频-视频联合生成与零样本语音克隆能力。与依赖粗略语义描述的先前工作不同，MM-Sonate采用统一的指令-音素输入来强制严格的语言学和时间对齐。为实现零样本语音克隆，我们引入了一种音色注入机制，有效将说话人身份与语言内容解耦。此外，针对标准无分类器引导在多模态环境中的局限性，我们提出了一种基于噪声的负条件策略，利用自然噪声先验显著提高声学保真度。实证评估表明，MM-Sonate在联合生成基准测试中建立了新的最先进性能，在唇同步和语音清晰度方面显著优于基线模型，同时实现了与专业文本到语音系统相当的语音克隆保真度。",
    "subjects": [
      "Artificial Intelligence (cs.AI)",
      "Sound (cs.SD)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Audio and Speech Processing (eess.AS)",
      "Multimedia (cs.MM)"
    ],
    "update_time": "2026-01-06",
    "paper_authors": "Chunyu Qiang, Jun Wang, Xiaopeng Wang, Kang Yin, Yuxin Guo, Xijuan Zeng, Nan Li, Zihan Li, Yuzhe Liang, Ziyu Zhang, Teng Ma, Yushen Chen, Zhongliang Liu, Feng Deng, Chen Zhang, Pengfei Wan",
    "topic": [
      "Speech Synthesis",
      "Video Generation",
      "Audio Representation Learning"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Towards Multi-Level Transcript Segmentation: LoRA Fine-Tuning for Table-of-Contents Generation",
    "paper_title_zh": "迈向多级文本分段：用于目录生成的LoRA微调",
    "paper_id": "2601.02128",
    "paper_abstract": "Segmenting speech transcripts into thematic sections benefits both downstream processing and users who depend on written text for accessibility. We introduce a novel approach to hierarchical topic segmentation in transcripts, generating multi-level tables of contents that capture both topic and subtopic boundaries. We compare zero-shot prompting and LoRA fine-tuning on large language models, while also exploring the integration of high-level speech pause features. Evaluations on English meeting recordings and multilingual lecture transcripts (Portuguese, German) show significant improvements over established topic segmentation baselines. Additionally, we adapt a common evaluation measure for multi-level segmentation, taking into account all hierarchical levels within one metric.",
    "paper_abstract_zh": "将语音转录文本分割成主题部分有利于下游处理以及依赖书面文本的可访问性用户。我们引入了一种新颖的转录文本层次主题分段方法，生成捕获主题和子主题边界的多级目录。我们比较了大型语言模型上的零样本提示和LoRA微调，同时探索了高级语音停顿特征的集成。在英语会议录音和多语言讲座转录（葡萄牙语、德语）上的评估显示，与既定的主题分段基线相比有显著改进。此外，我们调整了一种常见的多级分段评估指标，在一个指标中考虑了所有层次级别。",
    "subjects": [
      "Computation and Language (cs.CL)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2026-01-06",
    "paper_authors": "Steffen Freisinger, Philipp Seeberger, Thomas Ranzenberger, Tobias Bocklet, Korbinian Riedhammer",
    "topic": [
      "Speech Recognition",
      "Audio Representation Learning"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "DARC: Drum accompaniment generation with fine-grained rhythm control",
    "paper_title_zh": "DARC：具有细粒度节奏控制的鼓点伴奏生成",
    "paper_id": "2601.02357",
    "paper_abstract": "In music creation, rapid prototyping is essential for exploring and refining ideas, yet existing generative tools often fall short when users require both structural control and stylistic flexibility. Prior approaches in stem-to-stem generation can condition on other musical stems but offer limited control over rhythm, and timbre-transfer methods allow users to specify specific rhythms, but cannot condition on musical context. We introduce DARC, a generative drum accompaniment model that conditions both on musical context from other stems and explicit rhythm prompts such as beatboxing or tapping tracks. Using parameter-efficient fine-tuning, we augment STAGE, a state-of-the-art drum stem generator, with fine-grained rhythm control while maintaining musical context awareness.",
    "paper_abstract_zh": "在音乐创作中，快速原型制作对于探索和完善想法至关重要，然而当用户需要结构控制和风格灵活性时，现有的生成工具往往表现不佳。先前的stem-to-stem生成方法可以基于其他音乐stem进行条件生成，但对节奏的控制有限；而音色传输方法允许用户指定特定节奏，但无法基于音乐上下文进行条件生成。我们介绍了DARC，这是一种生成式鼓点伴奏模型，它既基于其他stem的音乐上下文，也基于明确的节奏提示（如beatboxing或敲击轨道）进行条件生成。通过参数高效的微调，我们在保持音乐上下文感知能力的同时，为最先进的鼓点stem生成器STAGE增加了细粒度的节奏控制功能。",
    "subjects": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)",
      "Artificial Intelligence (cs.AI)"
    ],
    "update_time": "2026-01-06",
    "paper_authors": "Trey Brosnan",
    "topic": [
      "Music Generation",
      "Music Information Retrieval"
    ],
    "category": [
      "Music"
    ]
  },
  {
    "paper_title": "BeatlesFC: Harmonic function annotations of Isophonics' The Beatles dataset",
    "paper_title_zh": "BeatlesFC：Isophonics的The Beatles数据集的和声功能标注",
    "paper_id": "2601.02099",
    "paper_abstract": "This paper presents BeatlesFC, a set of harmonic function annotations for Isophonics' The Beatles dataset. Harmonic function annotations characterize chord labels as stable (tonic) or unstable (predominant, dominant). They operate at the level of musical phrases, serving as a link between chord labels and higher-level formal structures.",
    "paper_abstract_zh": "本文介绍了BeatlesFC，这是为Isophonics的The Beatles数据集提供的一组和声功能标注。和声功能标注将和弦标签表征为稳定的主音或不稳定的优势音和属音。它们在音乐短语的层面上运作，作为和弦标签与更高级形式结构之间的桥梁。",
    "subjects": [
      "Sound (cs.SD)"
    ],
    "update_time": "2026-01-06",
    "paper_authors": "Ji Yeoung Sim, Rebecca Moranis, Johanna Devaney",
    "topic": [
      "Music Information Retrieval"
    ],
    "category": [
      "Music"
    ]
  },
  {
    "paper_title": "A Mamba-Based Model for Automatic Chord Recognition",
    "paper_title_zh": "基于Mamba的自动和弦识别模型",
    "paper_id": "2601.02101",
    "paper_abstract": "In this work, we propose a new efficient solution, which is a Mamba-based model named BMACE (Bidirectional Mamba-based network, for Automatic Chord Estimation), which utilizes selective structured state-space models in a bidirectional Mamba layer to effectively model temporal dependencies. Our model achieves high prediction performance comparable to state-of-the-art models, with the advantage of requiring fewer parameters and lower computational resources",
    "paper_abstract_zh": "在这项工作中，我们提出了一种新的高效解决方案，名为BMACE（基于双向Mamba网络的自动和弦估计模型），该模型利用双向Mamba层中的选择性结构化状态空间模型来有效建模时间依赖性。我们的模型实现了与最先进模型相当的高预测性能，同时具有参数更少和计算资源需求更低的优点。",
    "subjects": [
      "Sound (cs.SD)"
    ],
    "update_time": "2026-01-06",
    "paper_authors": "Chunyu Yuan, Johanna Devaney",
    "topic": [
      "Music Information Retrieval",
      "Audio Classification"
    ],
    "category": [
      "Music"
    ]
  },
  {
    "paper_title": "HyperCLOVA X 8B Omni",
    "paper_title_zh": "HyperCLOVA X 8B Omni",
    "paper_id": "2601.01792",
    "paper_abstract": "In this report, we present HyperCLOVA X 8B Omni, the first any-to-any omnimodal model in the HyperCLOVA X family that supports text, audio, and vision as both inputs and outputs. By consolidating multimodal understanding and generation into a single model rather than separate modality-specific pipelines, HyperCLOVA X 8B Omni serves as an 8B-scale omni-pathfinding point toward practical any-to-any omni assistants. At a high level, the model unifies modalities through a shared next-token prediction interface over an interleaved multimodal sequence, while vision and audio encoders inject continuous embeddings for fine-grained understanding and grounding. Empirical evaluations demonstrate competitive performance against comparably sized models across diverse input-output combinations spanning text, audio, and vision, in both Korean and English. We anticipate that the open-weight release of HyperCLOVA X 8B Omni will support a wide range of research and deployment scenarios.",
    "paper_abstract_zh": "在本报告中，我们介绍了HyperCLOVA X 8B Omni，这是HyperCLOVA X家族中首个支持文本、音频和视觉作为输入和输出的any-to-any多模态模型。通过将多模态理解和生成整合到单一模型中，而不是使用特定的模态流水线，HyperCLOVA X 8B Omni作为一个8B规模的全方位探索点，朝着实用的any-to-any全方位助手迈进。从高层次来看，该模型通过在交错的多模态序列上共享下一个token预测接口来统一模态，同时视觉和音频编码器注入连续嵌入以实现细粒度理解和定位。实证评估表明，在涵盖文本、音频和视觉的多样化输入输出组合中，该模型在韩语和英语方面与同等规模的模型相比具有竞争力。我们预计HyperCLOVA X 8B Omni的开源权重将支持广泛的研究和部署场景。",
    "subjects": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)"
    ],
    "update_time": "2026-01-06",
    "paper_authors": "NAVER Cloud HyperCLOVA X Team",
    "topic": [
      "Audio Representation Learning",
      "Speech Recognition",
      "Speech Synthesis",
      "Image Generation",
      "Video Generation"
    ],
    "category": [
      "Other"
    ]
  },
  {
    "paper_title": "ARCADE: A City-Scale Corpus for Fine-Grained Arabic Dialect Tagging",
    "paper_title_zh": "ARCADE：用于细粒度阿拉伯语方言标注的城市规模语料库",
    "paper_id": "2601.02209",
    "paper_abstract": "The Arabic language is characterized by a rich tapestry of regional dialects that differ substantially in phonetics and lexicon, reflecting the geographic and cultural diversity of its speakers. Despite the availability of many multi-dialect datasets, mapping speech to fine-grained dialect sources, such as cities, remains underexplored. We present ARCADE (Arabic Radio Corpus for Audio Dialect Evaluation), the first Arabic speech dataset designed explicitly with city-level dialect granularity. The corpus comprises Arabic radio speech collected from streaming services across the Arab world. Our data pipeline captures 30-second segments from verified radio streams, encompassing both Modern Standard Arabic (MSA) and diverse dialectal speech. To ensure reliability, each clip was annotated by one to three native Arabic reviewers who assigned rich metadata, including emotion, speech type, dialect category, and a validity flag for dialect identification tasks. The resulting corpus comprises 6,907 annotations and 3,790 unique audio segments spanning 58 cities across 19 countries. These fine-grained annotations enable robust multi-task learning, serving as a benchmark for city-level dialect tagging. We detail the data collection methodology, assess audio quality, and provide a comprehensive analysis of label distributions. The dataset is available on: this https URL",
    "paper_abstract_zh": "阿拉伯语以其丰富的地区方言为特征，这些方言在语音和词汇上存在显著差异，反映了其使用者的地理和文化多样性。尽管有许多多方言数据集可用，但将语音映射到细粒度的方言来源（如城市）的研究仍不充分。我们提出了ARCADE（阿拉伯语广播音频方言评估语料库），这是第一个专门为城市级方言粒度设计的阿拉伯语音数据集。该语料库包含从阿拉伯世界各地的流媒体服务收集的阿拉伯语广播语音。我们的数据管道从经过验证的广播流中捕获30秒的片段，涵盖现代标准阿拉伯语（MSA）和各种方言语音。为确保可靠性，每个片段由1至3名阿拉伯语母语评审者进行标注，分配丰富的元数据，包括情感、语音类型、方言类别以及方言识别任务的有效性标志。 resulting语料库包含6,907个标注和3,790个独特的音频片段，覆盖19个国家的58个城市。这些细粒度标注支持强大的多任务学习，可作为城市级方言标注的基准。我们详细介绍了数据收集方法，评估了音频质量，并提供了标签分布的全面分析。该数据集可在以下网址获取：this https URL",
    "subjects": [
      "Computation and Language (cs.CL)",
      "Computers and Society (cs.CY)",
      "Sound (cs.SD)"
    ],
    "update_time": "2026-01-06",
    "paper_authors": "Omer Nacar, Serry Sibaee, Adel Ammar, Yasser Alhabashi, Nadia Samer Sibai, Yara Farouk Ahmed, Ahmed Saud Alqusaiyer, Sulieman Mahmoud AlMahmoud, Abdulrhman Mamdoh Mukhaniq, Lubaba Raed, Sulaiman Mohammed Alatwah, Waad Nasser Alqahtani, Yousif Abdulmajeed Alnasser, Mohamed Aziz Khadraoui, Wadii Boulila",
    "topic": [
      "Audio Classification",
      "Speech Recognition"
    ],
    "category": [
      "Speech"
    ]
  }
]