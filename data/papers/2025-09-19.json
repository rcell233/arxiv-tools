[
  {
    "paper_title": "SpeechOp: Inference-Time Task Composition for Generative Speech Processing",
    "paper_title_zh": "SpeechOp：生成式语音处理的推理时任务组合",
    "paper_id": "2509.14298",
    "paper_abstract": "While generative Text-to-Speech (TTS) systems leverage vast ``in-the-wild\" data to achieve remarkable success, speech-to-speech processing tasks like enhancement face data limitations, which lead data-hungry generative approaches to distort speech content and speaker identity. To bridge this gap, we present SpeechOp, a multi-task latent diffusion model that transforms pre-trained TTS models into a universal speech processor capable of performing a wide range of speech tasks and composing them in novel ways at inference time. By adapting a pre-trained TTS model, SpeechOp inherits a rich understanding of natural speech, accelerating training and improving S2S task quality, while simultaneously enhancing core TTS performance. Finally, we introduce Implicit Task Composition (ITC), a novel pipeline where ASR-derived transcripts (e.g., from Whisper) guide SpeechOp's enhancement via our principled inference-time task composition. ITC achieves state-of-the-art content preservation by robustly combining web-scale speech understanding with SpeechOp's generative capabilities. Audio samples are available at this https URL",
    "paper_abstract_zh": "尽管生成式文本转语音（TTS）系统利用大量“野外”数据取得了显著成功，但语音增强等语音到语音处理任务面临数据限制，导致数据饥渴的生成方法扭曲语音内容和说话人身份。为弥补这一差距，我们提出了SpeechOp——一种多任务潜在扩散模型，可将预训练的TTS模型转化为通用语音处理器，能够在推理时执行多种语音任务并以新颖方式组合它们。通过适配预训练TTS模型，SpeechOp继承了对自然语音的丰富理解，加速了训练并提升了语音到语音任务质量，同时增强了核心TTS性能。最后，我们引入了隐式任务组合（ITC）这一新颖流程，其中ASR生成的转录文本（如来自Whisper）通过我们基于原则的推理时任务组合来指导SpeechOp的增强。ITC通过将网络级语音理解与SpeechOp的生成能力稳健结合，实现了最先进的内容保存效果。音频样本请访问此https URL",
    "subjects": [
      "Audio and Speech Processing (eess.AS)",
      "Machine Learning (cs.LG)"
    ],
    "update_time": "2025-09-19",
    "paper_authors": "Justin Lovelace, Rithesh Kumar, Jiaqi Su, Ke Chen, Kilian Q Weinberger, Zeyu Jin",
    "topic": [
      "Speech Synthesis",
      "Speech Enhancement"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Diffusion-Based Unsupervised Audio-Visual Speech Separation in Noisy Environments with Noise Prior",
    "paper_title_zh": "基于扩散模型的噪声环境下无监督视听语音分离方法——结合噪声先验信息",
    "paper_id": "2509.14379",
    "paper_abstract": "In this paper, we address the problem of single-microphone speech separation in the presence of ambient noise. We propose a generative unsupervised technique that directly models both clean speech and structured noise components, training exclusively on these individual signals rather than noisy mixtures. Our approach leverages an audio-visual score model that incorporates visual cues to serve as a strong generative speech prior. By explicitly modelling the noise distribution alongside the speech distribution, we enable effective decomposition through the inverse problem paradigm. We perform speech separation by sampling from the posterior distributions via a reverse diffusion process, which directly estimates and removes the modelled noise component to recover clean constituent signals. Experimental results demonstrate promising performance, highlighting the effectiveness of our direct noise modelling approach in challenging acoustic environments.",
    "paper_abstract_zh": "本文研究了在环境噪声存在下的单麦克风语音分离问题。我们提出了一种生成式无监督技术，直接对纯净语音和结构化噪声成分进行建模，并且仅在这些独立信号而非含噪混合信号上进行训练。我们的方法利用了一个结合视觉线索的视听评分模型，作为强大的生成式语音先验。通过同时显式建模噪声分布和语音分布，我们能够通过逆问题范式实现有效的分解。我们通过反向扩散过程从后验分布中采样来进行语音分离，直接估计并移除建模的噪声成分以恢复纯净的组成信号。实验结果表明了令人鼓舞的性能，凸显了我们在具有挑战性的声学环境中采用直接噪声建模方法的有效性。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)",
      "Machine Learning (cs.LG)"
    ],
    "update_time": "2025-09-19",
    "paper_authors": "Yochai Yemini, Rami Ben-Ari, Sharon Gannot, Ethan Fetaya",
    "topic": [
      "Speech Enhancement"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Multi-Channel Differential ASR for Robust Wearer Speech Recognition on Smart Glasses",
    "paper_title_zh": "多通道差分自动语音识别技术在智能眼镜上实现鲁棒性佩戴者语音识别",
    "paper_id": "2509.14430",
    "paper_abstract": "With the growing adoption of wearable devices such as smart glasses for AI assistants, wearer speech recognition (WSR) is becoming increasingly critical to next-generation human-computer interfaces. However, in real environments, interference from side-talk speech remains a significant challenge to WSR and may cause accumulated errors for downstream tasks such as natural language processing. In this work, we introduce a novel multi-channel differential automatic speech recognition (ASR) method for robust WSR on smart glasses. The proposed system takes differential inputs from different frontends that complement each other to improve the robustness of WSR, including a beamformer, microphone selection, and a lightweight side-talk detection model. Evaluations on both simulated and real datasets demonstrate that the proposed system outperforms the traditional approach, achieving up to an 18.0% relative reduction in word error rate.",
    "paper_abstract_zh": "随着智能眼镜等可穿戴设备在人工智能助手领域的日益普及，佩戴者语音识别（WSR）正逐渐成为下一代人机界面的关键技术。然而，在真实环境中，来自旁白语音的干扰仍然是WSR面临的重要挑战，并可能对自然语言处理等下游任务造成累积错误。本研究提出了一种新颖的多通道差分自动语音识别（ASR）方法，用于在智能眼镜上实现鲁棒性WSR。该系统采用来自不同前端且相互补充的差分输入，包括波束成形器、麦克风选择和轻量化旁白检测模型，以提升WSR的鲁棒性。在模拟和真实数据集上的评估表明，所提出的系统优于传统方法，实现了高达18.0%的词错误率相对降低。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "update_time": "2025-09-19",
    "paper_authors": "Yufeng Yang, Yiteng Huang, Yong Xu, Li Wan, Suwon Shon, Yang Liu, Yifeng Fan, Zhaojun Yang, Olivier Siohan, Yue Liu, Ming Sun, Florian Metze",
    "topic": [
      "Speech Recognition",
      "Speech Enhancement"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Mitigating Intra-Speaker Variability in Diarization with Style-Controllable Speech Augmentation",
    "paper_title_zh": "通过风格可控语音增强减轻说话人日志中的说话人内部变异性",
    "paper_id": "2509.14632",
    "paper_abstract": "Speaker diarization systems often struggle with high intrinsic intra-speaker variability, such as shifts in emotion, health, or content. This can cause segments from the same speaker to be misclassified as different individuals, for example, when one raises their voice or speaks faster during conversation. To address this, we propose a style-controllable speech generation model that augments speech across diverse styles while preserving the target speaker's identity. The proposed system starts with diarized segments from a conventional diarizer. For each diarized segment, it generates augmented speech samples enriched with phonetic and stylistic diversity. And then, speaker embeddings from both the original and generated audio are blended to enhance the system's robustness in grouping segments with high intrinsic intra-speaker variability. We validate our approach on a simulated emotional speech dataset and the truncated AMI dataset, demonstrating significant improvements, with error rate reductions of 49% and 35% on each dataset, respectively.",
    "paper_abstract_zh": "说话人日志系统常常难以应对高内在的说话人内部变异性，例如情绪、健康状况或内容的变化。这可能导致来自同一说话人的片段被错误分类为不同个体，例如当一个人在对话中提高音量或加快语速时。为了解决这个问题，我们提出了一种风格可控的语音生成模型，该模型能在保持目标说话人身份的同时，生成跨多样风格的增强语音。所提出的系统从传统日志器获取已标注的片段开始，针对每个已标注片段，生成富含音素和风格多样性的增强语音样本。然后，将原始音频和生成音频中的说话人嵌入进行融合，以增强系统在分组具有高内在说话人内部变异性片段时的鲁棒性。我们在模拟情感语音数据集和截断的AMI数据集上验证了我们的方法，结果显示显著改进，错误率分别降低了49%和35%。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)",
      "Artificial Intelligence (cs.AI)",
      "Signal Processing (eess.SP)"
    ],
    "update_time": "2025-09-19",
    "paper_authors": "Miseul Kim, Soo Jin Park, Kyungguen Byun, Hyeon-Kyeong Shin, Sunkuk Moon, Shuhua Zhang, Erik Visser",
    "topic": [
      "Speech Synthesis",
      "Speech Enhancement"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Enhancing Situational Awareness in Wearable Audio Devices Using a Lightweight Sound Event Localization and Detection System",
    "paper_title_zh": "使用轻量级声音事件定位与检测系统增强可穿戴音频设备的情境感知能力",
    "paper_id": "2509.14650",
    "paper_abstract": "Wearable audio devices with active noise control (ANC) enhance listening comfort but often at the expense of situational awareness. However, this auditory isolation may mask crucial environmental cues, posing significant safety risks. To address this, we propose an environmental intelligence framework that combines Acoustic Scene Classification (ASC) with Sound Event Localization and Detection (SELD). Our system first employs a lightweight ASC model to infer the current environment. The scene prediction then dynamically conditions a SELD network, tuning its sensitivity to detect and localize sounds that are most salient to the current context. On simulated headphone data, the proposed ASC-conditioned SELD system demonstrates improved spatial intelligence over a conventional baseline. This work represents a crucial step towards creating intelligent hearables that can deliver crucial environmental information, fostering a safer and more context-aware listening experience.",
    "paper_abstract_zh": "带有主动降噪（ANC）功能的可穿戴音频设备提升了聆听舒适度，但往往以牺牲情境感知为代价。然而，这种听觉隔离可能会掩盖关键的环境声学线索，构成重大的安全风险。为解决此问题，我们提出了一种环境智能框架，将声学场景分类（ASC）与声音事件定位与检测（SELD）相结合。我们的系统首先采用一个轻量级ASC模型来推断当前环境。随后，场景预测结果动态地调节一个SELD网络，调整其灵敏度以检测和定位在当前上下文中最为突出的声音。在模拟耳机数据上，所提出的ASC条件化SELD系统相较于传统基线方法展现出更强的空间智能。这项工作是朝着创建智能可听设备迈出的关键一步，此类设备能够传递关键的环境信息，从而营造更安全、更具情境感知能力的聆听体验。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2025-09-19",
    "paper_authors": "Jun-Wei Yeow, Ee-Leng Tan, Santi Peksi, Zhen-Ting Ong, Woon-Seng Gan",
    "topic": [
      "Audio Classification",
      "Speech Enhancement"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Aligning Audio Captions with Human Preferences",
    "paper_title_zh": "基于人类偏好的音频字幕对齐方法",
    "paper_id": "2509.14659",
    "paper_abstract": "Current audio captioning systems rely heavily on supervised learning with paired audio-caption datasets, which are expensive to curate and may not reflect human preferences in real-world scenarios. To address this limitation, we propose a preference-aligned audio captioning framework based on Reinforcement Learning from Human Feedback (RLHF). To effectively capture nuanced human preferences, we train a Contrastive Language-Audio Pretraining (CLAP)-based reward model using human-labeled pairwise preference data. This reward model is integrated into a reinforcement learning framework to fine-tune any baseline captioning system without relying on ground-truth caption annotations. Extensive human evaluations across multiple datasets show that our method produces captions preferred over those from baseline models, particularly in cases where the baseline models fail to provide correct and natural captions. Furthermore, our framework achieves performance comparable to supervised approaches with ground-truth data, demonstrating its effectiveness in aligning audio captioning with human preferences and its scalability in real-world scenarios.",
    "paper_abstract_zh": "当前的音频字幕系统主要依赖于配对音频-字幕数据集的监督学习，这些数据集制作成本高昂，且可能无法反映真实场景中的人类偏好。为解决这一局限性，我们提出了一种基于人类反馈强化学习（RLHF）的偏好对齐音频字幕框架。为有效捕捉细微的人类偏好，我们使用人工标注的成对偏好数据训练了一个基于对比语言-音频预训练（CLAP）的奖励模型。该奖励模型被集成到强化学习框架中，可在不依赖真实字幕标注的情况下微调任何基线字幕系统。在多个数据集上进行的大量人工评估表明，我们的方法生成的字幕比基线模型更受人类偏好，特别是在基线模型无法提供正确且自然字幕的情况下。此外，我们的框架在使用真实数据的情况下达到了与监督方法相当的性能，证明了其在将音频字幕与人类偏好对齐方面的有效性以及在真实场景中的可扩展性。",
    "subjects": [
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "update_time": "2025-09-19",
    "paper_authors": "Kartik Hegde, Rehana Mahfuz, Yinyi Guo, Erik Visser",
    "topic": [
      "Audio Representation Learning"
    ],
    "category": [
      "Other"
    ]
  },
  {
    "paper_title": "SpeechMLC: Speech Multi-label Classification",
    "paper_title_zh": "SpeechMLC：语音多标签分类",
    "paper_id": "2509.14677",
    "paper_abstract": "In this paper, we propose a multi-label classification framework to detect multiple speaking styles in a speech sample. Unlike previous studies that have primarily focused on identifying a single target style, our framework effectively captures various speaker characteristics within a unified structure, making it suitable for generalized human-computer interaction applications. The proposed framework integrates cross-attention mechanisms within a transformer decoder to extract salient features associated with each target label from the input speech. To mitigate the data imbalance inherent in multi-label speech datasets, we employ a data augmentation technique based on a speech generation model. We validate our model's effectiveness through multiple objective evaluations on seen and unseen corpora. In addition, we provide an analysis of the influence of human perception on classification accuracy by considering the impact of human labeling agreement on model performance.",
    "paper_abstract_zh": "本文提出了一种多标签分类框架，用于检测语音样本中的多种说话风格。与以往主要专注于识别单一目标风格的研究不同，我们的框架在一个统一结构中有效捕捉了各种说话人特征，使其适用于广义的人机交互应用。所提出的框架在Transformer解码器中集成了交叉注意力机制，以从输入语音中提取与每个目标标签相关的显著特征。为了缓解多标签语音数据集中固有的数据不平衡问题，我们采用了一种基于语音生成模型的数据增强技术。我们通过在已知和未知语料库上进行多项客观评估，验证了模型的有效性。此外，我们还通过考虑人类标注一致性对模型性能的影响，分析了人类感知对分类准确性的影响。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)",
      "Signal Processing (eess.SP)"
    ],
    "update_time": "2025-09-19",
    "paper_authors": "Miseul Kim, Seyun Um, Hyeonjin Cha, Hong-goo Kang",
    "topic": [
      "Audio Classification",
      "Speech Recognition"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "DAIEN-TTS: Disentangled Audio Infilling for Environment-Aware Text-to-Speech Synthesis",
    "paper_title_zh": "DAIEN-TTS：基于解耦音频填充的环境感知文本到语音合成",
    "paper_id": "2509.14684",
    "paper_abstract": "This paper presents DAIEN-TTS, a zero-shot text-to-speech (TTS) framework that enables ENvironment-aware synthesis through Disentangled Audio Infilling. By leveraging separate speaker and environment prompts, DAIEN-TTS allows independent control over the timbre and the background environment of the synthesized speech. Built upon F5-TTS, the proposed DAIEN-TTS first incorporates a pretrained speech-environment separation (SES) module to disentangle the environmental speech into mel-spectrograms of clean speech and environment audio. Two random span masks of varying lengths are then applied to both mel-spectrograms, which, together with the text embedding, serve as conditions for infilling the masked environmental mel-spectrogram, enabling the simultaneous continuation of personalized speech and time-varying environmental audio. To further enhance controllability during inference, we adopt dual class-free guidance (DCFG) for the speech and environment components and introduce a signal-to-noise ratio (SNR) adaptation strategy to align the synthesized speech with the environment prompt. Experimental results demonstrate that DAIEN-TTS generates environmental personalized speech with high naturalness, strong speaker similarity, and high environmental fidelity.",
    "paper_abstract_zh": "本文提出了DAIEN-TTS，一种零样本文本到语音（TTS）框架，通过解耦音频填充实现环境感知合成。该框架利用独立的说话者和环境提示，实现对合成语音音色和背景环境的独立控制。基于F5-TTS构建的DAIEN-TTS首先引入预训练的语音-环境分离（SES）模块，将环境语音解耦为纯净语音和环境音频的梅尔频谱图。随后对两个梅尔频谱图应用不同长度的随机跨度掩码，这些掩码与文本嵌入共同作为填充被掩码环境梅尔频谱图的条件，从而实现个性化语音和时变环境音频的同步延续。为进一步增强推理过程中的可控性，我们采用针对语音和环境组件的双重无分类器引导（DCFG）策略，并引入信噪比（SNR）自适应策略以使合成语音与环境提示对齐。实验结果表明，DAIEN-TTS能够生成具有高自然度、强说话人相似性和高环境保真度的环境个性化语音。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "update_time": "2025-09-19",
    "paper_authors": "Ye-Xin Lu, Yu Gu, Kun Wei, Hui-Peng Du, Yang Ai, Zhen-Hua Ling",
    "topic": [
      "Speech Synthesis",
      "Speech Enhancement"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "MELA-TTS: Joint transformer-diffusion model with representation alignment for speech synthesis",
    "paper_title_zh": "MELA-TTS：基于表示对齐的联合变换器-扩散模型用于语音合成",
    "paper_id": "2509.14784",
    "paper_abstract": "This work introduces MELA-TTS, a novel joint transformer-diffusion framework for end-to-end text-to-speech synthesis. By autoregressively generating continuous mel-spectrogram frames from linguistic and speaker conditions, our architecture eliminates the need for speech tokenization and multi-stage processing pipelines. To address the inherent difficulties of modeling continuous features, we propose a representation alignment module that aligns output representations of the transformer decoder with semantic embeddings from a pretrained ASR encoder during training. This mechanism not only speeds up training convergence, but also enhances cross-modal coherence between the textual and acoustic domains. Comprehensive experiments demonstrate that MELA-TTS achieves state-of-the-art performance across multiple evaluation metrics while maintaining robust zero-shot voice cloning capabilities, in both offline and streaming synthesis modes. Our results establish a new benchmark for continuous feature generation approaches in TTS, offering a compelling alternative to discrete-token-based paradigms.",
    "paper_abstract_zh": "本研究提出了MELA-TTS，一种新颖的联合变换器-扩散框架，用于端到端文本到语音合成。该架构通过自回归方式从语言和说话人条件生成连续的梅尔频谱图帧，从而消除了语音标记化和多阶段处理流程的需求。为解决连续特征建模的固有难题，我们提出了表示对齐模块，在训练过程中将变换器解码器的输出表示与预训练ASR编码器的语义嵌入进行对齐。该机制不仅加速了训练收敛，还增强了文本域与声学域之间的跨模态一致性。综合实验表明，MELA-TTS在离线和流式合成模式下均能实现多项评估指标的最先进性能，同时保持强大的零样本语音克隆能力。我们的研究成果为TTS领域的连续特征生成方法设立了新基准，为基于离散标记的范式提供了具有竞争力的替代方案。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2025-09-19",
    "paper_authors": "Keyu An, Zhiyu Zhang, Changfeng Gao, Yabin Li, Zhendong Peng, Haoxu Wang, Zhihao Du, Han Zhao, Zhifu Gao, Xiangang Li",
    "topic": [
      "Speech Synthesis",
      "Audio Representation Learning"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Acoustic Simulation Framework for Multi-channel Replay Speech Detection",
    "paper_title_zh": "用于多通道重放语音检测的声学仿真框架",
    "paper_id": "2509.14789",
    "paper_abstract": "Replay speech attacks pose a significant threat to voice-controlled systems, especially in smart environments where voice assistants are widely deployed. While multi-channel audio offers spatial cues that can enhance replay detection robustness, existing datasets and methods predominantly rely on single-channel recordings. In this work, we introduce an acoustic simulation framework designed to simulate multi-channel replay speech configurations using publicly available resources. Our setup models both genuine and spoofed speech across varied environments, including realistic microphone and loudspeaker impulse responses, room acoustics, and noise conditions. The framework employs measured loudspeaker directionalities during the replay attack to improve the realism of the simulation. We define two spoofing settings, which simulate whether a reverberant or an anechoic speech is used in the replay scenario, and evaluate the impact of omnidirectional and diffuse noise on detection performance. Using the state-of-the-art M-ALRAD model for replay speech detection, we demonstrate that synthetic data can support the generalization capabilities of the detector across unseen enclosures.",
    "paper_abstract_zh": "重放语音攻击对语音控制系统构成了重大威胁，尤其是在语音助手广泛部署的智能环境中。虽然多通道音频提供了可增强重放检测鲁棒性的空间线索，但现有的数据集和方法主要依赖于单通道录音。在这项工作中，我们引入了一个声学仿真框架，旨在利用公开可用的资源来模拟多通道重放语音配置。我们的设置模拟了不同环境下的真实语音和欺骗语音，包括真实的麦克风和扬声器脉冲响应、房间声学以及噪声条件。该框架在重放攻击期间采用测量的扬声器方向性以提高仿真的真实性。我们定义了两种欺骗设置，分别模拟在重放场景中使用混响语音还是消声室语音，并评估了全向噪声和扩散噪声对检测性能的影响。使用最先进的M-ALRAD模型进行重放语音检测，我们证明了合成数据可以支持检测器在未见过的封闭空间中的泛化能力。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)",
      "Cryptography and Security (cs.CR)",
      "Signal Processing (eess.SP)"
    ],
    "update_time": "2025-09-19",
    "paper_authors": "Michael Neri, Tuomas Virtanen",
    "topic": [
      "Speech Enhancement",
      "Audio Classification"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "AmbiDrop: Array-Agnostic Speech Enhancement Using Ambisonics Encoding and Dropout-Based Learning",
    "paper_title_zh": "AmbiDrop：基于Ambisonics编码和Dropout学习的阵列无关语音增强方法",
    "paper_id": "2509.14855",
    "paper_abstract": "Multichannel speech enhancement leverages spatial cues to improve intelligibility and quality, but most learning-based methods rely on specific microphone array geometry, unable to account for geometry changes. To mitigate this limitation, current array-agnostic approaches employ large multi-geometry datasets but may still fail to generalize to unseen layouts. We propose AmbiDrop (Ambisonics with Dropouts), an Ambisonics-based framework that encodes arbitrary array recordings into the spherical harmonics domain using Ambisonics Signal Matching (ASM). A deep neural network is trained on simulated Ambisonics data, combined with channel dropout for robustness against array-dependent encoding errors, therefore omitting the need for a diverse microphone array database. Experiments show that while the baseline and proposed models perform similarly on the training arrays, the baseline degrades on unseen arrays. In contrast, AmbiDrop consistently improves SI-SDR, PESQ, and STOI, demonstrating strong generalization and practical potential for array-agnostic speech enhancement.",
    "paper_abstract_zh": "多通道语音增强利用空间线索来提高清晰度和质量，但大多数基于学习的方法依赖于特定的麦克风阵列几何结构，无法适应几何结构的变化。为了缓解这一局限性，当前阵列无关方法采用大型多几何结构数据集，但仍可能无法泛化到未见过的布局。我们提出了AmbiDrop（基于Dropout的Ambisonics方法），这是一个基于Ambisonics的框架，使用Ambisonics信号匹配（ASM）将任意阵列录音编码到球谐波域。通过在模拟的Ambisonics数据上训练深度神经网络，并结合通道dropout以提高对阵列相关编码误差的鲁棒性，从而无需多样化的麦克风阵列数据库。实验表明，虽然基线模型和所提出模型在训练阵列上表现相似，但基线模型在未见阵列上性能下降。相比之下，AmbiDrop在SI-SDR、PESQ和STOI指标上持续提升，展示了强大的泛化能力和阵列无关语音增强的实际潜力。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2025-09-19",
    "paper_authors": "Michael Tatarjitzky, Boaz Rafaely",
    "topic": [
      "Speech Enhancement",
      "Audio Representation Learning"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Mitigating data replication in text-to-audio generative diffusion models through anti-memorization guidance",
    "paper_title_zh": "通过反记忆化引导缓解文本到音频生成扩散模型中的数据复制问题",
    "paper_id": "2509.14934",
    "paper_abstract": "A persistent challenge in generative audio models is data replication, where the model unintentionally generates parts of its training data during inference. In this work, we address this issue in text-to-audio diffusion models by exploring the use of anti-memorization strategies. We adopt Anti-Memorization Guidance (AMG), a technique that modifies the sampling process of pre-trained diffusion models to discourage memorization. Our study explores three types of guidance within AMG, each designed to reduce replication while preserving generation quality. We use Stable Audio Open as our backbone, leveraging its fully open-source architecture and training dataset. Our comprehensive experimental analysis suggests that AMG significantly mitigates memorization in diffusion-based text-to-audio generation without compromising audio fidelity or semantic alignment.",
    "paper_abstract_zh": "生成式音频模型面临的一个持续挑战是数据复制问题，即模型在推理过程中无意中生成了其训练数据的一部分。在这项工作中，我们通过探索反记忆化策略的使用来解决文本到音频扩散模型中的这一问题。我们采用了反记忆化引导（AMG）技术，该技术通过修改预训练扩散模型的采样过程来抑制记忆化行为。我们的研究探索了AMG中的三种引导类型，每种都旨在减少数据复制的同时保持生成质量。我们使用Stable Audio Open作为基础模型，利用其完全开源的架构和训练数据集。我们的综合实验分析表明，AMG在不影响音频保真度或语义对齐的情况下，显著减轻了基于扩散的文本到音频生成中的记忆化问题。",
    "subjects": [
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)",
      "Signal Processing (eess.SP)"
    ],
    "update_time": "2025-09-19",
    "paper_authors": "Francisco Messina, Francesca Ronchini, Luca Comanducci, Paolo Bestagini, Fabio Antonacci",
    "topic": [
      "Music Generation",
      "Audio Representation Learning"
    ],
    "category": [
      "Music"
    ]
  },
  {
    "paper_title": "SynParaSpeech: Automated Synthesis of Paralinguistic Datasets for Speech Generation and Understanding",
    "paper_title_zh": "SynParaSpeech：副语言数据集的自动化合成用于语音生成与理解",
    "paper_id": "2509.14946",
    "paper_abstract": "Paralinguistic sounds, like laughter and sighs, are crucial for synthesizing more realistic and engaging speech. However, existing methods typically depend on proprietary datasets, while publicly available resources often suffer from incomplete speech, inaccurate or missing timestamps, and limited real-world relevance. To address these problems, we propose an automated framework for generating large-scale paralinguistic data and apply it to construct the SynParaSpeech dataset. The dataset comprises 6 paralinguistic categories with 118.75 hours of data and precise timestamps, all derived from natural conversational speech. Our contributions lie in introducing the first automated method for constructing large-scale paralinguistic datasets and releasing the SynParaSpeech corpus, which advances speech generation through more natural paralinguistic synthesis and enhances speech understanding by improving paralinguistic event detection. The dataset and audio samples are available at this https URL.",
    "paper_abstract_zh": "副语言声音（如笑声和叹息）对于合成更真实、更具吸引力的语音至关重要。然而，现有方法通常依赖于专有数据集，而公开可用的资源往往存在语音不完整、时间戳不准确或缺失以及现实世界相关性有限的问题。为解决这些问题，我们提出了一个自动化框架用于生成大规模副语言数据，并应用该框架构建了SynParaSpeech数据集。该数据集包含6个副语言类别，共计118.75小时的数据和精确的时间戳，所有数据均源自自然对话语音。我们的贡献在于引入了首个自动化构建大规模副语言数据集的方法，并发布了SynParaSpeech语料库，该语料库通过更自然的副语言合成推进语音生成，并通过改进副语言事件检测增强语音理解。数据集和音频样本可在该https URL获取。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)",
      "Computation and Language (cs.CL)"
    ],
    "update_time": "2025-09-19",
    "paper_authors": "Bingsong Bai, Qihang Lu, Wenbing Yang, Zihan Sun, YueRan Hou, Peilei Jia, Songbai Pu, Ruibo Fu, Yingming Gao, Ya Li, Jun Gao",
    "topic": [
      "Speech Synthesis",
      "Audio Classification"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Discrete optimal transport is a strong audio adversarial attack",
    "paper_title_zh": "离散最优传输是一种强力的音频对抗攻击",
    "paper_id": "2509.14959",
    "paper_abstract": "In this paper, we show that discrete optimal transport (DOT) is an effective black-box adversarial attack against modern audio anti-spoofing countermeasures (CMs). Our attack operates as a post-processing, distribution-alignment step: frame-level WavLM embeddings of generated speech are aligned to an unpaired bona fide pool via entropic OT and a top-$k$ barycentric projection, then decoded with a neural vocoder. Evaluated on ASVspoof2019 and ASVspoof5 with AASIST baselines, DOT yields consistently high equal error rate (EER) across datasets and remains competitive after CM fine-tuning, outperforming several conventional attacks in cross-dataset transfer. Ablation analysis highlights the practical impact of vocoder overlap. Results indicate that distribution-level alignment is a powerful and stable attack surface for deployed CMs.",
    "paper_abstract_zh": "本文表明，离散最优传输（DOT）是一种针对现代音频反欺骗对抗措施（CMs）的有效黑盒对抗攻击。我们的攻击作为一种后处理的分布对齐步骤：通过熵最优传输和top-$k$重心投影，将生成语音的帧级WavLM嵌入与未配对的真实语音池进行对齐，然后使用神经声码器进行解码。在ASVspoof2019和ASVspoof5数据集上使用AASIST基线进行评估，DOT在不同数据集上均能产生较高的等错误率（EER），并且在CM微调后仍保持竞争力，在跨数据集迁移中优于多种传统攻击方法。消融分析突出了声码器重叠的实际影响。结果表明，分布级对齐是针对已部署CMs的一种强大且稳定的攻击面。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)",
      "Artificial Intelligence (cs.AI)"
    ],
    "update_time": "2025-09-19",
    "paper_authors": "Anton Selitskiy, Akib Shahriyar, Jishnuraj Prakasan",
    "topic": [
      "Speech Synthesis",
      "Audio Classification"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "BabyHuBERT: Multilingual Self-Supervised Learning for Segmenting Speakers in Child-Centered Long-Form Recordings",
    "paper_title_zh": "BabyHuBERT：基于多语言自监督学习的儿童中心化长时录音说话人分割方法",
    "paper_id": "2509.15001",
    "paper_abstract": "Child-centered long-form recordings are essential for studying early language development, but existing speech models trained on clean adult data perform poorly due to acoustic and linguistic differences. We introduce BabyHuBERT, the first self-supervised speech representation model trained on 13,000 hours of multilingual child-centered long-form recordings spanning over 40 languages. We evaluate BabyHuBERT on speaker segmentation, identifying when target children speak versus female adults, male adults, or other children -- a fundamental preprocessing step for analyzing naturalistic language experiences. BabyHuBERT achieves F1-scores from 52.1% to 74.4% across six diverse datasets, consistently outperforming W2V2-LL4300 (trained on English long-forms) and standard HuBERT (trained on clean adult speech). Notable improvements include 13.2 absolute F1 points over HuBERT on Vanuatu and 15.9 points on Solomon Islands corpora, demonstrating effectiveness on underrepresented languages. By sharing code and models, BabyHuBERT serves as a foundation model for child speech research, enabling fine-tuning on diverse downstream tasks.",
    "paper_abstract_zh": "儿童中心化长时录音对早期语言发展研究至关重要，但由于声学和语言特征的差异，基于纯净成人数据训练的现有语音模型表现不佳。我们提出了BabyHuBERT，这是首个基于超过40种语言、总计13,000小时的多语言儿童中心化长时录音训练的自监督语音表征模型。我们在说话人分割任务上评估BabyHuBERT，该任务需要识别目标儿童与女性成人、男性成人或其他儿童的说话时段——这是分析自然语言环境的基础预处理步骤。BabyHuBERT在六个多样化数据集上取得了52.1%至74.4%的F1分数，始终优于W2V2-LL4300（基于英语长时录音训练）和标准HuBERT（基于纯净成人语音训练）。显著改进包括在瓦努阿图语料库上相比HuBERT提升13.2个绝对F1值，在所罗门群岛语料库上提升15.9个F1值，证明了其在代表性不足语言上的有效性。通过开源代码和模型，BabyHuBERT可作为儿童语音研究的基础模型，支持多种下游任务的微调。",
    "subjects": [
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "update_time": "2025-09-19",
    "paper_authors": "Théo Charlot, Tarek Kunze, Maxime Poli, Alejandrina Cristia, Emmanuel Dupoux, Marvin Lavechin",
    "topic": [
      "Audio Representation Learning",
      "Audio Classification"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Transfer Learning for Paediatric Sleep Apnoea Detection Using Physiology-Guided Acoustic Models",
    "paper_title_zh": "基于生理引导声学模型的迁移学习在儿童睡眠呼吸暂停检测中的应用",
    "paper_id": "2509.15008",
    "paper_abstract": "Paediatric obstructive sleep apnoea (OSA) is clinically significant yet difficult to diagnose, as children poorly tolerate sensor-based polysomnography. Acoustic monitoring provides a non-invasive alternative for home-based OSA screening, but limited paediatric data hinders the development of robust deep learning approaches. This paper proposes a transfer learning framework that adapts acoustic models pretrained on adult sleep data to paediatric OSA detection, incorporating SpO2-based desaturation patterns to enhance model training. Using a large adult sleep dataset (157 nights) and a smaller paediatric dataset (15 nights), we systematically evaluate (i) single- versus multi-task learning, (ii) encoder freezing versus full fine-tuning, and (iii) the impact of delaying SpO2 labels to better align them with the acoustics and capture physiologically meaningful features. Results show that fine-tuning with SpO2 integration consistently improves paediatric OSA detection compared with baseline models without adaptation. These findings demonstrate the feasibility of transfer learning for home-based OSA screening in children and offer its potential clinical value for early diagnosis.",
    "paper_abstract_zh": "儿童阻塞性睡眠呼吸暂停（OSA）具有临床重要性但难以诊断，因为儿童对基于传感器的多导睡眠监测耐受性差。声学监测为家庭OSA筛查提供了一种非侵入性替代方案，但有限的儿科数据阻碍了鲁棒深度学习方法的开发。本文提出了一种迁移学习框架，将基于成人睡眠数据预训练的声学模型适配于儿童OSA检测，并整合基于血氧饱和度（SpO2）的去饱和模式以增强模型训练。使用大型成人睡眠数据集（157晚）和较小的儿科数据集（15晚），我们系统评估了（i）单任务与多任务学习，（ii）编码器冻结与完全微调，以及（iii）延迟SpO2标签以更好地与声学信号对齐并捕获具有生理意义特征的影响。结果表明，与未经适配的基线模型相比，集成SpO2的微调方法持续改善了儿童OSA检测性能。这些发现证明了迁移学习在儿童家庭OSA筛查中的可行性，并展现了其在早期诊断中的潜在临床价值。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2025-09-19",
    "paper_authors": "Chaoyue Niu, Veronica Rowe, Guy J. Brown, Heather Elphick, Heather Kenyon, Lowri Thomas, Sam Johnson, Ning Ma",
    "topic": [
      "Audio Representation Learning",
      "Audio Classification"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "From Who Said What to Who They Are: Modular Training-free Identity-Aware LLM Refinement of Speaker Diarization",
    "paper_title_zh": "从“谁说了什么”到“他们是谁”：基于模块化无训练身份感知大语言模型的说话人日志优化",
    "paper_id": "2509.15082",
    "paper_abstract": "Speaker diarization (SD) struggles in real-world scenarios due to dynamic environments and unknown speaker counts. SD is rarely used alone and is often paired with automatic speech recognition (ASR), but non-modular methods that jointly train on domain-specific data have limited flexibility. Moreover, many applications require true speaker identities rather than SD's pseudo labels. We propose a training-free modular pipeline combining off-the-shelf SD, ASR, and a large language model (LLM) to determine who spoke, what was said, and who they are. Using structured LLM prompting on reconciled SD and ASR outputs, our method leverages semantic continuity in conversational context to refine low-confidence speaker labels and assigns role identities while correcting split speakers. On a real-world patient-clinician dataset, our approach achieves a 29.7% relative error reduction over baseline reconciled SD and ASR. It enhances diarization performance without additional training and delivers a complete pipeline for SD, ASR, and speaker identity detection in practical applications.",
    "paper_abstract_zh": "说话人日志（SD）在现实场景中因动态环境和未知说话人数量而面临挑战。SD很少单独使用，通常与自动语音识别（ASR）结合，但基于领域特定数据联合训练的非模块化方法灵活性有限。此外，许多应用需要真实的说话人身份而非SD的伪标签。我们提出一种无需训练的模块化流程，结合现成的SD、ASR和大语言模型（LLM），以确定谁在说话、说了什么以及他们的身份。通过对对齐的SD和ASR输出进行结构化LLM提示，该方法利用对话上下文中的语义连续性来优化低置信度说话人标签，分配角色身份，同时纠正分裂的说话人。在真实医患数据集上，我们的方法相比基线对齐SD和ASR实现了29.7%的相对错误率降低。它无需额外训练即可提升日志性能，并为实际应用中的SD、ASR和说话人身份检测提供完整流程。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2025-09-19",
    "paper_authors": "Yu-Wen Chen, William Ho, Maxim Topaz, Julia Hirschberg, Zoran Kostic",
    "topic": [
      "Speech Recognition"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Real-Time Streaming Mel Vocoding with Generative Flow Matching",
    "paper_title_zh": "基于生成流匹配的实时流式梅尔声码合成",
    "paper_id": "2509.15085",
    "paper_abstract": "The task of Mel vocoding, i.e., the inversion of a Mel magnitude spectrogram to an audio waveform, is still a key component in many text-to-speech (TTS) systems today. Based on generative flow matching, our prior work on generative STFT phase retrieval (DiffPhase), and the pseudoinverse operator of the Mel filterbank, we develop MelFlow, a streaming-capable generative Mel vocoder for speech sampled at 16 kHz with an algorithmic latency of only 32 ms and a total latency of 48 ms. We show real-time streaming capability at this latency not only in theory, but in practice on a consumer laptop GPU. Furthermore, we show that our model achieves substantially better PESQ and SI-SDR values compared to well-established not streaming-capable baselines for Mel vocoding including HiFi-GAN.",
    "paper_abstract_zh": "梅尔声码合成任务，即将梅尔幅度谱图反转为音频波形，至今仍是许多文本转语音（TTS）系统的关键组成部分。基于生成流匹配、我们先前在生成STFT相位恢复（DiffPhase）方面的工作以及梅尔滤波器组的伪逆算子，我们开发了MelFlow——一种支持流式处理的生成式梅尔声码器，适用于16 kHz采样的语音，其算法延迟仅为32毫秒，总延迟为48毫秒。我们不仅在理论上展示了在此延迟下的实时流式处理能力，而且在消费级笔记本电脑GPU上进行了实际验证。此外，与包括HiFi-GAN在内的成熟非流式梅尔声码合成基线相比，我们的模型在PESQ和SI-SDR指标上取得了显著更好的结果。",
    "subjects": [
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)",
      "Signal Processing (eess.SP)"
    ],
    "update_time": "2025-09-19",
    "paper_authors": "Simon Welker, Tal Peer, Timo Gerkmann",
    "topic": [
      "Speech Synthesis",
      "Audio Codec"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Listening, Imagining \\& Refining: A Heuristic Optimized ASR Correction Framework with LLMs",
    "paper_title_zh": "聆听、想象与优化：基于启发式优化和LLMs的ASR纠错框架",
    "paper_id": "2509.15095",
    "paper_abstract": "Automatic Speech Recognition (ASR) systems remain prone to errors that affect downstream applications. In this paper, we propose LIR-ASR, a heuristic optimized iterative correction framework using LLMs, inspired by human auditory perception. LIR-ASR applies a \"Listening-Imagining-Refining\" strategy, generating phonetic variants and refining them in context. A heuristic optimization with finite state machine (FSM) is introduced to prevent the correction process from being trapped in local optima and rule-based constraints help maintain semantic fidelity. Experiments on both English and Chinese ASR outputs show that LIR-ASR achieves average reductions in CER/WER of up to 1.5 percentage points compared to baselines, demonstrating substantial accuracy gains in transcription.",
    "paper_abstract_zh": "自动语音识别（ASR）系统仍容易产生错误，影响下游应用。本文受人类听觉感知启发，提出LIR-ASR——一个基于大型语言模型（LLMs）的启发式优化迭代纠错框架。LIR-ASR采用“聆听-想象-优化”策略，生成语音变体并在上下文中进行优化。通过引入有限状态机（FSM）的启发式优化方法，避免纠错过程陷入局部最优解，同时基于规则的约束有助于保持语义保真度。在英语和中文ASR输出上的实验表明，与基线相比，LIR-ASR平均字错误率（CER/WER）降低高达1.5个百分点，显著提升了转录准确性。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)",
      "Artificial Intelligence (cs.AI)"
    ],
    "update_time": "2025-09-19",
    "paper_authors": "Yutong Liu, Ziyue Zhang, Yongbin Yu, Xiangxiang Wang, Yuqing Cai, Nyima Tashi",
    "topic": [
      "Speech Recognition",
      "Speech Enhancement"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Context-Enhanced Granular Edit Representation for Efficient and Accurate ASR Post-editing",
    "paper_title_zh": "面向高效准确ASR后编辑的上下文增强细粒度编辑表示",
    "paper_id": "2509.14263",
    "paper_abstract": "Despite ASR technology being full-scale adopted by industry and for large portions of the population, ASR systems often have errors that require editors to post-edit text quality. While LLMs are powerful post-editing tools, baseline full rewrite models have inference inefficiencies because they often generate the same redundant text over and over again. Compact edit representations have existed but often lack the efficacy and context required for optimal accuracy. This paper introduces CEGER (Context-Enhanced Granular Edit Representation), a compact edit representation that was generated for highly accurate, efficient ASR post-editing. CEGER allows LLMs to generate a sequence of structured, fine-grained, contextually rich commands to modify the original ASR output. A separate expansion module deterministically reconstructs the corrected text based on the commands. Extensive experiments on the LibriSpeech dataset that were conducted, CEGER achieves state-of-the-art accuracy, achieving the lowest word error rate (WER) versus full rewrite and prior compact representations.",
    "paper_abstract_zh": "尽管ASR技术已被工业界和广大用户全面采用，但ASR系统仍经常出现错误，需要编辑人员对文本质量进行后编辑。虽然大型语言模型（LLMs）是强大的后编辑工具，但基线全重写模型存在推理效率低下的问题，因为它们经常重复生成相同的冗余文本。紧凑的编辑表示虽已存在，但往往缺乏实现最佳准确性所需的效能和上下文信息。本文介绍了CEGER（上下文增强细粒度编辑表示），这是一种为高精度、高效率的ASR后编辑而生成的紧凑编辑表示。CEGER允许LLMs生成一系列结构化、细粒度、上下文丰富的命令来修改原始ASR输出。一个独立的扩展模块根据这些命令确定性地重建校正后的文本。在LibriSpeech数据集上进行的大量实验表明，CEGER实现了最先进的准确性，与全重写模型和先前的紧凑表示相比，达到了最低的词错误率（WER）。",
    "subjects": [
      "Computation and Language (cs.CL)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2025-09-19",
    "paper_authors": "Luan Vejsiu, Qianyu Zheng, Haoxuan Chen, Yizhou Han",
    "topic": [
      "Speech Recognition"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "SpeechWeave: Diverse Multilingual Synthetic Text & Audio Data Generation Pipeline for Training Text to Speech Models",
    "paper_title_zh": "SpeechWeave：用于训练文本转语音模型的多语言多样化合成文本与音频数据生成流水线",
    "paper_id": "2509.14270",
    "paper_abstract": "High-quality Text-to-Speech (TTS) model training requires extensive and diverse text and speech data. It is challenging to procure such data from real sources due to issues of domain specificity, licensing, and scalability. Large language models (LLMs) can certainly generate textual data, but they create repetitive text with insufficient variation in the prompt during the generation process. Another important aspect in TTS training data is text normalization. Tools for normalization might occasionally introduce anomalies or overlook valuable patterns, and thus impact data quality. Furthermore, it is also impractical to rely on voice artists for large scale speech recording in commercial TTS systems with standardized voices. To address these challenges, we propose SpeechWeave, a synthetic speech data generation pipeline that is capable of automating the generation of multilingual, domain-specific datasets for training TTS models. Our experiments reveal that our pipeline generates data that is 10-48% more diverse than the baseline across various linguistic and phonetic metrics, along with speaker-standardized speech audio while generating approximately 97% correctly normalized text. Our approach enables scalable, high-quality data generation for TTS training, improving diversity, normalization, and voice consistency in the generated datasets.",
    "paper_abstract_zh": "高质量的文本转语音（TTS）模型训练需要大量多样化的文本和语音数据。由于领域特异性、许可问题和可扩展性等因素，从真实来源获取此类数据具有挑战性。大型语言模型（LLMs）固然可以生成文本数据，但在生成过程中往往产生重复性文本且提示变化不足。TTS训练数据的另一个重要方面是文本规范化。规范化工具偶尔会引入异常或忽略有价值的模式，从而影响数据质量。此外，在商业TTS系统中依赖配音艺术家进行大规模标准化语音录制也不切实际。为解决这些挑战，我们提出了SpeechWeave——一种能够自动生成多语言、领域特异性数据集以训练TTS模型的合成语音数据生成流水线。实验表明，我们的流水线生成的数据在各种语言学和语音学指标上比基线数据多样性提高10-48%，同时生成约97%正确规范化的文本，并提供说话人标准化的语音音频。该方法实现了可扩展的高质量TTS训练数据生成，提升了数据集的多样性、规范化程度和语音一致性。",
    "subjects": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Multimedia (cs.MM)"
    ],
    "update_time": "2025-09-19",
    "paper_authors": "Karan Dua, Puneet Mittal, Ranjeet Gupta, Hitesh Laxmichand Patel",
    "topic": [
      "Speech Synthesis",
      "Audio Representation Learning"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Deploying UDM Series in Real-Life Stuttered Speech Applications: A Clinical Evaluation Framework",
    "paper_title_zh": "UDM系列在现实生活口吃语音应用中的部署：临床评估框架",
    "paper_id": "2509.14304",
    "paper_abstract": "Stuttered and dysfluent speech detection systems have traditionally suffered from the trade-off between accuracy and clinical interpretability. While end-to-end deep learning models achieve high performance, their black-box nature limits clinical adoption. This paper looks at the Unconstrained Dysfluency Modeling (UDM) series-the current state-of-the-art framework developed by Berkeley that combines modular architecture, explicit phoneme alignment, and interpretable outputs for real-world clinical deployment. Through extensive experiments involving patients and certified speech-language pathologists (SLPs), we demonstrate that UDM achieves state-of-the-art performance (F1: 0.89+-0.04) while providing clinically meaningful interpretability scores (4.2/5.0). Our deployment study shows 87% clinician acceptance rate and 34% reduction in diagnostic time. The results provide strong evidence that UDM represents a practical pathway toward AI-assisted speech therapy in clinical environments.",
    "paper_abstract_zh": "口吃和非流利语音检测系统传统上一直面临准确性与临床可解释性之间的权衡问题。虽然端到端深度学习模型实现了高性能，但其黑盒特性限制了临床采用。本文研究了无约束非流利性建模（UDM）系列——这是伯克利大学开发的最先进框架，结合了模块化架构、显式音素对齐和可解释输出，适用于现实世界的临床部署。通过涉及患者和认证言语病理学家（SLPs）的广泛实验，我们证明UDM实现了最先进的性能（F1分数：0.89±0.04），同时提供了具有临床意义的可解释性评分（4.2/5.0）。我们的部署研究显示临床医生接受率达到87%，诊断时间减少34%。这些结果强有力地证明UDM代表了在临床环境中实现AI辅助语音治疗的实际途径。",
    "subjects": [
      "Artificial Intelligence (cs.AI)",
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "update_time": "2025-09-19",
    "paper_authors": "Eric Zhang, Li Wei, Sarah Chen, Michael Wang",
    "topic": [
      "Speech Recognition",
      "Audio Classification"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "From Turn-Taking to Synchronous Dialogue: A Survey of Full-Duplex Spoken Language Models",
    "paper_title_zh": "从轮流对话到同步对话：全双工口语语言模型综述",
    "paper_id": "2509.14515",
    "paper_abstract": "True Full-Duplex (TFD) voice communication--enabling simultaneous listening and speaking with natural turn-taking, overlapping speech, and interruptions--represents a critical milestone toward human-like AI interaction. This survey comprehensively reviews Full-Duplex Spoken Language Models (FD-SLMs) in the LLM era. We establish a taxonomy distinguishing Engineered Synchronization (modular architectures) from Learned Synchronization (end-to-end architectures), and unify fragmented evaluation approaches into a framework encompassing Temporal Dynamics, Behavioral Arbitration, Semantic Coherence, and Acoustic Performance. Through comparative analysis of mainstream FD-SLMs, we identify fundamental challenges: synchronous data scarcity, architectural divergence, and evaluation gaps, providing a roadmap for advancing human-AI communication.",
    "paper_abstract_zh": "真正的全双工（TFD）语音通信——能够实现同时听和说，具有自然的轮流转换、重叠语音和打断功能——代表了实现类人人工智能交互的关键里程碑。本综述全面回顾了大语言模型（LLM）时代的全双工口语语言模型（FD-SLMs）。我们建立了一个分类法，区分了工程化同步（模块化架构）与学习式同步（端到端架构），并将零散的评估方法统一为一个涵盖时序动态、行为仲裁、语义连贯性和声学性能的框架。通过对主流FD-SLMs的比较分析，我们指出了其面临的根本挑战：同步数据稀缺、架构分歧和评估差距，并为推进人机通信提供了路线图。",
    "subjects": [
      "Computation and Language (cs.CL)",
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "update_time": "2025-09-19",
    "paper_authors": "Yuxuan Chen, Haoyuan Yu",
    "topic": [
      "Speech Recognition",
      "Speech Synthesis"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "How Does Instrumental Music Help SingFake Detection?",
    "paper_title_zh": "器乐如何帮助虚假歌声检测？",
    "paper_id": "2509.14675",
    "paper_abstract": "Although many models exist to detect singing voice deepfakes (SingFake), how these models operate, particularly with instrumental accompaniment, is unclear. We investigate how instrumental music affects SingFake detection from two perspectives. To investigate the behavioral effect, we test different backbones, unpaired instrumental tracks, and frequency subbands. To analyze the representational effect, we probe how fine-tuning alters encoders' speech and music capabilities. Our results show that instrumental accompaniment acts mainly as data augmentation rather than providing intrinsic cues (e.g., rhythm or harmony). Furthermore, fine-tuning increases reliance on shallow speaker features while reducing sensitivity to content, paralinguistic, and semantic information. These insights clarify how models exploit vocal versus instrumental cues and can inform the design of more interpretable and robust SingFake detection systems.",
    "paper_abstract_zh": "尽管已有许多模型用于检测歌声深度伪造（SingFake），但这些模型的具体运作机制，尤其是在存在器乐伴奏的情况下，仍不明确。我们从两个角度研究了器乐对虚假歌声检测的影响。为探究行为效应，我们测试了不同的模型主干网络、非配对的器乐音轨以及频率子带。为分析表征效应，我们探究了微调如何改变编码器的语音和音乐处理能力。实验结果表明，器乐伴奏主要起到数据增强的作用，而非提供内在线索（如节奏或和声）。此外，微调会增强模型对浅层说话人特征的依赖，同时降低对内容、副语言信息和语义信息的敏感性。这些发现阐明了模型如何利用人声与器乐线索，可为设计更具可解释性和鲁棒性的虚假歌声检测系统提供参考。",
    "subjects": [
      "Signal Processing (eess.SP)",
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "update_time": "2025-09-19",
    "paper_authors": "Xuanjun Chen, Chia-Yu Hu, I-Ming Lin, Yi-Cheng Lin, I-Hsiang Chiu, You Zhang, Sung-Feng Huang, Yi-Hsuan Yang, Haibin Wu, Hung-yi Lee, Jyh-Shing Roger Jang",
    "topic": [
      "Audio Classification",
      "Audio Representation Learning"
    ],
    "category": [
      "Music"
    ]
  },
  {
    "paper_title": "Temporally Heterogeneous Graph Contrastive Learning for Multimodal Acoustic event Classification",
    "paper_title_zh": "基于时序异构图对比学习的多模态声学事件分类",
    "paper_id": "2509.14893",
    "paper_abstract": "Multimodal acoustic event classification plays a key role in audio-visual systems. Although combining audio and visual signals improves recognition, it is still difficult to align them over time and to reduce the effect of noise across modalities. Existing methods often treat audio and visual streams separately, fusing features later with contrastive or mutual information objectives. Recent advances explore multimodal graph learning, but most fail to distinguish between intra- and inter-modal temporal dependencies. To address this, we propose Temporally Heterogeneous Graph-based Contrastive Learning (THGCL). Our framework constructs a temporal graph for each event, where audio and video segments form nodes and their temporal links form edges. We introduce Gaussian processes for intra-modal smoothness, Hawkes processes for inter-modal decay, and contrastive learning to capture fine-grained relationships. Experiments on AudioSet show that THGCL achieves state-of-the-art performance.",
    "paper_abstract_zh": "多模态声学事件分类在视听系统中扮演着关键角色。尽管结合音频和视觉信号能够提升识别性能，但实现它们在时间维度上的对齐并减少跨模态噪声的影响仍然具有挑战性。现有方法通常分别处理音频和视觉流，后期通过对比学习或互信息目标融合特征。近期研究探索了多模态图学习，但大多未能区分模态内和模态间的时间依赖性。为解决这一问题，我们提出了基于时序异构图对比学习（THGCL）的方法。我们的框架为每个事件构建时序图，其中音频和视频片段作为节点，它们的时间链接作为边。我们引入高斯过程处理模态内平滑性，霍克斯过程处理模态间衰减，并采用对比学习捕捉细粒度关系。在AudioSet上的实验表明，THGCL实现了最先进的性能。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "update_time": "2025-09-19",
    "paper_authors": "Yuanjian Chen, Yang Xiao, Jinjie Huang",
    "topic": [
      "Audio Classification",
      "Audio Representation Learning"
    ],
    "category": [
      "Other"
    ]
  },
  {
    "paper_title": "Estimating Respiratory Effort from Nocturnal Breathing Sounds for Obstructive Sleep Apnoea Screening",
    "paper_title_zh": "基于夜间呼吸音的呼吸努力度估计用于阻塞性睡眠呼吸暂停筛查",
    "paper_id": "2509.14944",
    "paper_abstract": "Obstructive sleep apnoea (OSA) is a prevalent condition with significant health consequences, yet many patients remain undiagnosed due to the complexity and cost of over-night polysomnography. Acoustic-based screening provides a scalable alternative, yet performance is limited by environmental noise and the lack of physiological context. Respiratory effort is a key signal used in clinical scoring of OSA events, but current approaches require additional contact sensors that reduce scalability and patient comfort. This paper presents the first study to estimate respiratory effort directly from nocturnal audio, enabling physiological context to be recovered from sound alone. We propose a latent-space fusion framework that integrates the estimated effort embeddings with acoustic features for OSA detection. Using a dataset of 157 nights from 103 participants recorded in home environments, our respiratory effort estimator achieves a concordance correlation coefficient of 0.48, capturing meaningful respiratory dynamics. Fusing effort and audio improves sensitivity and AUC over audio-only baselines, especially at low apnoea-hypopnoea index thresholds. The proposed approach requires only smartphone audio at test time, which enables sensor-free, scalable, and longitudinal OSA monitoring.",
    "paper_abstract_zh": "阻塞性睡眠呼吸暂停（OSA）是一种普遍存在且具有重大健康影响的疾病，但由于夜间多导睡眠监测的复杂性和成本，许多患者仍未得到诊断。基于声学的筛查提供了一种可扩展的替代方案，但其性能受到环境噪声和缺乏生理背景的限制。呼吸努力度是OSA事件临床评分中使用的关键信号，但当前方法需要额外的接触式传感器，降低了可扩展性和患者舒适度。本文提出了首个直接从夜间音频估计呼吸努力度的研究，使得仅从声音中恢复生理背景成为可能。我们提出了一个潜在空间融合框架，将估计的努力度嵌入与声学特征相结合用于OSA检测。使用在家庭环境中记录的103名参与者的157晚数据集，我们的呼吸努力度估计器达到了0.48的一致性相关系数，捕捉到了有意义的呼吸动态。融合努力度和音频相比仅使用音频的基线提高了敏感性和AUC，特别是在低呼吸暂停低通气指数阈值时。所提出的方法在测试时仅需要智能手机音频，实现了无需传感器、可扩展和纵向的OSA监测。",
    "subjects": [
      "Artificial Intelligence (cs.AI)",
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "update_time": "2025-09-19",
    "paper_authors": "Xiaolei Xu, Chaoyue Niu, Guy J. Brown, Hector Romero, Ning Ma",
    "topic": [
      "Audio Classification",
      "Audio Representation Learning"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Two Web Toolkits for Multimodal Piano Performance Dataset Acquisition and Fingering Annotation",
    "paper_title_zh": "用于多模态钢琴演奏数据集采集与指法标注的两种网页工具包",
    "paper_id": "2509.15222",
    "paper_abstract": "Piano performance is a multimodal activity that intrinsically combines physical actions with the acoustic rendition. Despite growing research interest in analyzing the multimodal nature of piano performance, the laborious process of acquiring large-scale multimodal data remains a significant bottleneck, hindering further progress in this field. To overcome this barrier, we present an integrated web toolkit comprising two graphical user interfaces (GUIs): (i) PiaRec, which supports the synchronized acquisition of audio, video, MIDI, and performance metadata. (ii) ASDF, which enables the efficient annotation of performer fingering from the visual data. Collectively, this system can streamline the acquisition of multimodal piano performance datasets.",
    "paper_abstract_zh": "钢琴演奏是一种多模态活动，本质上是将物理动作与声音呈现相结合。尽管分析钢琴演奏多模态性质的研究兴趣日益增长，但获取大规模多模态数据的繁琐过程仍然是一个重大瓶颈，阻碍了该领域的进一步发展。为克服这一障碍，我们提出了一个集成的网页工具包，包含两个图形用户界面（GUI）：(i) PiaRec，支持同步采集音频、视频、MIDI和演奏元数据；(ii) ASDF，能够从视觉数据中高效标注演奏者的指法。总体而言，该系统可以简化多模态钢琴演奏数据集的采集过程。",
    "subjects": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)",
      "Multimedia (cs.MM)",
      "Image and Video Processing (eess.IV)"
    ],
    "update_time": "2025-09-19",
    "paper_authors": "Junhyung Park, Yonghyun Kim, Joonhyung Bae, Kirak Kim, Taegyun Kwon, Alexander Lerch, Juhan Nam",
    "topic": [
      "Music Information Retrieval",
      "Other"
    ],
    "category": [
      "Music"
    ]
  },
  {
    "paper_title": "Measuring Soft Biometric Leakage in Speaker De-Identification Systems",
    "paper_title_zh": "测量说话人身份去标识化系统中的软生物特征泄露",
    "paper_id": "2509.14469",
    "paper_abstract": "We use the term re-identification to refer to the process of recovering the original speaker's identity from anonymized speech outputs. Speaker de-identification systems aim to reduce the risk of re-identification, but most evaluations focus only on individual-level measures and overlook broader risks from soft biometric leakage. We introduce the Soft Biometric Leakage Score (SBLS), a unified method that quantifies resistance to zero-shot inference attacks on non-unique traits such as channel type, age range, dialect, sex of the speaker, or speaking style. SBLS integrates three elements: direct attribute inference using pre-trained classifiers, linkage detection via mutual information analysis, and subgroup robustness across intersecting attributes. Applying SBLS with publicly available classifiers, we show that all five evaluated de-identification systems exhibit significant vulnerabilities. Our results indicate that adversaries using only pre-trained models - without access to original speech or system details - can still reliably recover soft biometric information from anonymized output, exposing fundamental weaknesses that standard distributional metrics fail to capture.",
    "paper_abstract_zh": "我们使用术语‘重新识别’来指代从匿名化语音输出中恢复原始说话人身份的过程。说话人身份去标识化系统旨在降低重新识别的风险，但大多数评估仅关注个体层面的度量，而忽视了软生物特征泄露带来的更广泛风险。我们引入了软生物特征泄露评分（SBLS），这是一种统一的方法，用于量化对非唯一特征（如信道类型、年龄范围、方言、说话人性别或说话风格）进行零样本推理攻击的抵抗能力。SBLS整合了三个要素：使用预训练分类器进行直接属性推断、通过互信息分析进行链接检测，以及跨交叉属性的子群体鲁棒性。通过应用SBLS和公开可用的分类器，我们发现所有五个被评估的去标识化系统都存在显著漏洞。我们的结果表明，即使攻击者仅使用预训练模型——无需访问原始语音或系统细节——仍然能够可靠地从匿名化输出中恢复软生物特征信息，这暴露了标准分布度量无法捕捉到的根本性弱点。",
    "subjects": [
      "Sound (cs.SD)"
    ],
    "update_time": "2025-09-19",
    "paper_authors": "Seungmin Seo, Oleg Aulov, P. Jonathon Phillips",
    "topic": [
      "Audio Classification",
      "Speech Enhancement"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "A long-form single-speaker real-time MRI speech dataset and benchmark",
    "paper_title_zh": "一个长格式单说话人实时磁共振成像语音数据集及基准",
    "paper_id": "2509.14479",
    "paper_abstract": "We release the USC Long Single-Speaker (LSS) dataset containing real-time MRI video of the vocal tract dynamics and simultaneous audio obtained during speech production. This unique dataset contains roughly one hour of video and audio data from a single native speaker of American English, making it one of the longer publicly available single-speaker datasets of real-time MRI speech data. Along with the articulatory and acoustic raw data, we release derived representations of the data that are suitable for a range of downstream tasks. This includes video cropped to the vocal tract region, sentence-level splits of the data, restored and denoised audio, and regions-of-interest timeseries. We also benchmark this dataset on articulatory synthesis and phoneme recognition tasks, providing baseline performance for these tasks on this dataset which future research can aim to improve upon.",
    "paper_abstract_zh": "我们发布了南加州大学长单说话人（LSS）数据集，包含语音产生过程中声道动力学的实时磁共振成像视频及同步音频。这一独特数据集包含来自一位美式英语母语说话者约一小时的视频和音频数据，使其成为公开可用的较长的单说话人实时磁共振成像语音数据集之一。除了发音器官和声学原始数据，我们还发布了适用于多种下游任务的数据衍生表示，包括裁剪至声道区域的视频、句子级别的数据分割、修复和去噪的音频以及感兴趣区域时间序列。我们还在发音合成和音素识别任务上对该数据集进行了基准测试，为未来研究提供了可改进的基线性能。",
    "subjects": [
      "Sound (cs.SD)"
    ],
    "update_time": "2025-09-19",
    "paper_authors": "Sean Foley, Jihwan Lee, Kevin Huang, Xuan Shi, Yoonjeong Lee, Louis Goldstein, Shrikanth Narayanan",
    "topic": [
      "Speech Synthesis",
      "Speech Recognition"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Cross-Lingual F5-TTS: Towards Language-Agnostic Voice Cloning and Speech Synthesis",
    "paper_title_zh": "跨语言F5-TTS：迈向语言无关的语音克隆与语音合成",
    "paper_id": "2509.14579",
    "paper_abstract": "Flow-matching-based text-to-speech (TTS) models have shown high-quality speech synthesis. However, most current flow-matching-based TTS models still rely on reference transcripts corresponding to the audio prompt for synthesis. This dependency prevents cross-lingual voice cloning when audio prompt transcripts are unavailable, particularly for unseen languages. The key challenges for flow-matching-based TTS models to remove audio prompt transcripts are identifying word boundaries during training and determining appropriate duration during inference. In this paper, we introduce Cross-Lingual F5-TTS, a framework that enables cross-lingual voice cloning without audio prompt transcripts. Our method preprocesses audio prompts by forced alignment to obtain word boundaries, enabling direct synthesis from audio prompts while excluding transcripts during training. To address the duration modeling challenge, we train speaking rate predictors at different linguistic granularities to derive duration from speaker pace. Experiments show that our approach matches the performance of F5-TTS while enabling cross-lingual voice cloning.",
    "paper_abstract_zh": "基于流匹配的文本转语音（TTS）模型已展现出高质量的语音合成能力。然而，当前大多数基于流匹配的TTS模型仍依赖与音频提示对应的参考文本进行合成。这种依赖性导致在音频提示文本不可用时（尤其是对于未见过的语言）无法实现跨语言语音克隆。基于流匹配的TTS模型去除音频提示文本的关键挑战在于训练时的词边界识别和推理时的时长确定。本文提出跨语言F5-TTS框架，该框架能够在无需音频提示文本的情况下实现跨语言语音克隆。我们的方法通过强制对齐对音频提示进行预处理以获取词边界，从而在训练时排除文本，实现直接从音频提示进行合成。为解决时长建模难题，我们训练了不同语言粒度的语速预测器，以从说话人节奏中推导时长。实验表明，我们的方法在实现跨语言语音克隆的同时，性能与F5-TTS相当。",
    "subjects": [
      "Sound (cs.SD)"
    ],
    "update_time": "2025-09-19",
    "paper_authors": "Qingyu Liu, Yushen Chen, Zhikang Niu, Chunhui Wang, Yunting Yang, Bowen Zhang, Jian Zhao, Pengcheng Zhu, Kai Yu, Xie Chen",
    "topic": [
      "Speech Synthesis"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Spatial Audio Motion Understanding and Reasoning",
    "paper_title_zh": "空间音频运动理解与推理",
    "paper_id": "2509.14666",
    "paper_abstract": "Spatial audio reasoning enables machines to interpret auditory scenes by understanding events and their spatial attributes. In this work, we focus on spatial audio understanding with an emphasis on reasoning about moving sources. First, we introduce a spatial audio encoder that processes spatial audio to detect multiple overlapping events and estimate their spatial attributes, Direction of Arrival (DoA) and source distance, at the frame level. To generalize to unseen events, we incorporate an audio grounding model that aligns audio features with semantic audio class text embeddings via a cross-attention mechanism. Second, to answer complex queries about dynamic audio scenes involving moving sources, we condition a large language model (LLM) on structured spatial attributes extracted by our model. Finally, we introduce a spatial audio motion understanding and reasoning benchmark dataset and demonstrate our framework's performance against the baseline model.",
    "paper_abstract_zh": "空间音频推理使机器能够通过理解事件及其空间属性来解析听觉场景。在本研究中，我们专注于空间音频理解，特别是对移动声源的推理。首先，我们引入了一个空间音频编码器，该编码器处理空间音频以检测多个重叠事件，并在帧级别估计它们的空间属性——到达方向(DoA)和声源距离。为了泛化到未见事件，我们整合了一个音频 grounding 模型，该模型通过跨注意力机制将音频特征与语义音频类别文本嵌入对齐。其次，为了回答涉及移动声源的动态音频场景的复杂查询，我们使用大型语言模型(LLM)，并以我们模型提取的结构化空间属性作为条件。最后，我们引入了一个空间音频运动理解与推理基准数据集，并展示了我们的框架相对于基线模型的性能。",
    "subjects": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ],
    "update_time": "2025-09-19",
    "paper_authors": "Arvind Krishna Sridhar, Yinyi Guo, Erik Visser",
    "topic": [
      "Audio Representation Learning",
      "Audio Classification"
    ],
    "category": [
      "Other"
    ]
  },
  {
    "paper_title": "Pushing the Limits of End-to-End Diarization",
    "paper_title_zh": "突破端到端语音分割的极限",
    "paper_id": "2509.14737",
    "paper_abstract": "In this paper, we present state-of-the-art diarization error rates (DERs) on multiple publicly available datasets, including AliMeeting-far, AliMeeting-near, AMI-Mix, AMI-SDM, DIHARD III, and MagicData RAMC. Leveraging EEND-TA, a single unified non-autoregressive model for end-to-end speaker diarization, we achieve new benchmark results, most notably a DER of 14.49% on DIHARD III. Our approach scales pretraining through 8-speaker simulation mixtures, ensuring each generated speaker mixture configuration is sufficiently represented. These experiments highlight that EEND-based architectures possess a greater capacity for learning than previously explored, surpassing many existing diarization solutions while maintaining efficient speeds during inference.",
    "paper_abstract_zh": "本文在多个公开数据集上实现了最先进的语音分割错误率（DERs），包括AliMeeting-far、AliMeeting-near、AMI-Mix、AMI-SDM、DIHARD III和MagicData RAMC。通过采用EEND-TA——一种用于端到端语音分割的单一统一非自回归模型，我们取得了新的基准测试结果，其中最显著的是在DIHARD III上实现了14.49%的DER。我们的方法通过8说话人模拟混合进行预训练扩展，确保每个生成的说话人混合配置都得到充分表示。这些实验突显了基于EEND的架构具有比先前探索更大的学习能力，在推理过程中保持高效速度的同时，超越了现有的许多语音分割解决方案。",
    "subjects": [
      "Sound (cs.SD)"
    ],
    "update_time": "2025-09-19",
    "paper_authors": "Samuel J. Broughton, Lahiru Samarakoon",
    "topic": [
      "Audio Representation Learning"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Spatial-CLAP: Learning Spatially-Aware audio--text Embeddings for Multi-Source Conditions",
    "paper_title_zh": "Spatial-CLAP：学习空间感知的音频-文本嵌入以应对多源条件",
    "paper_id": "2509.14785",
    "paper_abstract": "Contrastive language--audio pretraining (CLAP) has achieved remarkable success as an audio--text embedding framework, but existing approaches are limited to monaural or single-source conditions and cannot fully capture spatial information. The central challenge in modeling spatial information lies in multi-source conditions, where the correct correspondence between each sound source and its location is required. To tackle this problem, we propose Spatial-CLAP, which introduces a content-aware spatial encoder that enables spatial representations coupled with audio content. We further propose spatial contrastive learning (SCL), a training strategy that explicitly enforces the learning of the correct correspondence and promotes more reliable embeddings under multi-source conditions. Experimental evaluations, including downstream tasks, demonstrate that Spatial-CLAP learns effective embeddings even under multi-source conditions, and confirm the effectiveness of SCL. Moreover, evaluation on unseen three-source mixtures highlights the fundamental distinction between conventional single-source training and our proposed multi-source training paradigm. These findings establish a new paradigm for spatially-aware audio--text embeddings.",
    "paper_abstract_zh": "对比性语言-音频预训练（CLAP）作为音频-文本嵌入框架已取得显著成功，但现有方法仅限于单声道或单源条件，无法充分捕捉空间信息。建模空间信息的核心挑战在于多源条件，其中需要每个声源与其位置之间的正确对应关系。为解决此问题，我们提出了Spatial-CLAP，它引入了一个内容感知的空间编码器，能够实现与音频内容耦合的空间表示。我们进一步提出了空间对比学习（SCL），这是一种训练策略，明确强制学习正确的对应关系，并在多源条件下促进更可靠的嵌入。实验评估（包括下游任务）表明，Spatial-CLAP即使在多源条件下也能学习有效的嵌入，并证实了SCL的有效性。此外，对未见过的三源混合音的评估突显了传统单源训练与我们提出的多源训练范式之间的根本区别。这些发现为空间感知的音频-文本嵌入建立了一个新范式。",
    "subjects": [
      "Sound (cs.SD)"
    ],
    "update_time": "2025-09-19",
    "paper_authors": "Kentaro Seki, Yuki Okamoto, Kouei Yamaoka, Yuki Saito, Shinnosuke Takamichi, Hiroshi Saruwatari",
    "topic": [
      "Audio Representation Learning"
    ],
    "category": [
      "Other"
    ]
  },
  {
    "paper_title": "Towards Building Speech Large Language Models for Multitask Understanding in Low-Resource Languages",
    "paper_title_zh": "面向低资源语言多任务理解的语音大语言模型构建研究",
    "paper_id": "2509.14804",
    "paper_abstract": "Speech large language models (SLLMs) built on speech encoders, adapters, and LLMs demonstrate remarkable multitask understanding performance in high-resource languages such as English and Chinese. However, their effectiveness substantially degrades in low-resource languages such as Thai. This limitation arises from three factors: (1) existing commonly used speech encoders, like the Whisper family, underperform in low-resource languages and lack support for broader spoken language understanding tasks; (2) the ASR-based alignment paradigm requires training the entire SLLM, leading to high computational cost; (3) paired speech-text data in low-resource languages is scarce. To overcome these challenges in the low-resource language Thai, we introduce XLSR-Thai, the first self-supervised learning (SSL) speech encoder for Thai. It is obtained by continuously training the standard SSL XLSR model on 36,000 hours of Thai speech data. Furthermore, we propose U-Align, a speech-text alignment method that is more resource-efficient and multitask-effective than typical ASR-based alignment. Finally, we present Thai-SUP, a pipeline for generating Thai spoken language understanding data from high-resource languages, yielding the first Thai spoken language understanding dataset of over 1,000 hours. Multiple experiments demonstrate the effectiveness of our methods in building a Thai multitask-understanding SLLM. We open-source XLSR-Thai and Thai-SUP to facilitate future research.",
    "paper_abstract_zh": "基于语音编码器、适配器和大语言模型构建的语音大语言模型（SLLMs）在英语和汉语等高资源语言中展现出卓越的多任务理解性能。然而，在泰语等低资源语言中，其效能显著下降。这一局限性源于三个因素：（1）现有常用语音编码器（如Whisper系列）在低资源语言中表现不佳，且缺乏对更广泛口语理解任务的支持；（2）基于ASR的对齐范式需要训练整个SLLM，导致计算成本高昂；（3）低资源语言的语音-文本配对数据稀缺。为克服泰语低资源环境下的这些挑战，我们推出了XLSR-Thai——首个泰语自监督学习（SSL）语音编码器，通过对标准SSL XLSR模型进行36,000小时泰语语音数据的持续训练获得。此外，我们提出U-Align对齐方法，相比典型的基于ASR的对齐方式更具资源效率和多任务适应性。最后，我们开发了Thai-SUP数据生成流程，可从高资源语言生成泰语口语理解数据，构建了首个超1,000小时的泰语口语理解数据集。多项实验证明了我们的方法在构建泰语多任务理解SLLM方面的有效性。我们开源XLSR-Thai和Thai-SUP以促进未来研究。",
    "subjects": [
      "Sound (cs.SD)"
    ],
    "update_time": "2025-09-19",
    "paper_authors": "Mingchen Shao, Bingshen Mu, Chengyou Wang, Hai Li, Ying Yan, Zhonghua Fu, Lei Xie",
    "topic": [
      "Audio Representation Learning",
      "Speech Recognition"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "MeanFlowSE: one-step generative speech enhancement via conditional mean flow",
    "paper_title_zh": "MeanFlowSE：通过条件均值流实现一步生成式语音增强",
    "paper_id": "2509.14858",
    "paper_abstract": "Multistep inference is a bottleneck for real-time generative speech enhancement because flow- and diffusion-based systems learn an instantaneous velocity field and therefore rely on iterative ordinary differential equation (ODE) solvers. We introduce MeanFlowSE, a conditional generative model that learns the average velocity over finite intervals along a trajectory. Using a Jacobian-vector product (JVP) to instantiate the MeanFlow identity, we derive a local training objective that directly supervises finite-interval displacement while remaining consistent with the instantaneous-field constraint on the diagonal. At inference, MeanFlowSE performs single-step generation via a backward-in-time displacement, removing the need for multistep solvers; an optional few-step variant offers additional refinement. On VoiceBank-DEMAND, the single-step model achieves strong intelligibility, fidelity, and perceptual quality with substantially lower computational cost than multistep baselines. The method requires no knowledge distillation or external teachers, providing an efficient, high-fidelity framework for real-time generative speech enhancement.",
    "paper_abstract_zh": "多步推理是实时生成式语音增强的瓶颈，因为基于流和扩散的系统学习的是瞬时速度场，因此依赖于迭代的常微分方程（ODE）求解器。我们提出了MeanFlowSE，这是一种条件生成模型，能够学习轨迹上有限区间内的平均速度。通过使用雅可比向量积（JVP）实例化MeanFlow恒等式，我们推导出一个局部训练目标，该目标直接监督有限区间位移，同时与对角线上的瞬时场约束保持一致。在推理时，MeanFlowSE通过向后时间位移执行单步生成，无需多步求解器；可选的多步变体提供了额外的细化。在VoiceBank-DEMAND数据集上，单步模型实现了出色的可懂度、保真度和感知质量，且计算成本显著低于多步基线方法。该方法不需要知识蒸馏或外部教师，为实时生成式语音增强提供了一个高效、高保真的框架。",
    "subjects": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)"
    ],
    "update_time": "2025-09-19",
    "paper_authors": "Duojia Li, Shenghui Lu, Hongchen Pan, Zongyi Zhan, Qingyang Hong, Lin Li",
    "topic": [
      "Speech Enhancement"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "From Hype to Insight: Rethinking Large Language Model Integration in Visual Speech Recognition",
    "paper_title_zh": "从炒作到洞见：重新思考大语言模型在视觉语音识别中的整合",
    "paper_id": "2509.14880",
    "paper_abstract": "Advances in self-supervised encoders have improved Visual Speech Recognition (VSR). Recent approaches integrating these encoders with LLM decoders improves transcription accuracy; however, it remains unclear whether these gains stem from visual understanding or stronger language modeling. In this work, we systematically evaluate LLM decoders by freezing or selectively updating the visual encoder, scaling decoder size, comparing adaptation strategies and architectures, and varying training data across LRS2, LRS3, and their combination. Evaluation on LRS2, LRS3, and WildVSR shows that scaling and adaptation yield limited improvements, while combining datasets enhances generalization. Semantic analysis reveals that gains arise primarily from lexical rather than semantic processing. Our Llama-2-13B model trained on the combined set achieves 24.7\\% WER on LRS3 and 47.0\\% on WildVSR, establishing SOTA among models trained without additional supervision. Our findings indicate LLM decoders refine contextual reasoning rather than visual features, emphasizing the need for stronger visual encoders to drive meaningful progress.",
    "paper_abstract_zh": "自监督编码器的进展提升了视觉语音识别（VSR）的性能。近期方法将这些编码器与大语言模型（LLM）解码器相结合，提高了转录准确性；然而，尚不清楚这些增益是源于视觉理解还是更强的语言建模能力。在本研究中，我们通过冻结或选择性更新视觉编码器、扩展解码器规模、比较适应策略和架构，以及在LRS2、LRS3及其组合上变化训练数据，系统评估了LLM解码器。在LRS2、LRS3和WildVSR上的评估表明，扩展和适应带来的改进有限，而组合数据集能增强泛化能力。语义分析显示，增益主要来自词汇而非语义处理。我们在组合数据集上训练的Llama-2-13B模型在LRS3上达到24.7%的词错误率（WER），在WildVSR上达到47.0%，在没有额外监督训练的模型中确立了最先进水平。我们的发现表明，LLM解码器优化的是上下文推理而非视觉特征，强调需要更强的视觉编码器来推动有意义的进展。",
    "subjects": [
      "Sound (cs.SD)"
    ],
    "update_time": "2025-09-19",
    "paper_authors": "Rishabh Jain, Naomi Harte",
    "topic": [
      "Speech Recognition"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Back to Ear: Perceptually Driven High Fidelity Music Reconstruction",
    "paper_title_zh": "",
    "paper_id": "2509.14912",
    "paper_abstract": "Variational Autoencoders (VAEs) are essential for large-scale audio tasks like diffusion-based generation. However, existing open-source models often neglect auditory perceptual aspects during training, leading to weaknesses in phase accuracy and stereophonic spatial representation. To address these challenges, we propose {\\epsilon}ar-VAE, an open-source music signal reconstruction model that rethinks and optimizes the VAE training paradigm. Our contributions are threefold: (i) A K-weighting perceptual filter applied prior to loss calculation to align the objective with auditory perception. (ii) Two novel phase losses: a Correlation Loss for stereo coherence, and a Phase Loss using its derivatives--Instantaneous Frequency and Group Delay--for precision. (iii) A new spectral supervision paradigm where magnitude is supervised by all four Mid/Side/Left/Right components, while phase is supervised only by the LR components. Experiments show {\\epsilon}ar-VAE at 44.1kHz substantially outperforms leading open-source models across diverse metrics, showing particular strength in reconstructing high-frequency harmonics and the spatial characteristics.",
    "paper_abstract_zh": "",
    "subjects": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)"
    ],
    "update_time": "2025-09-19",
    "paper_authors": "Kangdi Wang, Zhiyue Wu, Dinghao Zhou, Rui Lin, Junyu Dai, Tao Jiang",
    "topic": [],
    "category": []
  },
  {
    "paper_title": "FCPE: A Fast Context-based Pitch Estimation Model",
    "paper_title_zh": "FCPE：一种基于上下文的快速基频估计模型",
    "paper_id": "2509.15140",
    "paper_abstract": "Pitch estimation (PE) in monophonic audio is crucial for MIDI transcription and singing voice conversion (SVC), but existing methods suffer significant performance degradation under noise. In this paper, we propose FCPE, a fast context-based pitch estimation model that employs a Lynx-Net architecture with depth-wise separable convolutions to effectively capture mel spectrogram features while maintaining low computational cost and robust noise tolerance. Experiments show that our method achieves 96.79\\% Raw Pitch Accuracy (RPA) on the MIR-1K dataset, on par with the state-of-the-art methods. The Real-Time Factor (RTF) is 0.0062 on a single RTX 4090 GPU, which significantly outperforms existing algorithms in efficiency. Code is available at this https URL.",
    "paper_abstract_zh": "单声道音频中的基频估计（PE）对于MIDI转录和歌声转换（SVC）至关重要，但现有方法在噪声环境下性能显著下降。本文提出FCPE，一种基于上下文的快速基频估计模型，采用Lynx-Net架构和深度可分离卷积，有效捕捉梅尔频谱图特征，同时保持低计算成本和强大的噪声容忍能力。实验表明，我们的方法在MIR-1K数据集上达到96.79%的原始基频准确率（RPA），与最先进方法相当。在单块RTX 4090 GPU上的实时因子（RTF）为0.0062，效率显著优于现有算法。代码可通过此https URL获取。",
    "subjects": [
      "Sound (cs.SD)",
      "Computation and Language (cs.CL)"
    ],
    "update_time": "2025-09-19",
    "paper_authors": "Yuxin Luo, Ruoyi Zhang, Lu-Chuan Liu, Tianyu Li, Hangyu Liu",
    "topic": [
      "Music Information Retrieval",
      "Audio Representation Learning"
    ],
    "category": [
      "Music"
    ]
  },
  {
    "paper_title": "Exploring How Audio Effects Alter Emotion with Foundation Models",
    "paper_title_zh": "探索音频效果如何利用基础模型改变情感",
    "paper_id": "2509.15151",
    "paper_abstract": "Audio effects (FX) such as reverberation, distortion, modulation, and dynamic range processing play a pivotal role in shaping emotional responses during music listening. While prior studies have examined links between low-level audio features and affective perception, the systematic impact of audio FX on emotion remains underexplored. This work investigates how foundation models - large-scale neural architectures pretrained on multimodal data - can be leveraged to analyze these effects. Such models encode rich associations between musical structure, timbre, and affective meaning, offering a powerful framework for probing the emotional consequences of sound design techniques. By applying various probing methods to embeddings from deep learning models, we examine the complex, nonlinear relationships between audio FX and estimated emotion, uncovering patterns tied to specific effects and evaluating the robustness of foundation audio models. Our findings aim to advance understanding of the perceptual impact of audio production practices, with implications for music cognition, performance, and affective computing.",
    "paper_abstract_zh": "混响、失真、调制和动态范围处理等音频效果（FX）在塑造音乐聆听过程中的情感反应方面起着关键作用。尽管先前的研究已经探讨了低级音频特征与情感感知之间的联系，但音频FX对情感的系统性影响仍未得到充分探索。本研究探讨了如何利用基础模型——即在大规模多模态数据上预训练的大型神经架构——来分析这些效果。此类模型编码了音乐结构、音色和情感意义之间丰富的关联，为探究声音设计技术的情感后果提供了一个强大的框架。通过对深度学习模型的嵌入表示应用各种探测方法，我们研究了音频FX与估计情感之间复杂的非线性关系，揭示了与特定效果相关的模式，并评估了基础音频模型的鲁棒性。我们的研究结果旨在推进对音频制作实践感知影响的理解，对音乐认知、表演和情感计算具有重要意义。",
    "subjects": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)"
    ],
    "update_time": "2025-09-19",
    "paper_authors": "Stelios Katsis, Vassilis Lyberatos, Spyridon Kantarelis, Edmund Dervakos, Giorgos Stamou",
    "topic": [
      "Music Information Retrieval",
      "Audio Representation Learning"
    ],
    "category": [
      "Music"
    ]
  },
  {
    "paper_title": "Explicit Context-Driven Neural Acoustic Modeling for High-Fidelity RIR Generation",
    "paper_title_zh": "基于显式上下文驱动的高保真房间脉冲响应生成的神经声学建模",
    "paper_id": "2509.15210",
    "paper_abstract": "Realistic sound simulation plays a critical role in many applications. A key element in sound simulation is the room impulse response (RIR), which characterizes how sound propagates from a source to a listener within a given space. Recent studies have applied neural implicit methods to learn RIR using context information collected from the environment, such as scene images. However, these approaches do not effectively leverage explicit geometric information from the environment. To further exploit the potential of neural implicit models with direct geometric features, we present Mesh-infused Neural Acoustic Field (MiNAF), which queries a rough room mesh at given locations and extracts distance distributions as an explicit representation of local context. Our approach demonstrates that incorporating explicit local geometric features can better guide the neural network in generating more accurate RIR predictions. Through comparisons with conventional and state-of-the-art baseline methods, we show that MiNAF performs competitively across various evaluation metrics. Furthermore, we verify the robustness of MiNAF in datasets with limited training samples, demonstrating an advance in high-fidelity sound simulation.",
    "paper_abstract_zh": "逼真的声音模拟在许多应用中起着关键作用。声音模拟中的一个核心要素是房间脉冲响应（RIR），它描述了声音在给定空间中从声源传播到听者的方式。最近的研究应用神经隐式方法，利用从环境中收集的上下文信息（如场景图像）来学习RIR。然而，这些方法未能有效利用环境中的显式几何信息。为了进一步挖掘神经隐式模型结合直接几何特征的潜力，我们提出了网格注入神经声学场（MiNAF），该方法在给定位置查询粗糙的房间网格，并提取距离分布作为局部上下文的显式表示。我们的方法表明，融入显式局部几何特征可以更好地指导神经网络生成更准确的RIR预测。通过与传统和最先进的基线方法进行比较，我们证明MiNAF在各种评估指标上均具有竞争力。此外，我们在训练样本有限的数据集中验证了MiNAF的鲁棒性，展示了其在高保真声音模拟方面的先进性。",
    "subjects": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "update_time": "2025-09-19",
    "paper_authors": "Chen Si, Qianyi Wu, Chaitanya Amballa, Romit Roy Choudhury",
    "topic": [
      "Audio Representation Learning"
    ],
    "category": [
      "Other"
    ]
  },
  {
    "paper_title": "CLAIP-Emo: Parameter-Efficient Adaptation of Language-supervised models for In-the-Wild Audiovisual Emotion Recognition",
    "paper_title_zh": "CLAIP-Emo：基于语言监督模型的参数高效自适应方法用于野外视听情感识别",
    "paper_id": "2509.14527",
    "paper_abstract": "Audiovisual emotion recognition (AVER) in the wild is still hindered by pose variation, occlusion, and background noise. Prevailing methods primarily rely on large-scale domain-specific pre-training, which is costly and often mismatched to real-world affective data. To address this, we present CLAIP-Emo, a modular framework that reframes in-the-wild AVER as a parameter-efficient adaptation of language-supervised foundation models (CLIP/CLAP). Specifically, it (i) preserves language-supervised priors by freezing CLIP/CLAP backbones and performing emotion-oriented adaptation via LoRA (updating \\ensuremath{\\le}4.0\\% of the total parameters), (ii) allocates temporal modeling asymmetrically, employing a lightweight Transformer for visual dynamics while applying mean pooling for audio prosody, and (iii) applies a simple fusion head for prediction. On DFEW and MAFW, CLAIP-Emo (ViT-L/14) achieves 80.14\\% and 61.18\\% weighted average recall with only 8M training parameters, setting a new state of the art. Our findings suggest that parameter-efficient adaptation of language-supervised foundation models provides a scalable alternative to domain-specific pre-training for real-world AVER. The code and models will be available at \\href{this https URL}{this https URL}.",
    "paper_abstract_zh": "野外环境下的视听情感识别（AVER）仍然受到姿态变化、遮挡和背景噪声的阻碍。主流方法主要依赖于大规模领域特定的预训练，这种方法成本高昂且往往与现实世界的情感数据不匹配。为了解决这个问题，我们提出了CLAIP-Emo，一个模块化框架，将野外AVER重新构建为语言监督基础模型（CLIP/CLAP）的参数高效自适应。具体而言，它（i）通过冻结CLIP/CLAP主干网络并利用LoRA进行情感导向的自适应（更新总参数量的≤4.0%）来保留语言监督先验；（ii）非对称地分配时序建模，对视觉动态使用轻量级Transformer，而对音频韵律应用平均池化；（iii）使用简单的融合头进行预测。在DFEW和MAFW数据集上，CLAIP-Emo（ViT-L/14）仅用800万训练参数就实现了80.14%和61.18%的加权平均召回率，创造了新的最先进水平。我们的研究结果表明，语言监督基础模型的参数高效自适应为现实世界AVER提供了一种可扩展的替代方案，避免了领域特定的预训练。代码和模型将在\\href{this https URL}{this https URL}提供。",
    "subjects": [
      "Multimedia (cs.MM)",
      "Sound (cs.SD)"
    ],
    "update_time": "2025-09-19",
    "paper_authors": "Yin Chen, Jia Li, Jinpeng Hu, Zhenzhen Hu, Richang Hong",
    "topic": [
      "Audio Classification"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "MMED: A Multimodal Micro-Expression Dataset based on Audio-Visual Fusion",
    "paper_title_zh": "MMED：基于视听融合的多模态微表情数据集",
    "paper_id": "2509.14592",
    "paper_abstract": "Micro-expressions (MEs) are crucial leakages of concealed emotion, yet their study has been constrained by a reliance on silent, visual-only data. To solve this issue, we introduce two principal contributions. First, MMED, to our knowledge, is the first dataset capturing the spontaneous vocal cues that co-occur with MEs in ecologically valid, high-stakes interactions. Second, the Asymmetric Multimodal Fusion Network (AMF-Net) is a novel method that effectively fuses a global visual summary with a dynamic audio sequence via an asymmetric cross-attention framework. Rigorous Leave-One-Subject-Out Cross-Validation (LOSO-CV) experiments validate our approach, providing conclusive evidence that audio offers critical, disambiguating information for ME analysis. Collectively, the MMED dataset and our AMF-Net method provide valuable resources and a validated analytical approach for micro-expression recognition.",
    "paper_abstract_zh": "微表情（MEs）是隐藏情绪的关键泄露指标，但其研究一直受限于对无声、纯视觉数据的依赖。为解决这一问题，我们提出了两个主要贡献。首先，据我们所知，MMED是首个在生态效度高、高风险交互中捕捉与微表情共现的自发声学线索的数据集。其次，我们提出了非对称多模态融合网络（AMF-Net），这是一种通过非对称交叉注意力框架将全局视觉摘要与动态音频序列有效融合的新方法。严格的留一被试交叉验证（LOSO-CV）实验验证了我们的方法，确凿证明音频为微表情分析提供了关键的去歧义信息。MMED数据集和AMF-Net方法共同为微表情识别提供了宝贵资源和经过验证的分析方法。",
    "subjects": [
      "Multimedia (cs.MM)",
      "Sound (cs.SD)"
    ],
    "update_time": "2025-09-19",
    "paper_authors": "Junbo Wang, Yan Zhao, Shuo Li, Shibo Wang, Shigang Wang, Jian Wei",
    "topic": [
      "Audio Classification",
      "Other"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Efficient Solutions for Mitigating Initialization Bias in Unsupervised Self-Adaptive Auditory Attention Decoding",
    "paper_title_zh": "缓解无监督自适应听觉注意力解码中初始化偏差的高效解决方案",
    "paper_id": "2509.14764",
    "paper_abstract": "Decoding the attended speaker in a multi-speaker environment from electroencephalography (EEG) has attracted growing interest in recent years, with neuro-steered hearing devices as a driver application. Current approaches typically rely on ground-truth labels of the attended speaker during training, necessitating calibration sessions for each user and each EEG set-up to achieve optimal performance. While unsupervised self-adaptive auditory attention decoding (AAD) for stimulus reconstruction has been developed to eliminate the need for labeled data, it suffers from an initialization bias that can compromise performance. Although an unbiased variant has been proposed to address this limitation, it introduces substantial computational complexity that scales with data size. This paper presents three computationally efficient alternatives that achieve comparable performance, but with a significantly lower and constant computational cost. The code for the proposed algorithms is available at this https URL.",
    "paper_abstract_zh": "在多说话人环境中从脑电图（EEG）解码被关注的说话人近年来引起了越来越多的兴趣，其中神经导向的听力设备是一个驱动应用。当前的方法通常在训练期间依赖于被关注说话人的真实标签，这需要为每个用户和每个EEG设置进行校准会话以实现最佳性能。虽然已经开发了用于刺激重建的无监督自适应听觉注意力解码（AAD）以消除对标记数据的需求，但它存在初始化偏差问题，可能会影响性能。尽管已经提出了一种无偏变体来解决这一限制，但它引入了随着数据规模增长而显著增加的计算复杂度。本文提出了三种计算高效替代方案，能够实现相当的性能，但计算成本显著更低且保持恒定。所提出算法的代码可在该https URL获取。",
    "subjects": [
      "Signal Processing (eess.SP)",
      "Sound (cs.SD)"
    ],
    "update_time": "2025-09-19",
    "paper_authors": "Yuanyuan Yao, Simon Geirnaert, Tinne Tuytelaars, Alexander Bertrand",
    "topic": [
      "Audio Representation Learning",
      "Speech Recognition"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Music4All A+A: A Multimodal Dataset for Music Information Retrieval Tasks",
    "paper_title_zh": "Music4All A+A：面向音乐信息检索任务的多模态数据集",
    "paper_id": "2509.14891",
    "paper_abstract": "Music is characterized by aspects related to different modalities, such as the audio signal, the lyrics, or the music video clips. This has motivated the development of multimodal datasets and methods for Music Information Retrieval (MIR) tasks such as genre classification or autotagging. Music can be described at different levels of granularity, for instance defining genres at the level of artists or music albums. However, most datasets for multimodal MIR neglect this aspect and provide data at the level of individual music tracks. We aim to fill this gap by providing Music4All Artist and Album (Music4All A+A), a dataset for multimodal MIR tasks based on music artists and albums. Music4All A+A is built on top of the Music4All-Onion dataset, an existing track-level dataset for MIR tasks. Music4All A+A provides metadata, genre labels, image representations, and textual descriptors for 6,741 artists and 19,511 albums. Furthermore, since Music4All A+A is built on top of Music4All-Onion, it allows access to other multimodal data at the track level, including user--item interaction data. This renders Music4All A+A suitable for a broad range of MIR tasks, including multimodal music recommendation, at several levels of granularity. To showcase the use of Music4All A+A, we carry out experiments on multimodal genre classification of artists and albums, including an analysis in missing-modality scenarios, and a quantitative comparison with genre classification in the movie domain. Our experiments show that images are more informative for classifying the genres of artists and albums, and that several multimodal models for genre classification struggle in generalizing across domains. We provide the code to reproduce our experiments at this https URL, the dataset is linked in the repository and provided open-source under a CC BY-NC-SA 4.0 license.",
    "paper_abstract_zh": "音乐的特征体现在与不同模态相关的方面，例如音频信号、歌词或音乐视频片段。这推动了针对音乐信息检索（MIR）任务（如流派分类或自动标注）的多模态数据集和方法的开发。音乐可以在不同的粒度级别上进行描述，例如在艺术家或音乐专辑的级别上定义流派。然而，大多数多模态MIR数据集忽略了这一点，仅在单个音乐曲目的级别上提供数据。我们旨在通过提供Music4All艺术家与专辑（Music4All A+A）数据集来填补这一空白，这是一个基于音乐艺术家和专辑的多模态MIR任务数据集。Music4All A+A建立在Music4All-Onion数据集之上，这是一个现有的曲目级MIR任务数据集。Music4All A+A为6,741名艺术家和19,511张专辑提供了元数据、流派标签、图像表示和文本描述。此外，由于Music4All A+A建立在Music4All-Onion之上，它还允许访问曲目级别的其他多模态数据，包括用户-项目交互数据。这使得Music4All A+A适用于广泛的多粒度MIR任务，包括多模态音乐推荐。为了展示Music4All A+A的使用，我们进行了艺术家和专辑的多模态流派分类实验，包括在缺失模态场景下的分析，以及与电影领域流派分类的定量比较。我们的实验表明，图像在分类艺术家和专辑的流派方面更具信息性，并且多种多模态流派分类模型在跨领域泛化方面存在困难。我们提供了重现实验的代码（此https URL），数据集在存储库中链接，并在CC BY-NC-SA 4.0许可下开源提供。",
    "subjects": [
      "Multimedia (cs.MM)",
      "Information Retrieval (cs.IR)",
      "Sound (cs.SD)"
    ],
    "update_time": "2025-09-19",
    "paper_authors": "Jonas Geiger, Marta Moscati, Shah Nawaz, Markus Schedl",
    "topic": [
      "Music Information Retrieval",
      "Audio Classification"
    ],
    "category": [
      "Music"
    ]
  }
]