[
  {
    "paper_title": "Automatic Speech Recognition in the Modern Era: Architectures, Training, and Evaluation",
    "paper_title_zh": "现代时代的自动语音识别：架构、训练与评估",
    "paper_id": "2510.12827",
    "paper_abstract": "Automatic Speech Recognition (ASR) has undergone a profound transformation over the past decade, driven by advances in deep learning. This survey provides a comprehensive overview of the modern era of ASR, charting its evolution from traditional hybrid systems, such as Gaussian Mixture Model-Hidden Markov Models (GMM-HMMs) and Deep Neural Network-HMMs (DNN-HMMs), to the now-dominant end-to-end neural architectures. We systematically review the foundational end-to-end paradigms: Connectionist Temporal Classification (CTC), attention-based encoder-decoder models, and the Recurrent Neural Network Transducer (RNN-T), which established the groundwork for fully integrated speech-to-text systems. We then detail the subsequent architectural shift towards Transformer and Conformer models, which leverage self-attention to capture long-range dependencies with high computational efficiency. A central theme of this survey is the parallel revolution in training paradigms. We examine the progression from fully supervised learning, augmented by techniques like SpecAugment, to the rise of self-supervised learning (SSL) with foundation models such as wav2vec 2.0, which drastically reduce the reliance on transcribed data. Furthermore, we analyze the impact of largescale, weakly supervised models like Whisper, which achieve unprecedented robustness through massive data diversity. The paper also covers essential ecosystem components, including key datasets and benchmarks (e.g., LibriSpeech, Switchboard, CHiME), standard evaluation metrics (e.g., Word Error Rate), and critical considerations for real-world deployment, such as streaming inference, on-device efficiency, and the ethical imperatives of fairness and robustness. We conclude by outlining open challenges and future research directions.",
    "paper_abstract_zh": "自动语音识别(ASR)在过去十年中经历了深刻的变革，这得益于深度学习的进步。本综述全面概述了现代ASR的发展历程，从传统的混合系统（如高斯混合模型-隐马尔可夫模型(GMM-HMM)和深度神经网络-隐马尔可夫模型(DNN-HMM)）演变为如今占主导地位的端到端神经架构。我们系统地回顾了基础端到端范式：连接主义时间分类(CTC)、基于注意力的编码器-解码器模型和循环神经网络转换器(RNN-T)，这些为完全集成的语音到文本系统奠定了基础。随后，我们详细介绍了向Transformer和Conformer模型的架构转变，这些模型利用自注意力机制以高计算效率捕获长距离依赖关系。本综述的一个核心主题是训练范式的并行革命。我们考察了从完全监督学习（通过SpecAugment等技术增强）到自监督学习(SSL)的演进，以及wav 2.0等基础模型的兴起，这些模型大幅减少了对转录数据的依赖。此外，我们还分析了大规模弱监督模型（如Whisper）的影响，这些模型通过海量数据多样性实现了前所未有的鲁棒性。本文还涵盖了关键的生态系统组件，包括主要数据集和基准（如LibriSpeech、Switchboard、CHiME）、标准评估指标（如词错误率）以及实际部署的关键考虑因素，如流式推理、设备效率和公平性与鲁棒性的道德必要性。最后，我们概述了开放挑战和未来研究方向。",
    "subjects": [
      "Artificial Intelligence (cs.AI)",
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "update_time": "2025-10-16",
    "paper_authors": "Md. Nayeem, Md Shamse Tabrej, Kabbojit Jit Deb, Shaonti Goswami, Md. Azizul Hakim",
    "topic": [
      "Speech Recognition",
      "Audio Representation Learning"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "HyWA: Hypernetwork Weight Adapting Personalized Voice Activity Detection",
    "paper_title_zh": "HyWA: 超网络权重适配个性化语音活动检测",
    "paper_id": "2510.12947",
    "paper_abstract": "Personalized Voice Activity Detection (PVAD) systems activate only in response to a specific target speaker by incorporating speaker embeddings from enrollment utterances. Unlike existing methods that require architectural changes, such as FiLM layers, our approach employs a hypernetwork to modify the weights of a few selected layers within a standard voice activity detection (VAD) model. This enables speaker conditioning without changing the VAD architecture, allowing the same VAD model to adapt to different speakers by updating only a small subset of the layers. We propose HyWA-PVAD, a hypernetwork weight adaptation method, and evaluate it against multiple baseline conditioning techniques. Our comparison shows consistent improvements in PVAD performance. HyWA also offers practical advantages for deployment by preserving the core VAD architecture. Our new approach improves the current conditioning techniques in two ways: i) increases the mean average precision, ii) simplifies deployment by reusing the same VAD architecture.",
    "paper_abstract_zh": "个性化语音活动检测(PVAD)系统通过整合注册语音的说话人嵌入，仅在响应特定目标说话人时激活。与需要架构更改(如FiLM层)的现有方法不同，我们的方法采用超网络来修改标准语音活动检测(VAD)模型中少数选定层的权重。这使得能够在不改变VAD架构的情况下实现说话人条件化，允许同一VAD模型通过仅更新一小部分层来适应不同的说话人。我们提出了HyWA-PVAD，一种超网络权重适配方法，并与多种基线条件化技术进行了评估。我们的比较显示PVAD性能的一致性提升。HyWA还通过保留核心VAD架构为部署提供了实际优势。我们的新方法从两个方面改进了当前的条件化技术：i) 提高了平均平均精度，ii) 通过重用相同的VAD架构简化了部署。",
    "subjects": [
      "Artificial Intelligence (cs.AI)",
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)"
    ],
    "update_time": "2025-10-16",
    "paper_authors": "Mahsa Ghazvini Nejad, Hamed Jafarzadeh Asl, Amin Edraki, Mohammadreza Sadeghi, Masoud Asgharian, Yuanhao Yu, Vahid Partovi Nia",
    "topic": [
      "Audio Classification",
      "Speech Recognition"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs",
    "paper_title_zh": "多模态大语言模型中基于说话人引用的连续标记扩散文本转语音",
    "paper_id": "2510.12995",
    "paper_abstract": "Unified architectures in multimodal large language models (MLLM) have shown promise in handling diverse tasks within a single framework. In the text-to-speech (TTS) task, current MLLM-based approaches rely on discrete token representations, which disregard the inherently continuous nature of speech and can lead to loss of fine-grained acoustic this http URL this work, we investigate the TTS within the MLLM paradigm using continuous speech representations. We design a dual-head architecture and implement two complementary training strategies for a robust model. (1) A diffusion head generating continuous speech representations is added on the MLLM, which is on frame-level and strictly autoregressive. (2) The original language model head is retained to preserve multitask capability and to control the start and end of speech synthesis. (3) Masked training is employed to address exposure bias in autoregressive decoding. (4) To stabilize optimization, we propose a two-stage scheme where the LM is frozen in the second stage, ensuring the diffusion head learns from a fixed input distribution. Evaluations on LibriSpeech(PC) test-clean show that our approach achieves state-of-the-art autoregressive performance, with a WER of 1.95%, speaker similarity of 0.54, and UTMOS of 4.00. The two-stage training yields a 46% relative WER reduction over the one-stage training baseline. These results highlight the effectiveness of combining autoregressive modeling with continuous-token diffusion, supported by a two-stage training procedure.",
    "paper_abstract_zh": "多模态大语言模型(MLLM)中的统一架构在单一框架内处理多样化任务方面显示出潜力。在文本转语音(TTS)任务中，当前基于MLLM的方法依赖于离散标记表示，这忽视了语音固有的连续特性，可能导致细粒度声学信息的丢失。在本工作中，我们研究在MLLM范式下使用连续语音表示进行TTS。我们设计了一个双头架构，并实现了两种互补的训练策略以构建鲁棒模型。(1)在MLLM上添加一个生成连续语音表示的扩散头，该头在帧级别上运行且严格自回归。(2)保留原始语言模型头以保持多任务能力并控制语音合成的开始和结束。(3)采用掩码训练来解决自回归解码中的曝光偏差。(4)为了稳定优化，我们提出一个两阶段方案，其中第二阶段冻结语言模型，确保扩散头从固定的输入分布中学习。在LibriSpeech(PC)测试集上的评估显示，我们的方法达到了最先进的自回归性能，词错误率(WER)为1.95%，说话人相似度为0.54，UTMOS为4.00。两阶段训练相比单阶段训练基线实现了46%的相对WER降低。这些结果突显了结合自回归建模与连续标记扩散的有效性，并由两阶段训练程序支持。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "update_time": "2025-10-16",
    "paper_authors": "Xinlu He, Swayambhu Nath Ray, Harish Mallidi, Jia-Hong Huang, Ashwin Bellur, Chander Chandak, M. Maruf, Venkatesh Ravichandran",
    "topic": [
      "Speech Synthesis",
      "Audio Representation Learning"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Acoustic Teleportation via Disentangled Neural Audio Codec Representations",
    "paper_title_zh": "基于解耦神经音频编解码器表示的声音传输",
    "paper_id": "2510.13221",
    "paper_abstract": "This paper presents an approach for acoustic teleportation by disentangling speech content from acoustic environment characteristics in neural audio codec representations. Acoustic teleportation transfers room characteristics between speech recordings while preserving content and speaker identity. We build upon previous work using the EnCodec architecture, achieving substantial objective quality improvements with non-intrusive ScoreQ scores of 3.03, compared to 2.44 for prior methods. Our training strategy incorporates five tasks: clean reconstruction, reverberated reconstruction, dereverberation, and two variants of acoustic teleportation. We demonstrate that temporal downsampling of the acoustic embedding significantly degrades performance, with even 2x downsampling resulting in a statistically significant reduction in quality. The learned acoustic embeddings exhibit strong correlations with RT60. Effective disentanglement is demonstrated using t-SNE clustering analysis, where acoustic embeddings cluster by room while speech embeddings cluster by speaker.",
    "paper_abstract_zh": "本文提出了一种通过解耦神经音频编解码器表示中的语音内容与声学环境特征来实现声音传输的方法。声音传输能够在保持语音内容和说话人身份的同时，在不同语音录音之间传输房间特征。我们基于之前的EnCodec架构工作，实现了显著的目标质量提升，非侵入式ScoreQ得分为3.03，而先前方法为2.44。我们的训练策略结合了五项任务：干净语音重建、混响语音重建、去混响以及两种变体的声音传输。我们证明声学嵌入的时间下采样会显著降低性能，即使是2倍下采样也会导致质量显著下降。学习的声学嵌入与RT60表现出强相关性。通过t-SNE聚类分析证明了有效的解耦，其中声学嵌入按房间聚类，而语音嵌入按说话人聚类。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2025-10-16",
    "paper_authors": "Philipp Grundhuber, Mhd Modar Halimeh, Emanuël A. P. Habets",
    "topic": [
      "Audio Representation Learning",
      "Audio Codec",
      "Speech Enhancement"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Two Heads Are Better Than One: Audio-Visual Speech Error Correction with Dual Hypotheses",
    "paper_title_zh": "两个脑袋比一个好：基于双假设的视听语音错误纠正",
    "paper_id": "2510.13281",
    "paper_abstract": "This paper introduces a new paradigm for generative error correction (GER) framework in audio-visual speech recognition (AVSR) that reasons over modality-specific evidences directly in the language space. Our framework, DualHyp, empowers a large language model (LLM) to compose independent N-best hypotheses from separate automatic speech recognition (ASR) and visual speech recognition (VSR) models. To maximize the effectiveness of DualHyp, we further introduce RelPrompt, a noise-aware guidance mechanism that provides modality-grounded prompts to the LLM. RelPrompt offers the temporal reliability of each modality stream, guiding the model to dynamically switch its focus between ASR and VSR hypotheses for an accurate correction. Under various corruption scenarios, our framework attains up to 57.7% error rate gain on the LRS2 benchmark over standard ASR baseline, contrary to single-stream GER approaches that achieve only 10% gain. To facilitate research within our DualHyp framework, we release the code and the dataset comprising ASR and VSR hypotheses at this https URL.",
    "paper_abstract_zh": "本文在视听语音识别(AVSR)的生成式错误纠正(GER)框架中引入了一种新范式，直接在语言空间中推理模态特定证据。我们的框架DualHyp使大型语言模型(LLM)能够从独立的自动语音识别(ASR)和视觉语音识别(VSR)模型中组合独立的N-best假设。为了最大化DualHyp的有效性，我们进一步引入了RelPrompt，这是一种噪声感知引导机制，为LLM提供基于模态的提示。RelPrompt提供每个模态流的时间可靠性，引导模型在ASR和VSR假设之间动态切换焦点以进行准确纠正。在各种损坏场景下，我们的框架在LRS2基准测试上比标准ASR基线实现了高达57.7%的错误率提升，而单流GER方法仅实现10%的提升。为了促进我们DualHyp框架内的研究，我们在提供的网址发布了代码和包含ASR和VSR假设的数据集。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "update_time": "2025-10-16",
    "paper_authors": "Sungnyun Kim, Kangwook Jang, Sungwoo Cho, Joon Son Chung, Hoirin Kim, Se-Young Yun",
    "topic": [
      "Speech Recognition",
      "Audio Representation Learning"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Towards Multimodal Query-Based Spatial Audio Source Extraction",
    "paper_title_zh": "面向多模态查询的空音频源提取",
    "paper_id": "2510.13308",
    "paper_abstract": "Query-based audio source extraction seeks to recover a target source from a mixture conditioned on a query. Existing approaches are largely confined to single-channel audio, leaving the spatial information in multi-channel recordings underexploited. We introduce a query-based spatial audio source extraction framework for recovering dry target signals from first-order ambisonics (FOA) mixtures. Our method accepts either an audio prompt or a text prompt as condition input, enabling flexible end-to-end extraction. The core of our proposed model lies in a tri-axial Transformer that jointly models temporal, frequency, and spatial channel dependencies. The model uses contrastive language-audio pretraining (CLAP) embeddings to enable unified audio-text conditioning via feature-wise linear modulation (FiLM). To eliminate costly annotations and improve generalization, we propose a label-free data pipeline that dynamically generates spatial mixtures and corresponding targets for training. The result of our experiment with high separation quality demonstrates the efficacy of multimodal conditioning and tri-axial modeling. This work establishes a new paradigm for high-fidelity spatial audio separation in immersive applications.",
    "paper_abstract_zh": "基于查询的音频源提取旨在根据查询从混合信号中恢复目标源。现有方法主要局限于单通道音频，导致多通道录音中的空间信息未被充分利用。我们提出了一种基于查询的空音频源提取框架，用于从一阶 Ambisonics (FOA) 混合信号中恢复干目标信号。我们的方法接受音频提示或文本提示作为条件输入，实现灵活的端到端提取。我们提出的模型核心是一个三轴 Transformer，它联合建模时间、频率和空间通道依赖关系。模型使用对比语言-音频预训练 (CLAP) 嵌入，通过特征级线性调制 (FiLM) 实现统一的音频-文本条件输入。为消除昂贵的标注并提高泛化能力，我们提出了一种无标签数据管道，动态生成空间混合信号及其对应的目标信号进行训练。实验结果表明，高质量的分离效果证明了多模态条件和三轴建模的有效性。这项工作为沉浸式应用中的高保真空音频分离建立了新范式。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2025-10-16",
    "paper_authors": "Chenxin Yu, Hao Ma, Xu Li, Xiao-Lei Zhang, Mingjie Shao, Chi Zhang, Xuelong Li",
    "topic": [
      "Audio Representation Learning",
      "Speech Enhancement"
    ],
    "category": [
      "Other"
    ]
  },
  {
    "paper_title": "Beyond Discrete Categories: Multi-Task Valence-Arousal Modeling for Pet Vocalization Analysis",
    "paper_title_zh": "超越离散类别：宠物发声分析的多任务效价-唤醒度建模",
    "paper_id": "2510.12819",
    "paper_abstract": "Traditional pet emotion recognition from vocalizations, based on discrete classification, struggles with ambiguity and capturing intensity variations. We propose a continuous Valence-Arousal (VA) model that represents emotions in a two-dimensional space. Our method uses an automatic VA label generation algorithm, enabling large-scale annotation of 42,553 pet vocalization samples. A multi-task learning framework jointly trains VA regression with auxiliary tasks (emotion, body size, gender) to enhance prediction by improving feature learning. Our Audio Transformer model achieves a validation Valence Pearson correlation of r = 0.9024 and an Arousal r = 0.7155, effectively resolving confusion between discrete categories like \"territorial\" and \"happy.\" This work introduces the first continuous VA framework for pet vocalization analysis, offering a more expressive representation for human-pet interaction, veterinary diagnostics, and behavioral training. The approach shows strong potential for deployment in consumer products like AI pet emotion translators.",
    "paper_abstract_zh": "传统的基于离散分类的宠物情绪识别方法在处理模糊性和捕捉强度变化方面存在困难。我们提出了一种连续的效价-唤醒度(VA)模型，用于在二维空间中表示情绪。我们的方法使用自动VA标签生成算法，实现了对42,553个宠物发声样本的大规模标注。多任务学习框架将VA回归与辅助任务（情绪、体型、性别）联合训练，通过改进特征学习来增强预测效果。我们的Audio Transformer模型在验证集上达到了效价皮尔逊相关系数r=0.9024和唤醒度r=0.7155，有效解决了离散类别如'领地性'和'快乐'之间的混淆问题。这项工作首次引入了宠物发声分析的连续VA框架，为人宠互动、兽医诊断和行为训练提供了更具表现力的表示方式。该方法在AI宠物情绪翻译器等消费产品中显示出强大的应用潜力。",
    "subjects": [
      "Artificial Intelligence (cs.AI)",
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "update_time": "2025-10-16",
    "paper_authors": "Junyao Huang, Rumin Situ",
    "topic": [
      "Audio Representation Learning",
      "Audio Classification"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Production and Manufacturing of 3D Printed Acoustic Guitars",
    "paper_title_zh": "3D打印声学吉他的生产与制造",
    "paper_id": "2510.12823",
    "paper_abstract": "This research investigates the feasibility of producing affordable, functional acoustic guitars using 3D printing, with a focus on producing structural designs with proper tonal performance. Conducted in collaboration with William Schiesser, the study uses a classical guitar model, chosen for its lower string tension, to evaluate the tonal characteristics of a 3D-printed prototype made from polylactic acid (PLA). Due to the build plate size constraints of the Prusa Mark 4 printer, the guitar body was divided into multiple sections joined with press-fit tolerances and minimal cyanoacrylate adhesive. CAD modeling in Fusion 360 ensured dimensional accuracy in press-fit connections and the overall assembly. Following assembly, the guitar was strung with nylon strings and tested using Audacity software to compare recorded frequencies and notes with standard reference values. Results showed large deviations in lower string frequencies, likely caused by the material choice utilized in printing. Accurate pitches were reached with all strings despite frequency differences through tuning, demonstrating that PLA and modern manufacturing methods can produce affordable, playable acoustic guitars despite inevitable challenges. Further research may investigate alternative plastics for superior frequency matching. This approach holds significant potential for expanding access to quality instruments while reducing reliance on endangered tonewoods, thereby encouraging both sustainable instrument production and increased musical participation. This also creates opportunities for disadvantaged communities where access to musical instruments remains a challenge.\nKeywords: Luthiery, Stereolithography, 3D-Print, Guitar Making",
    "paper_abstract_zh": "本研究探讨了使用3D打印技术生产经济实惠、功能齐全的声学吉他的可行性，重点在于生产具有适当音质性能的结构设计。与William Schiesser合作进行的研究采用古典吉他模型（因其弦张力较低）来评估使用聚乳酸（PLA）材料打印的原型的音质特性。受Prusa Mark 4打印机打印平台尺寸限制，吉他主体被分成多个部分，采用压配合公差和少量氰基丙烯酸酯粘合剂连接。使用Fusion 360进行CAD建模，确保压配合连接和整体组装的尺寸精度。组装完成后，吉他装上尼龙弦，并使用Audacity软件进行测试，将录制的频率和音符与标准参考值进行比较。结果显示低音弦频率存在较大偏差，可能是由于打印材料选择所致。尽管存在频率差异，但通过调音，所有弦都达到了准确的音高，这表明尽管存在不可避免的挑战，PLA和现代制造方法仍能生产出经济实惠、可演奏的声学吉他。进一步研究可以探索替代塑料材料以实现更好的频率匹配。这种方法在扩大优质乐器获取渠道的同时减少对濒危音木的依赖，从而促进可持续乐器生产和音乐参与度的提高。这也为乐器获取仍然存在劣势的社区创造了机会。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "update_time": "2025-10-16",
    "paper_authors": "Timothy Tran, William Schiesser",
    "topic": [
      "Music Generation",
      "Other"
    ],
    "category": [
      "Music"
    ]
  },
  {
    "paper_title": "Gelina: Unified Speech and Gesture Synthesis via Interleaved Token Prediction",
    "paper_title_zh": "Gelina：通过交错令牌预测实现语音和手势的统一合成",
    "paper_id": "2510.12834",
    "paper_abstract": "Human communication is multimodal, with speech and gestures tightly coupled, yet most computational methods for generating speech and gestures synthesize them sequentially, weakening synchrony and prosody alignment. We introduce Gelina, a unified framework that jointly synthesizes speech and co-speech gestures from text using interleaved token sequences in a discrete autoregressive backbone, with modality-specific decoders. Gelina supports multi-speaker and multi-style cloning and enables gesture-only synthesis from speech inputs. Subjective and objective evaluations demonstrate competitive speech quality and improved gesture generation over unimodal baselines.",
    "paper_abstract_zh": "人类交流是多模态的，语音和手势紧密耦合，但大多数生成语音和手势的计算方法顺序合成它们，削弱了同步性和韵律对齐。我们介绍了Gelina，这是一个统一框架，使用离散自回归主干中的交错令牌序列从文本联合合成语音和伴随手势的语音，并具有模态特定的解码器。Gelina支持多说话人和多风格克隆，并支持从语音输入进行纯手势合成。主观和客观评估表明，与单模态基线相比，Gelina具有竞争力的语音质量和改进的手势生成。",
    "subjects": [
      "Artificial Intelligence (cs.AI)",
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "update_time": "2025-10-16",
    "paper_authors": "Téo Guichoux, Théodor Lemerle, Shivam Mehta, Jonas Beskow, Gustave Eje Henter, Laure Soulier, Catherine Pelachaud, Nicolas Obin",
    "topic": [
      "Speech Synthesis"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Adaptive vector steering: A training-free, layer-wise intervention for hallucination mitigation in large audio and multimodal models",
    "paper_title_zh": "自适应向量转向：一种用于减轻大型音频和多模态模型中幻觉的无训练、逐层干预方法",
    "paper_id": "2510.12851",
    "paper_abstract": "Large Audio-Language Models and Multi-Modal Large Language Models have demonstrated strong capabilities in tasks such as Audio Question Answering (AQA), Audio Captioning, and Automatic Speech Recognition (ASR). However, there is growing evidence that these models can hallucinate about the content of the audio. To address this issue, we probe the models' internal states and propose Adaptive Vector Steering (AVS), a method that better grounds generation in audio content. We also identify a strong correlation between output correctness and internal representations. Experiments show consistent performance gains across two models and two benchmarks. On the Audio Hallucination QA dataset, our method boosts the F1-score of Gemma from 0.550 to 0.619 and Qwen from 0.626 to 0.632. Furthermore, our method increases the accuracy of Qwen on MMAU from 0.548 to 0.592, marking an 8% relative increase. To the best of our knowledge, this is the first work to apply vector steering to mitigate hallucination in audio.",
    "paper_abstract_zh": "大型音频语言模型和多模态大型语言模型在音频问答（AQA）、音频描述和自动语音识别（ASR）等任务中表现出强大的能力。然而，越来越多的证据表明，这些模型可能会对音频内容产生幻觉。为了解决这个问题，我们研究了模型的内部状态，并提出了自适应向量转向（AVS）方法，该方法使生成更好地基于音频内容。我们还发现输出正确性与内部表示之间存在强相关性。实验显示在两个模型和两个基准测试上均有一致的性能提升。在音频幻觉问答数据集上，我们的方法将Gemma的F1分数从0.550提高到0.619，将Qwen的F1分数从0.626提高到0.632。此外，我们的方法将Qwen在MMAU上的准确率从0.548提高到0.592，实现了8%的相对增长。据我们所知，这是首次将向量转向应用于减轻音频幻觉的工作。",
    "subjects": [
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "update_time": "2025-10-16",
    "paper_authors": "Tsung-En Lin, Kuan-Yi Lee, Hung-Yi Lee",
    "topic": [
      "Speech Recognition",
      "Other"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Closing the Gap Between Text and Speech Understanding in LLMs",
    "paper_title_zh": "缩小大型语言模型中文本与语音理解之间的差距",
    "paper_id": "2510.13632",
    "paper_abstract": "Large Language Models (LLMs) can be adapted to extend their text capabilities to speech inputs. However, these speech-adapted LLMs consistently underperform their text-based counterparts--and even cascaded pipelines--on language understanding tasks. We term this shortfall the text-speech understanding gap: the performance drop observed when a speech-adapted LLM processes spoken inputs relative to when the original text-based LLM processes the equivalent text. Recent approaches to narrowing this gap either rely on large-scale speech synthesis of text corpora, which is costly and heavily dependent on synthetic data, or on large-scale proprietary speech datasets, which are not reproducible. As a result, there remains a need for more data-efficient alternatives for closing the text-speech understanding gap. In this work, we analyze the gap as driven by two factors: (i) forgetting of text capabilities during adaptation, and (ii) cross-modal misalignment between speech and text. Based on this analysis, we introduce SALAD--Sample-efficient Alignment with Learning through Active selection and cross-modal Distillation--which combines cross-modal distillation with targeted synthetic data to improve alignment while mitigating forgetting. Applied to 3B and 7B LLMs, SALAD achieves competitive performance with a strong open-weight model across broad-domain benchmarks in knowledge, language understanding, and reasoning, while training on over an order of magnitude less speech data from public corpora.",
    "paper_abstract_zh": "大型语言模型（LLMs）可以进行调整以将其文本能力扩展到语音输入。然而，这些语音适配的LLMs在语言理解任务上持续表现逊于其基于文本的对应模型——甚至级联管道。我们将这种不足称为文本-语音理解差距：当语音适配的LLMs处理语音输入时相对于原始基于文本的LLMs处理等效文本时观察到的性能下降。最近缩小这一差距的方法要么依赖于文本语料库的大规模语音合成，这成本高昂且严重依赖合成数据；要么依赖于大规模专有语音数据集，这些数据集不可复现。因此，仍然需要更高效的数据替代方案来缩小文本-语音理解差距。在这项工作中，我们将差距分析归因于两个因素：（i）适应过程中文本能力的遗忘，以及（ii）语音与文本之间的跨模态失准。基于这一分析，我们引入了SALAD——通过主动选择和跨模态蒸馏进行的高效样本对齐与学习——它将跨模态蒸馏与有针对性的合成数据相结合，以改善对齐同时减轻遗忘。应用于30亿和70亿参数的LLMs时，SALAD在知识、语言理解和推理的广泛领域基准测试中，与一个强大的开源权重模型实现了具有竞争力的性能，同时在公共语料库上训练时使用的语音数据量减少了超过一个数量级。",
    "subjects": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2025-10-16",
    "paper_authors": "Santiago Cuervo, Skyler Seto, Maureen de Seyssel, Richard He Bai, Zijin Gu, Tatiana Likhomanenko, Navdeep Jaitly, Zakaria Aldeneh",
    "topic": [
      "Speech Recognition",
      "Audio Representation Learning"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "VCTR: A Transformer-Based Model for Non-parallel Voice Conversion",
    "paper_title_zh": "VCTR：一种基于Transformer的非并行语音转换模型",
    "paper_id": "2510.12964",
    "paper_abstract": "Non-parallel voice conversion aims to convert voice from a source domain to a target domain without paired training data. Cycle-Consistent Generative Adversarial Networks (CycleGAN) and Variational Autoencoders (VAE) have been used for this task, but these models suffer from difficult training and unsatisfactory results. Later, Contrastive Voice Conversion (CVC) was introduced, utilizing a contrastive learning-based approach to address these issues. However, these methods use CNN-based generators, which can capture local semantics but lacks the ability to capture long-range dependencies necessary for global semantics. In this paper, we propose VCTR, an efficient method for non-parallel voice conversion that leverages the Hybrid Perception Block (HPB) and Dual Pruned Self-Attention (DPSA) along with a contrastive learning-based adversarial approach. The code can be found in this https URL.",
    "paper_abstract_zh": "非并行语音转换旨在无需成对训练数据的情况下将语音从源域转换到目标域。循环一致性生成对抗网络（CycleGAN）和变分自编码器（VAE）已被用于此任务，但这些模型存在训练困难和结果不理想的问题。后来，对比语音转换（CVC）被提出，采用基于对比学习的方法解决这些问题。然而，这些方法使用基于CNN的生成器，能够捕获局部语义，但缺乏捕获全局语义所需的长期依赖关系的能力。在本文中，我们提出了VCTR，一种用于非并行语音转换的高效方法，它利用混合感知块（HPB）和双重修剪自注意力（DPSA）以及基于对比学习的对抗方法。代码可在提供的URL中找到。",
    "subjects": [
      "Sound (cs.SD)"
    ],
    "update_time": "2025-10-16",
    "paper_authors": "Maharnab Saikia",
    "topic": [
      "Speech Synthesis",
      "Audio Representation Learning"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "MotionBeat: Motion-Aligned Music Representation via Embodied Contrastive Learning and Bar-Equivariant Contact-Aware Encoding",
    "paper_title_zh": "MotionBeat: 基于具身对比学习和节拍等变接触感知编码的运动对齐音乐表示",
    "paper_id": "2510.13244",
    "paper_abstract": "Music is both an auditory and an embodied phenomenon, closely linked to human motion and naturally expressed through dance. However, most existing audio representations neglect this embodied dimension, limiting their ability to capture rhythmic and structural cues that drive movement. We propose MotionBeat, a framework for motion-aligned music representation learning. MotionBeat is trained with two newly proposed objectives: the Embodied Contrastive Loss (ECL), an enhanced InfoNCE formulation with tempo-aware and beat-jitter negatives to achieve fine-grained rhythmic discrimination, and the Structural Rhythm Alignment Loss (SRAL), which ensures rhythm consistency by aligning music accents with corresponding motion events. Architecturally, MotionBeat introduces bar-equivariant phase rotations to capture cyclic rhythmic patterns and contact-guided attention to emphasize motion events synchronized with musical accents. Experiments show that MotionBeat outperforms state-of-the-art audio encoders in music-to-dance generation and transfers effectively to beat tracking, music tagging, genre and instrument classification, emotion recognition, and audio-visual retrieval. Our project demo page: this https URL.",
    "paper_abstract_zh": "音乐既是一种听觉现象，也是一种具身现象，与人体运动密切相关，并通过舞蹈自然表达。然而，大多数现有的音频表示忽略了这种具身维度，限制了它们捕捉驱动运动的节奏和结构线索的能力。我们提出了MotionBeat，一个用于运动对齐音乐表示学习的框架。MotionBeat通过两个新提出的目标进行训练：具身对比损失（ECL），这是一种增强的InfoNCE公式，具有节拍感知和节拍抖动负样本，以实现细粒度的节奏辨别；以及结构节奏对齐损失（SRAL），通过将音乐重音与相应的运动事件对齐来确保节奏一致性。在架构上，MotionBeat引入了节拍等变的相位旋转来捕获循环节奏模式，以及接触引导的注意力来强调与音乐重音同步的运动事件。实验表明，MotionBeat在音乐到舞蹈生成方面优于最先进的音频编码器，并有效地迁移到节拍跟踪、音乐标记、流派和乐器分类、情感识别以及音频-视觉检索任务中。我们的项目演示页面：this https URL。",
    "subjects": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Multimedia (cs.MM)"
    ],
    "update_time": "2025-10-16",
    "paper_authors": "Xuanchen Wang, Heng Wang, Weidong Cai",
    "topic": [
      "Audio Representation Learning",
      "Music Information Retrieval",
      "Music Generation"
    ],
    "category": [
      "Music"
    ]
  },
  {
    "paper_title": "UniMoE-Audio: Unified Speech and Music Generation with Dynamic-Capacity MoE",
    "paper_title_zh": "UniMoE-Audio：基于动态容量MoE的统一语音和音乐生成",
    "paper_id": "2510.13344",
    "paper_abstract": "Recent advances in unified multimodal models indicate a clear trend towards comprehensive content generation. However, the auditory domain remains a significant challenge, with music and speech often developed in isolation, hindering progress towards universal audio synthesis. This separation stems from inherent task conflicts and severe data imbalances, which impede the development of a truly unified audio generation model. To address this challenge, we propose UniMoE-Audio, a unified speech and music generation model within a novel Dynamic-Capacity Mixture-of-Experts (MoE) framework. Architecturally, UniMoE-Audio introduces a Top-P routing strategy for dynamic expert number allocation, and a hybrid expert design comprising routed experts for domain-specific knowledge, shared experts for domain-agnostic features, and null experts for adaptive computation skipping. To tackle data imbalance, we introduce a three-stage training curriculum: 1) Independent Specialist Training leverages original datasets to instill domain-specific knowledge into each \"proto-expert\" without interference; 2) MoE Integration and Warmup incorporates these specialists into the UniMoE-Audio architecture, warming up the gate module and shared expert using a subset of balanced dataset; and 3) Synergistic Joint Training trains the entire model end-to-end on the fully balanced dataset, fostering enhanced cross-domain synergy. Extensive experiments show that UniMoE-Audio not only achieves state-of-the-art performance on major speech and music generation benchmarks, but also demonstrates superior synergistic learning, mitigating the performance degradation typically seen in naive joint training. Our findings highlight the substantial potential of specialized MoE architecture and curated training strategies in advancing the field of universal audio generation. Homepage: this https URL",
    "paper_abstract_zh": "最近统一多模态模型的进展表明，全面内容生成是一个明显的趋势。然而，听觉领域仍然是一个重大挑战，音乐和语音通常独立开发，阻碍了通用音频合成的进展。这种分离源于固有的任务冲突和严重的数据不平衡，阻碍了真正统一音频生成模型的发展。为了应对这一挑战，我们提出了UniMoE-Audio，这是一个在新型动态容量专家混合(MoE)框架内的统一语音和音乐生成模型。在架构上，UniMoE-Audio引入了Top-P路由策略用于动态专家数量分配，以及混合专家设计，包括用于领域特定知识的路由专家、用于领域无关特征的共享专家，以及用于自适应计算跳过的空专家。为了解决数据不平衡问题，我们引入了一个三阶段训练课程：1) 独立专家训练利用原始数据集为每个'原型专家'灌输领域特定知识，避免干扰；2) MoE集成和预热将这些专家整合到UniMoE-Audio架构中，使用平衡数据集子集预热门模块和共享专家；3) 协同联合训练在完全平衡的数据集上端到端训练整个模型，促进增强的跨域协同。大量实验表明，UniMoE-Audio不仅在主要语音和音乐生成基准上取得了最先进的性能，还展示了优越的协同学习能力，减轻了朴素联合训练中常见的性能下降。我们的研究结果表明，专门的MoE架构和精心设计的训练策略在推进通用音频生成领域具有巨大潜力。",
    "subjects": [
      "Sound (cs.SD)",
      "Computation and Language (cs.CL)"
    ],
    "update_time": "2025-10-16",
    "paper_authors": "Zhenyu Liu, Yunxin Li, Xuanyu Zhang, Qixun Teng, Shenyuan Jiang, Xinyu Chen, Haoyuan Shi, Jinchao Li, Qi Wang, Haolan Chen, Fanbo Meng, Mingjun Zhao, Yu Xu, Yancheng He, Baotian Hu, Min Zhang",
    "topic": [
      "Speech Synthesis",
      "Music Generation"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Steer-MoE: Efficient Audio-Language Alignment with a Mixture-of-Experts Steering Module",
    "paper_title_zh": "Steer-MoE：使用专家混合转向模块实现高效的音频-语言对齐",
    "paper_id": "2510.13558",
    "paper_abstract": "Aligning pretrained audio encoders and Large Language Models (LLMs) offers a promising, parameter-efficient path to building powerful multimodal agents. However, existing methods often require costly full-model finetuning or rely on static adapters that may lack expressive power. Drawing inspiration from the Platonic Representation Hypothesis, we introduce SteerMoE, a novel and modular framework for audio-language alignment. SteerMoE freezes both the audio encoder and the LLM decoder, training only a lightweight steering module integrated within the encoder's layers. This module uses a Mixture-of-Experts (MoE) router to dynamically select and apply learned steering vectors, progressively transforming continuous audio representations into a space comprehensible to the LLM. By operating entirely in the continuous embedding space, our approach requires no modifications to the LLM's vocabulary and preserves its advanced reasoning and agentic capabilities. We demonstrate through experiments on ASR, audio understanding, and a qualitative function-calling task that SteerMoE achieves strong performance while remaining highly modular and computationally efficient, offering a robust new paradigm for developing sophisticated audio-language systems.",
    "paper_abstract_zh": "对齐预训练的音频编码器和大语言模型(LLMs)为构建强大的多模态代理提供了一条有前途的参数高效路径。然而，现有方法通常需要昂贵的大模型微调或依赖于可能缺乏表达能力的静态适配器。受柏拉图表征假设的启发，我们引入了SteerMoE，这是一个用于音频-语言对齐的新型模块化框架。SteerMoE冻结了音频编码器和LLM解码器，仅训练一个集成在编码器层中的轻量级转向模块。该模块使用专家混合(MoE)路由器动态选择和应用学习的转向向量，逐步将连续的音频表征转换为LLM可理解的空间。通过完全在连续嵌入空间中操作，我们的方法无需修改LLM的词汇表，并保留了其高级推理和代理能力。我们在ASR、音频理解和定性函数调用任务上的实验表明，SteerMoE在保持高度模块化和计算效率的同时实现了强大的性能，为开发复杂的音频-语言系统提供了新的稳健范式。",
    "subjects": [
      "Sound (cs.SD)"
    ],
    "update_time": "2025-10-16",
    "paper_authors": "Ruitao Feng, Bixi Zhang, Sheng Liang, Zheng Yuan",
    "topic": [
      "Audio Representation Learning",
      "Speech Recognition"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "A Critical Review of the Need for Knowledge-Centric Evaluation of Quranic Recitation",
    "paper_title_zh": "对《古兰经》诵读知识中心评估需求的批判性回顾",
    "paper_id": "2510.12858",
    "paper_abstract": "The sacred practice of Quranic recitation (Tajweed), governed by precise phonetic, prosodic, and theological rules, faces significant pedagogical challenges in the modern era. While digital technologies promise unprecedented access to education, automated tools for recitation evaluation have failed to achieve widespread adoption or pedagogical efficacy. This literature review investigates this critical gap, conducting a comprehensive analysis of academic research, web platforms, and commercial applications developed over the past two decades. Our synthesis reveals a fundamental misalignment in prevailing approaches that repurpose Automatic Speech Recognition (ASR) architectures, which prioritize lexical recognition over qualitative acoustic assessment and are plagued by data dependency, demographic biases, and an inability to provide diagnostically useful feedback. Critiquing these data--driven paradigms, we argue for a foundational paradigm shift towards a knowledge-centric computational framework. Capitalizing on the immutable nature of the Quranic text and the precisely defined rules of Tajweed, we propose that a robust evaluator must be architected around anticipatory acoustic modeling based on canonical rules and articulation points (Makhraj), rather than relying on statistical patterns learned from imperfect and biased datasets. This review concludes that the future of automated Quranic evaluation lies in hybrid systems that integrate deep linguistic knowledge with advanced audio analysis, offering a path toward robust, equitable, and pedagogically sound tools that can faithfully support learners worldwide.",
    "paper_abstract_zh": "《古兰经》诵读（Tajweed）这一神圣实践受精确的语音、韵律和神学规则约束，在现代教育时代面临重大的教学挑战。尽管数字技术承诺提供前所未有的教育机会，但用于诵读评估的自动化工具未能实现广泛采用或教学效果。本文献综述调查了这一关键差距，对过去二十年开发的学术研究、网络平台和商业应用进行了全面分析。我们的综合分析表明，现有方法存在根本性错位，这些方法重新利用了自动语音识别（ASR）架构，这些架构优先考虑词汇识别而非定性声学评估，并且受到数据依赖、人口统计偏差以及无法提供诊断性有用反馈的困扰。批判这些数据驱动范式，我们主张向知识中心计算框架进行基础范式转变。利用《古兰经》文本的不变性质和Tajweed的精确定义规则，我们提出一个强大的评估器必须围绕基于规范规则和发音点（Makhraj）的预期声学建模来构建，而不是依赖于从有缺陷和有偏见的数据集中学习到的统计模式。本综述得出结论，自动化《古兰经》评估的未来在于将深度语言知识与高级音频分析相结合的混合系统，为全球学习者提供强大、公平且教学有效的工具支持。",
    "subjects": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Sound (cs.SD)"
    ],
    "update_time": "2025-10-16",
    "paper_authors": "Mohammed Hilal Al-Kharusi, Khizar Hayat, Khalil Bader Al Ruqeishi, Haroon Rashid Lone",
    "topic": [
      "Speech Recognition",
      "Audio Classification"
    ],
    "category": [
      "Speech"
    ]
  }
]