[
  {
    "paper_title": "Learning Recursive Attenuation Filters Under Noisy Conditions",
    "paper_title_zh": "噪声条件下学习递归衰减滤波器",
    "paper_id": "2512.16318",
    "paper_abstract": "Recursion is a fundamental concept in the design of filters and audio systems. In particular, artificial reverberation systems that use delay networks depend on recursive paths to control both echo density and the decay rate of modal components. The differentiable digital signal processing framework has shown promise in automatically tuning both recursive and non-recursive elements given a target room impulse response. This is done by applying gradient descent to loss functions based on energy-decay or spectrogram differences. However, these representations are highly sensitive to background noise, which is ubiquitous in real measurements, producing spurious loss minima and leading to incorrect attenuation. This paper addresses the problem of tuning recursive attenuation filters of a feedback delay network when targets are noisy. We examine the loss landscape associated with different optimization objectives and propose a method that ensures correct minima under low signal-to-noise conditions. We demonstrate the effectiveness of the proposed approach through statistical analysis on 80 individual optimization examples. The results reveal that explicitly modeling the noise restores correct minima. Furthermore, we identify the sensitivity of attenuation filter parameters tuning to perturbations in frequency-independent parameters. These findings provide practical guidelines for more robust and reproducible gradient-based optimization of feedback delay networks.",
    "paper_abstract_zh": "递归是滤波器和音频系统设计中的基本概念。特别是，使用延迟网络的人工混响系统依赖于递归路径来控制回声密度和模态分量的衰减率。可微分数字信号处理框架在给定目标房间脉冲响应的情况下，自动调整递归和非递归元素方面显示出潜力。这是通过基于能量衰减或频谱图差异的损失函数应用梯度下降来实现的。然而，这些表示对背景噪声高度敏感，而背景噪声在实际测量中无处不在，会产生虚假的损失最小值并导致错误的衰减。本文解决了在目标存在噪声时调整反馈延迟网络的递归衰减滤波器的问题。我们研究了与不同优化目标相关的损失景观，并提出了一种在低信噪条件下确保正确最小值的方法。通过对80个单独优化示例进行统计分析，我们证明了所提出方法的有效性。结果表明，明确建模噪声可以恢复正确的最小值。此外，我们确定了衰减滤波器参数调整对频率无关参数扰动的敏感性。这些发现为反馈延迟网络的更稳健和可重现的基于梯度的优化提供了实用指南。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2025-12-19",
    "paper_authors": "Gloria Dal Santo, Karolina Prawda, Sebastian J. Schlecht, Vesa Välimäki",
    "topic": [
      "Audio Representation Learning",
      "Other"
    ],
    "category": [
      "Other"
    ]
  },
  {
    "paper_title": "BEST-STD2.0: Balanced and Efficient Speech Tokenizer for Spoken Term Detection",
    "paper_title_zh": "BEST-STD2.0：用于口语术语检测的平衡高效语音分词器",
    "paper_id": "2512.16395",
    "paper_abstract": "Fast and accurate spoken content retrieval is vital for applications such as voice search. Query-by-Example Spoken Term Detection (STD) involves retrieving matching segments from an audio database given a spoken query. Token-based STD systems, which use discrete speech representations, enable efficient search but struggle with robustness to noise and reverberation, and with inefficient token utilization. We address these challenges by proposing a noise and reverberation-augmented training strategy to improve tokenizer robustness. In addition, we introduce optimal transport-based regularization to ensure balanced token usage and enhance token efficiency. To further speed up retrieval, we adopt a TF-IDF-based search mechanism. Empirical evaluations demonstrate that the proposed method outperforms STD baselines across various distortion levels while maintaining high search efficiency.",
    "paper_abstract_zh": "快速准确的口语内容检索对语音搜索等应用至关重要。基于示例的口语术语检测(STD)涉及根据口语查询从音频数据库中检索匹配的语音片段。基于分词的STD系统使用离散语音表示，能够实现高效搜索，但在抗噪性和抗混响性方面表现不佳，且分词利用率低。我们提出了一种噪声和混响增强的训练策略来提高分词器的鲁棒性。此外，我们引入了基于最优传输的正则化方法，以确保分词的平衡使用并提高分词效率。为进一步加速检索，我们采用了基于TF-IDF的搜索机制。实证评估表明，在各种失真水平下，所提出的方法均优于STD基线，同时保持高搜索效率。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2025-12-19",
    "paper_authors": "Anup Singh, Kris Demuynck, Vipul Arora",
    "topic": [
      "Audio Representation Learning",
      "Speech Recognition"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Poster: Recognizing Hidden-in-the-Ear Private Key for Reliable Silent Speech Interface Using Multi-Task Learning",
    "paper_title_zh": "海报：使用多任务学习识别隐藏在耳中的私钥，以实现可靠的静默语音界面",
    "paper_id": "2512.16518",
    "paper_abstract": "Silent speech interface (SSI) enables hands-free input without audible vocalization, but most SSI systems do not verify speaker identity. We present HEar-ID, which uses consumer active noise-canceling earbuds to capture low-frequency \"whisper\" audio and high-frequency ultrasonic reflections. Features from both streams pass through a shared encoder, producing embeddings that feed a contrastive branch for user authentication and an SSI head for silent spelling recognition. This design supports decoding of 50 words while reliably rejecting impostors, all on commodity earbuds with a single model. Experiments demonstrate that HEar-ID achieves strong spelling accuracy and robust authentication.",
    "paper_abstract_zh": "静默语音界面(SSI) enables hands-free input without audible vocalization，但大多数SSI系统不验证说话人身份。我们提出了HEar-ID，它使用消费级主动降噪耳机捕获低频'whisper'音频和高频超声反射。来自两个流的特征通过共享编码器，生成嵌入向量，这些嵌入向量输入到一个对比分支用于用户认证，以及一个SSI头部用于静默拼写识别。该设计支持解码50个单词，同时可靠地拒绝冒名顶替者，所有这些都在普通耳机上使用单个模型完成。实验表明，HEar-ID实现了强大的拼写准确性和鲁棒的认证。",
    "subjects": [
      "Human-Computer Interaction (cs.HC)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2025-12-19",
    "paper_authors": "Xuefu Dong, Liqiang Xu, Lixing He, Zengyi Han, Ken Christofferson, Yifei Chen, Akihito Taya, Yuuki Nishiyama, Kaoru Sezaki",
    "topic": [
      "Speech Recognition",
      "Audio Classification"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Pseudo-Cepstrum: Pitch Modification for Mel-Based Neural Vocoders",
    "paper_title_zh": "伪倒谱：基于梅尔频谱图的神经声码器的音高修改",
    "paper_id": "2512.16519",
    "paper_abstract": "This paper introduces a cepstrum-based pitch modification method that can be applied to any mel-spectrogram representation. As a result, this method is compatible with any mel-based vocoder without requiring any additional training or changes to the model. This is achieved by directly modifying the cepstrum feature space in order to shift the harmonic structure to the desired target. The spectrogram magnitude is computed via the pseudo-inverse mel transform, then converted to the cepstrum by applying DCT. In this domain, the cepstral peak is shifted without having to estimate its position and the modified mel is recomputed by applying IDCT and mel-filterbank. These pitch-shifted mel-spectrogram features can be converted to speech with any compatible vocoder. The proposed method is validated experimentally with objective and subjective metrics on various state-of-the-art neural vocoders as well as in comparison with traditional pitch modification methods.",
    "paper_abstract_zh": "本文介绍了一种基于倒谱的音高修改方法，可应用于任何梅尔频谱图表示。因此，该方法与任何基于梅尔的声码器兼容，无需额外的训练或对模型进行任何更改。这是通过直接修改倒谱特征空间，将谐波结构移动到所需目标来实现的。频谱图幅度通过伪逆梅尔变换计算，然后通过应用DCT转换为倒谱。在此域中，倒谱峰值被移动而无需估计其位置，然后通过应用IDCT和梅尔滤波器组重新计算修改后的梅尔。这些音高调整后的梅尔频谱图特征可以使用任何兼容的声码器转换为语音。该方法在各种最先进的神经声码器上通过客观和主观指标进行了实验验证，并与传统的音高修改方法进行了比较。",
    "subjects": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2025-12-19",
    "paper_authors": "Nikolaos Ellinas, Alexandra Vioni, Panos Kakoulidis, Georgios Vamvoukakis, Myrsini Christidou, Konstantinos Markopoulos, Junkwang Oh, Gunu Jho, Inchul Hwang, Aimilios Chalamandaris, Pirros Tsiakoulis",
    "topic": [
      "Speech Synthesis",
      "Audio Representation Learning"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "From Minutes to Days: Scaling Intracranial Speech Decoding with Supervised Pretraining",
    "paper_title_zh": "从分钟到天：通过监督预训练扩展颅内语音解码",
    "paper_id": "2512.15830",
    "paper_abstract": "Decoding speech from brain activity has typically relied on limited neural recordings collected during short and highly controlled experiments. Here, we introduce a framework to leverage week-long intracranial and audio recordings from patients undergoing clinical monitoring, effectively increasing the training dataset size by over two orders of magnitude. With this pretraining, our contrastive learning model substantially outperforms models trained solely on classic experimental data, with gains that scale log-linearly with dataset size. Analysis of the learned representations reveals that, while brain activity represents speech features, its global structure largely drifts across days, highlighting the need for models that explicitly account for cross-day variability. Overall, our approach opens a scalable path toward decoding and modeling brain representations in both real-life and controlled task settings.",
    "paper_abstract_zh": "从脑活动中解码语音通常依赖于在短暂且高度受控的实验中收集的有限神经记录。在这里，我们引入了一个框架，利用接受临床监测的患者为期一周的颅内和音频记录，有效地将训练数据集的大小增加了两个数量级以上。通过这种预训练，我们的对比学习模型明显优于仅基于经典实验数据训练的模型，且性能提升与数据集大小呈对数线性关系。对学习到的表征的分析表明，虽然脑活动代表语音特征，但其全局结构在几天内会发生很大变化，这突显了需要能够明确解释跨日变化的模型。总体而言，我们的方法为在现实生活和受控任务环境中解码和建模大脑表征提供了一条可扩展的路径。",
    "subjects": [
      "Sound (cs.SD)",
      "Computation and Language (cs.CL)",
      "Neurons and Cognition (q-bio.NC)"
    ],
    "update_time": "2025-12-19",
    "paper_authors": "Linnea Evanson, Mingfang, Zhang, Hubert Banville, Saarang Panchavati, Pierre Bourdillon, Jean-Rémi King",
    "topic": [
      "Speech Recognition",
      "Audio Representation Learning"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Domain-Agnostic Causal-Aware Audio Transformer for Infant Cry Classification",
    "paper_title_zh": "领域无关因果感知音频Transformer用于婴儿哭声分类",
    "paper_id": "2512.16271",
    "paper_abstract": "Accurate and interpretable classification of infant cry paralinguistics is essential for early detection of neonatal distress and clinical decision support. However, many existing deep learning methods rely on correlation-driven acoustic representations, which makes them vulnerable to noise, spurious cues, and domain shifts across recording environments. We propose DACH-TIC, a Domain-Agnostic Causal-Aware Hierarchical Audio Transformer for robust infant cry classification. The model integrates causal attention, hierarchical representation learning, multi-task supervision, and adversarial domain generalization within a unified framework.\nDACH-TIC employs a structured transformer backbone with local token-level and global semantic encoders, augmented by causal attention masking and controlled perturbation training to approximate counterfactual acoustic variations. A domain-adversarial objective promotes environment-invariant representations, while multi-task learning jointly optimizes cry type recognition, distress intensity estimation, and causal relevance prediction. The model is evaluated on the Baby Chillanto and Donate-a-Cry datasets, with ESC-50 environmental noise overlays for domain augmentation.\nExperimental results show that DACH-TIC outperforms state-of-the-art baselines, including HTS-AT and SE-ResNet Transformer, achieving improvements of 2.6 percent in accuracy and 2.2 points in macro-F1 score, alongside enhanced causal fidelity. The model generalizes effectively to unseen acoustic environments, with a domain performance gap of only 2.4 percent, demonstrating its suitability for real-world neonatal acoustic monitoring systems.",
    "paper_abstract_zh": "准确且可解释的婴儿副语言哭声分类对于新生儿早期痛苦检测和临床决策支持至关重要。然而，许多现有的深度学习方法依赖于相关性驱动的声学表示，这使得它们容易受到噪声、虚假线索和不同录制环境间领域转移的影响。我们提出了DACH-TIC，一种用于鲁棒婴儿哭声分类的领域无关因果感知分层音频Transformer。该模型在一个统一框架内集成了因果注意力、分层表示学习、多任务监督和对抗性领域泛化。DACH-TIC采用结构化的Transformer骨干网络，包含局部令牌级和全局语义编码器，并通过因果注意力掩码和受控扰动训练来近似反事实声学变化。领域对抗性目标促进环境不变表示，而多任务学习联合优化哭声类型识别、痛苦强度估计和因果相关性预测。该模型在Baby Chillanto和Donate-a-Cry数据集上进行了评估，并使用ESC-50环境噪声叠加进行领域增强。实验结果表明，DACH-TIC优于最先进的基线方法，包括HTS-AT和SE-ResNet Transformer，在准确率上提高了2.6个百分点，在宏F1分数上提高了2.2分，同时增强了因果保真度。该模型能有效泛化到未见过的声学环境，领域性能差距仅为2.4%，证明了其适用于现实世界新生儿声学监测系统。",
    "subjects": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)"
    ],
    "update_time": "2025-12-19",
    "paper_authors": "Geofrey Owino, Bernard Shibwabo Kasamani, Ahmed M. Abdelmoniem, Edem Wornyo",
    "topic": [
      "Audio Representation Learning",
      "Audio Classification"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "CogSR: Semantic-Aware Speech Super-Resolution via Chain-of-Thought Guided Flow Matching",
    "paper_title_zh": "CogSR：基于思维链引导的流匹配的语义感知语音超分辨率",
    "paper_id": "2512.16304",
    "paper_abstract": "Applying speech super-resolution (SR) to recordings with severely low sampling rates is a critical challenge in digital archiving and investigative audio recovery. In these scenarios, the input lacks essential acoustic cues. Consequently, existing generative models often fail; without sufficient context, they hallucinate phonetic content, guessing words based on probability rather than meaning.\nTo address this, we propose CogSR, a framework designed specifically for high-precision, offline restoration. Our approach shifts the focus from simple signal mapping to cognitive reconstruction. By integrating a Large Audio-Language Model, we employ Chain-of-Thought reasoning to act as a semantic anchor, while explicit acoustic priors ensure the speaker's identity remains consistent. This guides a Rectified Flow backbone to synthesize high-frequency details that are not only realistic but linguistically accurate. Evaluations show that CogSR effectively eliminates ambiguity in severe degradation regimes, making it a robust solution for restoring high-value legacy and surveillance audio.",
    "paper_abstract_zh": "将语音超分辨率（SR）应用于采样率极低的录音是数字存档和调查音频恢复中的一个关键挑战。在这些场景中，输入缺乏必要的声学线索。因此，现有的生成模型常常失效；在没有足够上下文的情况下，它们会产生语音幻觉，基于概率而非意义猜测单词。为解决这一问题，我们提出了CogSR，这是一个专为高精度离线恢复而设计的框架。我们的方法将重点从简单的信号映射转向认知重建。通过集成大型音频语言模型，我们采用思维链推理作为语义锚点，同时显式声学先验确保说话人身份保持一致。这引导修正流骨干网络合成不仅逼真而且语言准确的高频细节。评估表明，CogSR能有效消除严重退化情况下的歧义，使其成为恢复高价值遗留音频和监控音频的稳健解决方案。",
    "subjects": [
      "Sound (cs.SD)"
    ],
    "update_time": "2025-12-19",
    "paper_authors": "Jiajun Yuan, Xiaochen Wang, Yuhang Xiao, Yulin Wu, Chenhao Hu, Xueyang Lv",
    "topic": [
      "Speech Enhancement",
      "Speech Synthesis"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "DPDFNet: Boosting DeepFilterNet2 via Dual-Path RNN",
    "paper_title_zh": "",
    "paper_id": "2512.16420",
    "paper_abstract": "We present DPDFNet, a causal single-channel speech enhancement model that extends DeepFilterNet2 architecture with dual-path blocks in the encoder, strengthening long-range temporal and cross-band modeling while preserving the original enhancement framework. In addition, we demonstrate that adding a loss component to mitigate over-attenuation in the enhanced speech, combined with a fine-tuning phase tailored for \"always-on\" applications, leads to substantial improvements in overall model performance. To compare our proposed architecture with a variety of causal open-source models, we created a new evaluation set comprising long, low-SNR recordings in 12 languages across everyday noise scenarios, better reflecting real-world conditions than commonly used benchmarks. On this evaluation set, DPDFNet delivers superior performance to other causal open-source models, including some that are substantially larger and more computationally demanding. We also propose an holistic metric named PRISM, a composite, scale-normalized aggregate of intrusive and non-intrusive metrics, which demonstrates clear scalability with the number of dual-path blocks. We further demonstrate on-device feasibility by deploying DPDFNet on Ceva-NeuPro-Nano edge NPUs. Results indicate that DPDFNet-4, our second-largest model, achieves real-time performance on NPN32 and runs even faster on NPN64, confirming that state-of-the-art quality can be sustained within strict embedded power and latency constraints.",
    "paper_abstract_zh": "",
    "subjects": [
      "Sound (cs.SD)"
    ],
    "update_time": "2025-12-19",
    "paper_authors": "Daniel Rika, Nino Sapir, Ido Gus",
    "topic": [],
    "category": []
  },
  {
    "paper_title": "Hearing to Translate: The Effectiveness of Speech Modality Integration into LLMs",
    "paper_title_zh": "听而译：语音模态集成到大型语言模型的有效性",
    "paper_id": "2512.16378",
    "paper_abstract": "As Large Language Models (LLMs) expand beyond text, integrating speech as a native modality has given rise to SpeechLLMs, which aim to translate spoken language directly, thereby bypassing traditional transcription-based pipelines. Whether this integration improves speech-to-text translation quality over established cascaded architectures, however, remains an open question. We present Hearing to Translate, the first comprehensive test suite rigorously benchmarking 5 state-of-the-art SpeechLLMs against 16 strong direct and cascade systems that couple leading speech foundation models (SFM), with multilingual LLMs. Our analysis spans 16 benchmarks, 13 language pairs, and 9 challenging conditions, including disfluent, noisy, and long-form speech. Across this extensive evaluation, we find that cascaded systems remain the most reliable overall, while current SpeechLLMs only match cascades in selected settings and SFMs lag behind both, highlighting that integrating an LLM, either within the model or in a pipeline, is essential for high-quality speech translation.",
    "paper_abstract_zh": "随着大型语言模型（LLMs）超越文本范畴，将语音作为原生模态进行集成催生了SpeechLLMs，其目标直接翻译口语，从而绕过传统的基于转录的流水线。然而，这种集成是否比既定的级联架构提高了语音到文本的翻译质量，仍然是一个悬而未决的问题。我们提出了'听而译'（Hearing to Translate），这是首个全面的测试套件，严格地将5个最先进的SpeechLLMs与16个强大的直接和级联系统进行了基准对比，这些系统结合了领先的语音基础模型（SFM）和多语言LLMs。我们的分析涵盖了16个基准测试、13种语言对以及9种具有挑战性的条件，包括不流畅、嘈杂和长篇语音。在这广泛的评估中，我们发现级联系统仍然是最可靠的，而当前的SpeechLLMs仅在特定设置中与级联系统相匹配，SFM则落后于两者。这表明，无论是在模型内部还是在流水线中集成LLM，对于高质量的语音翻译都是必不可少的。",
    "subjects": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Sound (cs.SD)"
    ],
    "update_time": "2025-12-19",
    "paper_authors": "Sara Papi, Javier Garcia Gilabert, Zachary Hopton, Vilém Zouhar, Carlos Escolano, Gerard I. Gállego, Jorge Iranzo-Sánchez, Ahrii Kim, Dominik Macháček, Patricia Schmidtova, Maike Züfle",
    "topic": [
      "Speech Recognition",
      "Other"
    ],
    "category": [
      "Speech"
    ]
  }
]