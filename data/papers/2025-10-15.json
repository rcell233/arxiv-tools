[
  {
    "paper_title": "FakeMark: Deepfake Speech Attribution With Watermarked Artifacts",
    "paper_title_zh": "FakeMark: 基于水印伪迹的深度伪造语音溯源",
    "paper_id": "2510.12042",
    "paper_abstract": "Deepfake speech attribution remains challenging for existing solutions. Classifier-based solutions often fail to generalize to domain-shifted samples, and watermarking-based solutions are easily compromised by distortions like codec compression or malicious removal attacks. To address these issues, we propose FakeMark, a novel watermarking framework that injects artifact-correlated watermarks associated with deepfake systems rather than pre-assigned bitstring messages. This design allows a detector to attribute the source system by leveraging both injected watermark and intrinsic deepfake artifacts, remaining effective even if one of these cues is elusive or removed. Experimental results show that FakeMark improves generalization to cross-dataset samples where classifier-based solutions struggle and maintains high accuracy under various distortions where conventional watermarking-based solutions fail.",
    "paper_abstract_zh": "深度伪造语音溯源对现有解决方案来说仍然具有挑战性。基于分类器的解决方案通常难以推广到域偏移样本，而基于水印的解决方案容易被编解码压缩或恶意移除攻击等失真所破坏。为解决这些问题，我们提出了FakeMark，一种新颖的水印框架，它注入与深度伪造系统相关的伪迹相关水印，而不是预分配的比特串消息。这种设计允许检测器利用注入的水印和内在的深度伪造伪迹来溯源源系统，即使其中一个线索难以捉摸或被移除，仍然有效。实验结果表明，FakeMark提高了对跨数据集样本的泛化能力，而基于分类器的解决方案在这方面表现不佳，并且在各种失真条件下保持高精度，而传统基于水印的解决方案在这些条件下会失效。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2025-10-15",
    "paper_authors": "Wanying Ge, Xin Wang, Junichi Yamagishi",
    "topic": [
      "Speech Synthesis",
      "Audio Classification"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "DiSTAR: Diffusion over a Scalable Token Autoregressive Representation for Speech Generation",
    "paper_title_zh": "DiSTAR: 基于可扩展标记自回归表示的扩散语音生成",
    "paper_id": "2510.12210",
    "paper_abstract": "Recent attempts to interleave autoregressive (AR) sketchers with diffusion-based refiners over continuous speech representations have shown promise, but they remain brittle under distribution shift and offer limited levers for controllability. We introduce DISTAR, a zero-shot text-to-speech framework that operates entirely in a discrete residual vector quantization (RVQ) code space and tightly couples an AR language model with a masked diffusion model, without forced alignment or a duration predictor. Concretely, DISTAR drafts block-level RVQ tokens with an AR language model and then performs parallel masked-diffusion infilling conditioned on the draft to complete the next block, yielding long-form synthesis with blockwise parallelism while mitigating classic AR exposure bias. The discrete code space affords explicit control at inference: DISTAR produces high-quality audio under both greedy and sample-based decoding using classifier-free guidance, supports trade-offs between robustness and diversity, and enables variable bit-rate and controllable computation via RVQ layer pruning at test time. Extensive experiments and ablations demonstrate that DISTAR surpasses state-of-the-art zero-shot TTS systems in robustness, naturalness, and speaker/style consistency, while maintaining rich output diversity. Audio samples are provided on this https URL.",
    "paper_abstract_zh": "最近尝试将自回归(AR)草图生成器与基于扩散的细化器在连续语音表示上交错进行，显示出一定的前景，但它们在分布偏移下仍然脆弱，且提供有限的控制杠杆。我们引入了DISTAR，一个零样本文本到语音框架，完全在离散残差向量量化(RVQ)码空间中运行，并将AR语言模型与掩码扩散模型紧密耦合，无需强制对齐或持续时间预测。具体而言，DISTAR使用AR语言模型生成块级RVQ标记，然后在草稿条件下执行并行掩码扩散填充以完成下一个块，从而实现块级并行的长形式合成，同时减轻了经典AR的暴露偏差。离散码空间在推理时提供显式控制：DISTAR在贪心解码和基于采样的解码下都能产生高质量音频，支持鲁棒性和多样性之间的权衡，并通过测试时的RVQ层剪枝实现可变比特率和可控计算。大量实验和消融研究表明，DISTAR在鲁棒性、自然度和说话者/风格一致性方面超越了最先进的零样本TTS系统，同时保持了丰富的输出多样性。音频样本可在https URL上获取。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "update_time": "2025-10-15",
    "paper_authors": "Yakun Song, Xiaobin Zhuang, Jiawei Chen, Zhikang Niu, Guanrou Yang, Chenpeng Du, Zhuo Chen, Yuping Wang, Yuxuan Wang, Xie Chen",
    "topic": [
      "Speech Synthesis",
      "Audio Representation Learning"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "DeePAQ: A Perceptual Audio Quality Metric Based On Foundational Models and Weakly Supervised Learning",
    "paper_title_zh": "DeePAQ: 一种基于基础模型和弱监督学习的感知音频质量度量",
    "paper_id": "2510.12326",
    "paper_abstract": "This paper presents the Deep learning-based Perceptual Audio Quality metric (DeePAQ) for evaluating general audio quality. Our approach leverages metric learning together with the music foundation model MERT, guided by surrogate labels, to construct an embedding space that captures distortion intensity in general audio. To the best of our knowledge, DeePAQ is the first in the general audio quality domain to leverage weakly supervised labels and metric learning for fine-tuning a music foundation model with Low-Rank Adaptation (LoRA), a direction not yet explored by other state-of-the-art methods. We benchmark the proposed model against state-of-the-art objective audio quality metrics across listening tests spanning audio coding and source separation. Results show that our method surpasses existing metrics in detecting coding artifacts and generalizes well to unseen distortions such as source separation, highlighting its robustness and versatility.",
    "paper_abstract_zh": "本文提出了基于深度学习的感知音频质量度量(DeePAQ)，用于评估通用音频质量。我们的方法结合了度量学习和音乐基础模型MERT，在代理标签的指导下构建了一个嵌入空间，以捕捉通用音频中的失真强度。据我们所知，DeePAQ是首个在通用音频质量领域利用弱监督标签和度量学习来微调音乐基础模型的方法，采用低秩适应(LoRA)技术，这是其他最先进方法尚未探索的方向。我们在涵盖音频编码和源分离的听力测试中，将所提模型与最先进的目标音频质量度量进行了基准测试。结果表明，我们的方法在检测编码伪影方面优于现有度量，并且对源分离等未见过的失真具有良好的泛化能力，凸显了其鲁棒性和多功能性。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2025-10-15",
    "paper_authors": "Guanxin Jiang, Andreas Brendel, Pablo M. Delgado, Jürgen Herre",
    "topic": [
      "Audio Representation Learning",
      "Audio Codec"
    ],
    "category": [
      "Music"
    ]
  },
  {
    "paper_title": "A Phase Synthesizer for Decorrelation to Improve Acoustic Feedback Cancellation",
    "paper_title_zh": "一种用于解相关以提高声学反馈消除的相位合成器",
    "paper_id": "2510.12377",
    "paper_abstract": "Undesired acoustic feedback is a known issue in communication systems, such as speech in-car communication, public address systems, or hearing aids. Without additional precautions, there is a high risk that the adaptive filter - intended to cancel the feedback path - also suppresses parts of the desired signal. One solution is to decorrelate the loudspeaker and microphone signals. In this work, we combine the two decorrelation approaches frequency shifting and phase modulation in a unified framework: a so-called \\textit{phase synthesizer}, implemented in a discrete Fourier transform (DFT) filter bank. Furthermore, we extend the phase modulation technique using variable delay lines, as known from vibrato and chorus effects. We demonstrate the benefits of the proposed phase synthesizer using an example from speech in-car communication, employing an adaptive frequency-domain Kalman filter. Improvements in system stability, speech quality measured by perceptual evaluation of speech quality (PESQ) are presented.",
    "paper_abstract_zh": "不期望的声学反馈是通信系统中的一个已知问题，例如车载通信、公共广播系统或助听器中的语音通信。如果没有额外的预防措施，自适应滤波器（旨在消除反馈路径）有很大风险也会抑制部分期望信号。一种解决方案是对扬声器和麦克风信号进行解相关。在这项工作中，我们将频移和相位调制这两种解相关方法在一个统一的框架中结合：所谓的相位合成器，在离散傅里叶变换（DFT）滤波器组中实现。此外，我们使用可变延迟线扩展了相位调制技术，这些延迟线在颤音和合唱效果中广为人知。我们通过车载通信中的一个例子展示了所提出的相位合成器的优势，该例子采用了自适应频域卡尔曼滤波器。展示了系统稳定性和通过语音质量感知评估（PESQ）测量的语音质量的改进。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2025-10-15",
    "paper_authors": "Klaus Linhard, Philipp Bulling",
    "topic": [
      "Speech Enhancement",
      "Other"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "I-DCCRN-VAE: An Improved Deep Representation Learning Framework for Complex VAE-based Single-channel Speech Enhancement",
    "paper_title_zh": "I-DCCRN-VAE: 一种用于复杂VAE单通道语音增强的改进深度表示学习框架",
    "paper_id": "2510.12485",
    "paper_abstract": "Recently, a complex variational autoencoder (VAE)-based single-channel speech enhancement system based on the DCCRN architecture has been proposed. In this system, a noise suppression VAE (NSVAE) learns to extract clean speech representations from noisy speech using pretrained clean speech and noise VAEs with skip connections. In this paper, we improve DCCRN-VAE by incorporating three key modifications: 1) removing the skip connections in the pretrained VAEs to encourage more informative speech and noise latent representations; 2) using $\\beta$-VAE in pretraining to better balance reconstruction and latent space regularization; and 3) a NSVAE generating both speech and noise latent representations. Experiments show that the proposed system achieves comparable performance as the DCCRN and DCCRN-VAE baselines on the matched DNS3 dataset but outperforms the baselines on mismatched datasets (WSJ0-QUT, Voicebank-DEMEND), demonstrating improved generalization ability. In addition, an ablation study shows that a similar performance can be achieved with classical fine-tuning instead of adversarial training, resulting in a simpler training pipeline.",
    "paper_abstract_zh": "最近，基于DCCRN架构的复杂变分自编码器(VAE)单通道语音增强系统已被提出。在该系统中，噪声抑制VAE (NSVAE)利用预训练的干净语音和噪声VAE以及跳跃连接，从含噪语音中学习提取干净语音表示。本文通过引入三个关键改进来改进DCCRN-VAE：1)移除预训练VAE中的跳跃连接，以鼓励更具信息性的语音和噪声潜在表示；2)在预训练中使用β-VAE，以更好地平衡重建和潜在空间正则化；3)NSVAE同时生成语音和噪声潜在表示。实验表明，所提出的系统在匹配的DNS3数据集上与DCCRN和DCCRN-VAE基线性能相当，但在不匹配的数据集(WSJ0-QUT, Voicebank-DEMEND)上优于基线，显示出更好的泛化能力。此外，消融研究表明，可以使用经典微调而非对抗训练实现类似性能，从而简化训练流程。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2025-10-15",
    "paper_authors": "Jiatong Li, Simon Doclo",
    "topic": [
      "Audio Representation Learning",
      "Speech Enhancement"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Serial-Parallel Dual-Path Architecture for Speaking Style Recognition",
    "paper_title_zh": "串行并行双路径架构用于说话风格识别",
    "paper_id": "2510.11732",
    "paper_abstract": "Speaking Style Recognition (SSR) identifies a speaker's speaking style characteristics from speech. Existing style recognition approaches primarily rely on linguistic information, with limited integration of acoustic information, which restricts recognition accuracy improvements. The fusion of acoustic and linguistic modalities offers significant potential to enhance recognition performance. In this paper, we propose a novel serial-parallel dual-path architecture for SSR that leverages acoustic-linguistic bimodal information. The serial path follows the ASR+STYLE serial paradigm, reflecting a sequential temporal dependency, while the parallel path integrates our designed Acoustic-Linguistic Similarity Module (ALSM) to facilitate cross-modal interaction with temporal simultaneity. Compared to the existing SSR baseline -- the OSUM model, our approach reduces parameter size by 88.4% and achieves a 30.3% improvement in SSR accuracy for eight styles on the test set.",
    "paper_abstract_zh": "说话风格识别(SSR)从语音中识别说话者的风格特征。现有的风格识别方法主要依赖语言信息，对声学信息的整合有限，这限制了识别准确率的提升。声学和语言学模态的融合为增强识别性能提供了巨大潜力。在本文中，我们提出了一种用于SSR的新型串行并行双路径架构，该架构利用声学-语言学双模态信息。串行路径遵循ASR+STYLE串行范式，反映顺序时间依赖性，而并行路径集成了我们设计的声学-语言学相似度模块(ALSM)，以促进具有时间同时性的跨模态交互。与现有的SSR基线模型OSUM相比，我们的方法将参数大小减少了88.4%，并在测试集上对八种风格的SSR准确率实现了30.3%的提升。",
    "subjects": [
      "Artificial Intelligence (cs.AI)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2025-10-15",
    "paper_authors": "Guojian Li, Qijie Shao, Zhixian Zhao, Shuiyuan Wang, Zhonghua Fu, Lei Xie",
    "topic": [
      "Speech Recognition",
      "Audio Classification"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Audio Palette: A Diffusion Transformer with Multi-Signal Conditioning for Controllable Foley Synthesis",
    "paper_title_zh": "音频调色板：一种用于可控 Foley 合成的多信号条件扩散 Transformer",
    "paper_id": "2510.12175",
    "paper_abstract": "Recent advances in diffusion-based generative models have enabled high-quality text-to-audio synthesis, but fine-grained acoustic control remains a significant challenge in open-source research. We present Audio Palette, a diffusion transformer (DiT) based model that extends the Stable Audio Open architecture to address this \"control gap\" in controllable audio generation. Unlike prior approaches that rely solely on semantic conditioning, Audio Palette introduces four time-varying control signals: loudness, pitch, spectral centroid, and timbre, for precise and interpretable manipulation of acoustic features. The model is efficiently adapted for the nuanced domain of Foley synthesis using Low-Rank Adaptation (LoRA) on a curated subset of AudioSet, requiring only 0.85 percent of the original parameters to be trained. Experiments demonstrate that Audio Palette achieves fine-grained, interpretable control of sound attributes. Crucially, it accomplishes this novel controllability while maintaining high audio quality and strong semantic alignment to text prompts, with performance on standard metrics such as Frechet Audio Distance (FAD) and LAION-CLAP scores remaining comparable to the original baseline model. We provide a scalable, modular pipeline for audio research, emphasizing sequence-based conditioning, memory efficiency, and a three-scale classifier-free guidance mechanism for nuanced inference-time control. This work establishes a robust foundation for controllable sound design and performative audio synthesis in open-source settings, enabling a more artist-centric workflow.",
    "paper_abstract_zh": "基于扩散的生成模型的最新进展实现了高质量文本到音频的合成，但在开源研究中，细粒度的声学控制仍然是一个重大挑战。我们提出了 Audio Palette，这是一种基于扩散 Transformer（DiT）的模型，它扩展了 Stable Audio Open 架构，以解决可控音频生成中的“控制差距”。与仅依赖语义条件的前期方法不同，Audio Palette 引入了四个时变控制信号：响度、音高、频谱质心和音色，用于对声学特征进行精确且可解释的操作。该模型使用精心挑选的 AudioSet 子集通过低秩适应（LoRA）进行高效调整，以适应 Foley 合成的细微领域，仅需训练原始参数的 0.85%。实验表明，Audio Palette 实现了对声音属性的细粒度、可解释控制。关键的是，它在保持高音频质量和与文本提示的强语义对齐的同时，实现了这种新颖的可控性，在 Frechet 音频距离（FAD）和 LAION-CLAP 等标准指标上的性能与原始基线模型相当。我们为音频研究提供了一个可扩展、模块化的流程，强调基于序列的条件设置、内存效率和三尺度无分类器引导机制，用于细微的推理时控制。这项工作为开源环境中的可控声音设计和表演性音频合成奠定了坚实的基础，使工作流程更加以艺术家为中心。",
    "subjects": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2025-10-15",
    "paper_authors": "Junnuo Wang",
    "topic": [
      "Music Generation",
      "Audio Representation Learning"
    ],
    "category": [
      "Music"
    ]
  },
  {
    "paper_title": "SeeingSounds: Learning Audio-to-Visual Alignment via Text",
    "paper_title_zh": "",
    "paper_id": "2510.11738",
    "paper_abstract": "We introduce SeeingSounds, a lightweight and modular framework for audio-to-image generation that leverages the interplay between audio, language, and vision-without requiring any paired audio-visual data or training on visual generative models. Rather than treating audio as a substitute for text or relying solely on audio-to-text mappings, our method performs dual alignment: audio is projected into a semantic language space via a frozen language encoder, and, contextually grounded into the visual domain using a vision-language model. This approach, inspired by cognitive neuroscience, reflects the natural cross-modal associations observed in human perception. The model operates on frozen diffusion backbones and trains only lightweight adapters, enabling efficient and scalable learning. Moreover, it supports fine-grained and interpretable control through procedural text prompt generation, where audio transformations (e.g., volume or pitch shifts) translate into descriptive prompts (e.g., \"a distant thunder\") that guide visual outputs. Extensive experiments across standard benchmarks confirm that SeeingSounds outperforms existing methods in both zero-shot and supervised settings, establishing a new state of the art in controllable audio-to-visual generation.",
    "paper_abstract_zh": "",
    "subjects": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Multimedia (cs.MM)"
    ],
    "update_time": "2025-10-15",
    "paper_authors": "Simone Carnemolla, Matteo Pennisi, Chiara Russo, Simone Palazzo, Daniela Giordano, Concetto Spampinato",
    "topic": [],
    "category": []
  },
  {
    "paper_title": "Audio-Guided Visual Perception for Audio-Visual Navigation",
    "paper_title_zh": "",
    "paper_id": "2510.11760",
    "paper_abstract": "Audio-Visual Embodied Navigation aims to enable agents to autonomously navigate to sound sources in unknown 3D environments using auditory cues. While current AVN methods excel on in-distribution sound sources, they exhibit poor cross-source generalization: navigation success rates plummet and search paths become excessively long when agents encounter unheard sounds or unseen environments. This limitation stems from the lack of explicit alignment mechanisms between auditory signals and corresponding visual regions. Policies tend to memorize spurious \\enquote{acoustic fingerprint-scenario} correlations during training, leading to blind exploration when exposed to novel sound sources. To address this, we propose the AGVP framework, which transforms sound from policy-memorable acoustic fingerprint cues into spatial guidance. The framework first extracts global auditory context via audio self-attention, then uses this context as queries to guide visual feature attention, highlighting sound-source-related regions at the feature level. Subsequent temporal modeling and policy optimization are then performed. This design, centered on interpretable cross-modal alignment and region reweighting, reduces dependency on specific acoustic fingerprints. Experimental results demonstrate that AGVP improves both navigation efficiency and robustness while achieving superior cross-scenario generalization on previously unheard sounds.",
    "paper_abstract_zh": "",
    "subjects": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Multimedia (cs.MM)"
    ],
    "update_time": "2025-10-15",
    "paper_authors": "Yi Wang, Yinfeng Yu, Fuchun Sun, Liejun Wang, Wendong Zheng",
    "topic": [],
    "category": []
  },
  {
    "paper_title": "UALM: Unified Audio Language Model for Understanding, Generation and Reasoning",
    "paper_title_zh": "UALM: 统一音频语言模型用于理解、生成和推理",
    "paper_id": "2510.12000",
    "paper_abstract": "Recent advances in the audio language modeling (ALM) domain tackle audio understanding and text-to-audio generation as separate tasks. Very few studies attempt to unify these tasks -- an essential step toward advanced multimodal reasoning. This paper introduces U}nified Audio Language Model (UALM), which aims to unify audio understanding, text-to-audio generation, and multimodal reasoning in a single model. To achieve this goal, we first present UALM-Gen, a text-to-audio language model that directly predicts audio tokens and is comparable to state-of-the-art diffusion-based models. We then demonstrate, using proper data blending, training recipes, and inference techniques, that our single UALM model matches the quality of state-of-the-art specialized models in audio understanding, text-to-audio generation, and text reasoning. Furthermore, we present UALM-Reason, a multimodal reasoning model that utilizes both text and audio in the intermediate thinking steps to facilitate complex generation tasks. To our knowledge, this is the first demonstration in audio research of cross-modal generative reasoning, with its effectiveness confirmed by subjective evaluations.",
    "paper_abstract_zh": "音频语言建模（ALM）领域的最新进展将音频理解和文本到音频生成作为独立任务处理。很少有研究尝试统一这些任务——这是迈向高级多模态推理的重要一步。本文介绍了统一音频语言模型（UALM），旨在在单一模型中统一音频理解、文本到音频生成和多模态推理。为实现这一目标，我们首先提出了UALM-Gen，一个直接预测音频标记的文本到音频语言模型，其性能可与最先进的基于扩散的模型相媲美。然后，通过适当的数据混合、训练配方和推理技术，我们展示了单一UALM模型在音频理解、文本到音频生成和文本推理方面的质量可与最先进的专业模型相匹配。此外，我们提出了UALM-Reason，一个多模态推理模型，在中间思维步骤中同时利用文本和音频，以促进复杂的生成任务。据我们所知，这是音频研究中跨模态生成推理的首次演示，其有效性已通过主观评估得到确认。",
    "subjects": [
      "Sound (cs.SD)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "update_time": "2025-10-15",
    "paper_authors": "Jinchuan Tian, Sang-gil Lee, Zhifeng Kong, Sreyan Ghosh, Arushi Goel, Chao-Han Huck Yang, Wenliang Dai, Zihan Liu, Hanrong Ye, Shinji Watanabe, Mohammad Shoeybi, Bryan Catanzaro, Rafael Valle, Wei Ping",
    "topic": [
      "Audio Representation Learning",
      "Music Generation",
      "Speech Synthesis"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "TFGA-Net: Temporal-Frequency Graph Attention Network for Brain-Controlled Speaker Extraction",
    "paper_title_zh": "TFGA-Net: 基于时频图注意力网络的脑控说话人提取",
    "paper_id": "2510.12275",
    "paper_abstract": "The rapid development of auditory attention decoding (AAD) based on electroencephalography (EEG) signals offers the possibility EEG-driven target speaker extraction. However, how to effectively utilize the target-speaker common information between EEG and speech remains an unresolved problem. In this paper, we propose a model for brain-controlled speaker extraction, which utilizes the EEG recorded from the listener to extract the target speech. In order to effectively extract information from EEG signals, we derive multi-scale time--frequency features and further incorporate cortical topological structures that are selectively engaged during the task. Moreover, to effectively exploit the non-Euclidean structure of EEG signals and capture their global features, the graph convolutional networks and self-attention mechanism are used in the EEG encoder. In addition, to make full use of the fused EEG and speech feature and preserve global context and capture speech rhythm and prosody, we introduce MossFormer2 which combines MossFormer and RNN-Free Recurrent as separator. Experimental results on both the public Cocktail Party and KUL dataset in this paper show that our TFGA-Net model significantly outper-forms the state-of-the-art method in certain objective evaluation metrics. The source code is available at: this https URL.",
    "paper_abstract_zh": "基于脑电图(EEG)信号的听觉注意解码(AAD)的快速发展为EEG驱动的目标说话人提取提供了可能性。然而，如何有效利用EEG和语音之间的目标说话人共同信息仍然是一个未解决的问题。在本文中，我们提出了一种脑控说话人提取模型，该模型利用记录的听者EEG来提取目标语音。为了有效提取EEG信号中的信息，我们推导了多尺度时频特征，并进一步整合了在任务选择性地参与期间的大脑皮层拓扑结构。此外，为了有效利用EEG信号的非欧几里得结构并捕获其全局特征，EEG编码器中使用了图卷积网络和自注意力机制。此外，为了充分利用融合的EEG和语音特征，并保留全局上下文并捕获语音节奏和韵律，我们引入了结合MossFormer和RNN-Free Recurrent的MossFormer2作为分离器。本文在公共的鸡尾酒会和KUL数据集上的实验结果表明，我们的TFGA-Net模型在某些客观评估指标上显著优于最先进的方法。源代码可在以下网址获取：this https URL。",
    "subjects": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)"
    ],
    "update_time": "2025-10-15",
    "paper_authors": "Youhao Si, Yuan Liao, Qiushi Han, Yuhang Yang, Rui Dai, Liya Huang",
    "topic": [
      "Speech Enhancement",
      "Audio Representation Learning"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Content Anonymization for Privacy in Long-form Audio",
    "paper_title_zh": "长音频内容匿名化以保护隐私",
    "paper_id": "2510.12780",
    "paper_abstract": "Voice anonymization techniques have been found to successfully obscure a speaker's acoustic identity in short, isolated utterances in benchmarks such as the VoicePrivacy Challenge. In practice, however, utterances seldom occur in isolation: long-form audio is commonplace in domains such as interviews, phone calls, and meetings. In these cases, many utterances from the same speaker are available, which pose a significantly greater privacy risk: given multiple utterances from the same speaker, an attacker could exploit an individual's vocabulary, syntax, and turns of phrase to re-identify them, even when their voice is completely disguised. To address this risk, we propose new content anonymization approaches. Our approach performs a contextual rewriting of the transcripts in an ASR-TTS pipeline to eliminate speaker-specific style while preserving meaning. We present results in a long-form telephone conversation setting demonstrating the effectiveness of a content-based attack on voice-anonymized speech. Then we show how the proposed content-based anonymization methods can mitigate this risk while preserving speech utility. Overall, we find that paraphrasing is an effective defense against content-based attacks and recommend that stakeholders adopt this step to ensure anonymity in long-form audio.",
    "paper_abstract_zh": "语音匿名化技术已被发现在基准测试（如VoicePrivacy Challenge）中能够成功掩盖说话者在简短、孤立的话语中的声学身份。然而，在实际应用中，话语很少孤立出现：长音频在访谈、电话和会议等领域中很常见。在这些情况下，同一说话者的多个话语可用，这带来了更大的隐私风险：给定同一说话者的多个话语，攻击者可以利用个人的词汇、语法和表达方式来重新识别他们，即使他们的声音被完全伪装。为解决这一风险，我们提出了新的内容匿名化方法。我们的方法在ASR-TTS流程中对转录文本进行上下文重写，以消除说话者特定风格，同时保留意义。我们在长篇电话对话场景中展示了结果，证明了基于内容的攻击对语音匿名化语音的有效性。然后，我们展示了所提出的内容匿名化方法如何能够缓解这一风险，同时保留语音的实用性。总体而言，我们发现释义是抵御基于内容攻击的有效防御，并建议利益相关者采取这一步骤以确保长音频的匿名性。",
    "subjects": [
      "Sound (cs.SD)",
      "Computation and Language (cs.CL)"
    ],
    "update_time": "2025-10-15",
    "paper_authors": "Cristina Aggazzotti, Ashi Garg, Zexin Cai, Nicholas Andrews",
    "topic": [
      "Speech Recognition",
      "Speech Synthesis"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Not in Sync: Unveiling Temporal Bias in Audio Chat Models",
    "paper_title_zh": "不同步：揭示音频聊天模型中的时间偏差",
    "paper_id": "2510.12185",
    "paper_abstract": "Large Audio Language Models (LALMs) are increasingly applied to audio understanding and multimodal reasoning, yet their ability to locate when events occur remains underexplored. We present the first systematic study of temporal bias in LALMs, revealing a key limitation in their timestamp prediction. For example, when asked \"At which second does the lecturer introduce the key formula?\", models often predict timestamps that are consistently earlier or later than the ground truth. Through controlled experiments on timestamped datasets, we find that temporal bias (i) is prevalent across datasets and models, (ii) increases with audio length - even accumulating to tens of seconds in extended recordings, and (iii) varies across event types and positions. We quantify this effect with the Temporal Bias Index (TBI), measuring systematic misalignment in predicted event timings, and complement it with a visualization framework. Our findings highlight a fundamental limitation in current LALMs and call for the development of temporally robust architectures.",
    "paper_abstract_zh": "大型音频语言模型（LALMs）越来越多地应用于音频理解和多模态推理，但它们定位事件发生时间的能力仍未得到充分探索。我们首次对LALMs中的时间偏差进行了系统性研究，揭示了其在时间戳预测方面的关键局限性。例如，当被问及'讲师在哪个秒数介绍了关键公式？'时，模型预测的时间戳往往比真实时间 consistently 更早或更晚。通过对带时间戳数据集的受控实验，我们发现时间偏差（i）在数据集和模型中普遍存在，（ii）随着音频长度增加而加剧——甚至在长录音中累积到几十秒，（iii）因事件类型和位置而异。我们通过时间偏差指数（TBI）量化了这一效应，衡量预测事件时间的系统性错位，并辅以可视化框架。我们的研究结果突显了当前LALMs的根本局限性，呼吁开发具有时间鲁棒性的架构。",
    "subjects": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)"
    ],
    "update_time": "2025-10-15",
    "paper_authors": "Jiayu Yao, Shenghua Liu, Yiwei Wang, Rundong Cheng, Lingrui Mei, Baolong Bi, Zhen Xiong, Xueqi Cheng",
    "topic": [
      "Speech Recognition",
      "Other"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Omni-Captioner: Data Pipeline, Models, and Benchmark for Omni Detailed Perception",
    "paper_title_zh": "Omni-Captioner：用于全方位详细感知的数据管道、模型和基准",
    "paper_id": "2510.12720",
    "paper_abstract": "Fine-grained perception of multimodal information is critical for advancing human-AI interaction. With recent progress in audio-visual technologies, Omni Language Models (OLMs), capable of processing audio and video signals in parallel, have emerged as a promising paradigm for achieving richer understanding and reasoning. However, their capacity to capture and describe fine-grained details remains limited explored. In this work, we present a systematic and comprehensive investigation of omni detailed perception from the perspectives of the data pipeline, models, and benchmark. We first identify an inherent \"co-growth\" between detail and hallucination in current OLMs. To address this, we propose Omni-Detective, an agentic data generation pipeline integrating tool-calling, to autonomously produce highly detailed yet minimally hallucinatory multimodal data. Based on the data generated with Omni-Detective, we train two captioning models: Audio-Captioner for audio-only detailed perception, and Omni-Captioner for audio-visual detailed perception. Under the cascade evaluation protocol, Audio-Captioner achieves the best performance on MMAU and MMAR among all open-source models, surpassing Gemini 2.5 Flash and delivering performance comparable to Gemini 2.5 Pro. On existing detailed captioning benchmarks, Omni-Captioner sets a new state-of-the-art on VDC and achieves the best trade-off between detail and hallucination on the video-SALMONN 2 testset. Given the absence of a dedicated benchmark for omni detailed perception, we design Omni-Cloze, a novel cloze-style evaluation for detailed audio, visual, and audio-visual captioning that ensures stable, efficient, and reliable assessment. Experimental results and analysis demonstrate the effectiveness of Omni-Detective in generating high-quality detailed captions, as well as the superiority of Omni-Cloze in evaluating such detailed captions.",
    "paper_abstract_zh": "多模态信息的细粒度感知对于推进人机交互至关重要。随着视听技术的最新进展，能够并行处理音频和视频信号的全能语言模型(OLMs)已成为实现更丰富理解和推理的有前景的范式。然而，它们捕获和描述细粒度细节的能力仍有限探索。在这项工作中，我们从数据管道、模型和基准的角度对全方位详细感知进行了系统而全面的研究。我们首先确定了当前OLMs中细节与幻觉之间的固有'共生'关系。为解决这一问题，我们提出了Omni-Detective，这是一个集成了工具调用的智能数据生成管道，能够自主生成高度详细且最少幻觉的多模态数据。基于使用Omni-Detective生成的数据，我们训练了两个字幕模型：用于纯音频详细感知的Audio-Captioner，以及用于视听详细感知的Omni-Captioner。在级联评估协议下，Audio-Captioner在所有开源模型中于MMAU和MMAR上取得了最佳性能，超越了Gemini 2.5 Flash，并达到了与Gemini 2.5 Pro相当的性能。在现有的详细字幕基准测试中，Omni-Captioner在VDC上设立了新的最先进水平，并在video-SALMONN 2测试集上实现了细节与幻觉之间的最佳权衡。鉴于缺乏专门用于全方位详细感知的基准，我们设计了Omni-Cloze，这是一种新颖的填空式评估方法，用于详细的音频、视觉和视听字幕，确保稳定、高效和可靠的评估。实验结果和分析证明了Omni-Detective在生成高质量详细字幕方面的有效性，以及Omni-Cloze在评估此类详细字幕方面的优越性。",
    "subjects": [
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Multimedia (cs.MM)",
      "Sound (cs.SD)"
    ],
    "update_time": "2025-10-15",
    "paper_authors": "Ziyang Ma, Ruiyang Xu, Zhenghao Xing, Yunfei Chu, Yuxuan Wang, Jinzheng He, Jin Xu, Pheng-Ann Heng, Kai Yu, Junyang Lin, Eng Siong Chng, Xie Chen",
    "topic": [
      "Audio Representation Learning",
      "Other"
    ],
    "category": [
      "Image&Video"
    ]
  }
]