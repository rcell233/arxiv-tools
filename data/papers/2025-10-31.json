[
  {
    "paper_title": "SPEAR: A Unified SSL Framework for Learning Speech and Audio Representations",
    "paper_title_zh": "SPEAR：用于学习语音和音频表示的统一自监督学习框架",
    "paper_id": "2510.25955",
    "paper_abstract": "Self-Supervised Learning (SSL) excels at learning generic representations of acoustic signals, yet prevailing methods remain domain-specific, tailored to either speech or general audio, hindering the development of a unified representation model with a comprehensive capability over both domains. To address this, we present SPEAR (SPEech and Audio Representations), the first SSL framework to successfully learn unified speech and audio representations from a mixture of speech and audio data. SPEAR proposes a unified pre-training objective based on masked prediction of fine-grained discrete tokens for both speech and general audio. These tokens are derived from continuous speech and audio representations using a Multi-codebook Vector Quantisation (MVQ) method, retaining rich acoustic detail essential for modelling both speech and complex audio events. SPEAR is applied to pre-train both single-domain and unified speech-and-audio SSL models. Our speech-domain model establishes a new state-of-the-art on the SUPERB benchmark, a speech processing benchmark for SSL models, matching or surpassing the highly competitive WavLM Large on 12 out of 15 tasks with the same pre-training corpora and a similar model size. Crucially, our unified model learns complementary features and demonstrates comprehensive capabilities across two major benchmarks, SUPERB and HEAR, for evaluating audio representations. By further scaling up the model size and pre-training data, we present a unified model with 600M parameters that excels in both domains, establishing it as one of the most powerful and versatile open-source SSL models for auditory understanding. The inference code and pre-trained models will be made publicly available.",
    "paper_abstract_zh": "自监督学习（SSL）在学习声学信号的通用表示方面表现出色，然而现有方法仍然是领域特定的，专门针对语音或通用音频，阻碍了具有全面跨领域能力的统一表示模型的发展。为此，我们提出了SPEAR（SPEech and Audio Representations），这是第一个成功从混合语音和音频数据中学习统一语音和音频表示的SSL框架。SPEAR提出了一种统一的预训练目标，基于对语音和通用音频的细粒度离散标记的掩码预测。这些标记通过多码本矢量量化（MVQ）方法从连续语音和音频表示中提取，保留了建模语音和复杂音频事件所需的关键声学细节。SPEAR被应用于预训练单领域和统一的语音-音频SSL模型。我们的语音领域模型在SUPERB基准测试（SSL模型的语音处理基准测试）上建立了新的最先进水平，使用相同的预训练语料库和相似模型规模，在15个任务中的12个上匹配或超越了极具竞争力的WavLM Large模型。关键的是，我们的统一模型学习互补特征，并在两个主要基准测试（SUPERB和HEAR）中展示了全面的音频表示能力。通过进一步扩大模型规模和预训练数据，我们提出了一个具有6亿参数的统一模型，该模型在两个领域都表现出色，使其成为听觉理解领域最强大和最通用的开源SSL模型之一。推理代码和预训练模型将公开提供。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2025-10-31",
    "paper_authors": "Xiaoyu Yang, Yifan Yang, Zengrui Jin, Ziyun Cui, Wen Wu, Baoxiang Li, Chao Zhang, Phil Woodland",
    "topic": [
      "Audio Representation Learning",
      "Speech Recognition"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "POWSM: A Phonetic Open Whisper-Style Speech Foundation Model",
    "paper_title_zh": "POWSM: 一种语音学开放Whisper风格语音基础模型",
    "paper_id": "2510.24992",
    "paper_abstract": "Recent advances in spoken language processing have led to substantial progress in phonetic tasks such as automatic speech recognition (ASR), phone recognition (PR), grapheme-to-phoneme conversion (G2P), and phoneme-to-grapheme conversion (P2G). Despite their conceptual similarity, these tasks have largely been studied in isolation, each relying on task-specific architectures and datasets. In this paper, we introduce POWSM (Phonetic Open Whisper-style Speech Model), the first unified framework capable of jointly performing multiple phone-related tasks. POWSM enables seamless conversion between audio, text (graphemes), and phones, opening up new possibilities for universal and low-resource speech processing. Our model outperforms or matches specialized PR models of similar size (Wav2Vec2Phoneme and ZIPA) while jointly supporting G2P, P2G, and ASR. Our training data, code and models are released to foster open science.",
    "paper_abstract_zh": "近期口语处理技术的进步在语音学任务（如自动语音识别(ASR)、音素识别(PR)、字符到音素转换(G2P)和音素到字符转换(P2G)）方面取得了显著进展。尽管这些任务在概念上具有相似性，但它们大多被孤立地研究，每个任务都依赖于特定的架构和数据集。本文介绍了POWSM（Phonetic Open Whisper-style Speech Model），这是第一个能够联合执行多个音素相关任务的统一框架。POWSM实现了音频、文本（字符）和音素之间的无缝转换，为通用和低资源语音处理开辟了新的可能性。我们的模型在性能上优于或匹敌类似规模的专用PR模型（Wav2Vec2Phoneme和ZIPA），同时支持G2P、P2G和ASR。我们已发布训练数据、代码和模型以促进开放科学。",
    "subjects": [
      "Computation and Language (cs.CL)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2025-10-31",
    "paper_authors": "Chin-Jou Li, Kalvin Chang, Shikhar Bharadwaj, Eunjung Yeo, Kwanghee Choi, Jian Zhu, David Mortensen, Shinji Watanabe",
    "topic": [
      "Speech Recognition",
      "Audio Representation Learning"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "SP-MCQA: Evaluating Intelligibility of TTS Beyond the Word Level",
    "paper_title_zh": "SP-MCQA：超越词级评估TTS的可懂度",
    "paper_id": "2510.26190",
    "paper_abstract": "The evaluation of intelligibility for TTS has reached a bottleneck, as existing assessments heavily rely on word-by-word accuracy metrics such as WER, which fail to capture the complexity of real-world speech or reflect human comprehension needs. To address this, we propose Spoken-Passage Multiple-Choice Question Answering, a novel subjective approach evaluating the accuracy of key information in synthesized speech, and release SP-MCQA-Eval, an 8.76-hour news-style benchmark dataset for SP-MCQA evaluation. Our experiments reveal that low WER does not necessarily guarantee high key-information accuracy, exposing a gap between traditional metrics and practical intelligibility. SP-MCQA shows that even state-of-the-art (SOTA) models still lack robust text normalization and phonetic accuracy. This work underscores the urgent need for high-level, more life-like evaluation criteria now that many systems already excel at WER yet may fall short on real-world intelligibility.",
    "paper_abstract_zh": "TTS可懂度的评估已达到瓶颈，因为现有评估严重依赖逐词准确率指标（如WER），这些指标无法捕捉真实语音的复杂性或反映人类的理解需求。为此，我们提出了语音段落多选题回答（SP-MCQA），一种新颖的主观方法，用于评估合成语音中关键信息的准确性，并发布了SP-MCQA-Eval，一个用于SP-MCQA评估的8.76小时新闻风格基准数据集。实验表明，低WER并不一定保证高关键信息准确率，揭示了传统指标与实际可懂度之间的差距。SP-MCQA显示，即使是最先进的模型仍然缺乏稳健的文本规范化和语音准确性。这项工作强调了当前迫切需要更高级、更逼真的评估标准，因为许多系统在WER方面表现出色，但在实际可懂度方面可能仍有不足。",
    "subjects": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)",
      "Computation and Language (cs.CL)"
    ],
    "update_time": "2025-10-31",
    "paper_authors": "Hitomi Jin Ling Tee, Chaoren Wang, Zijie Zhang, Zhizheng Wu",
    "topic": [
      "Speech Synthesis",
      "Speech Recognition"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Modeling strategies for speech enhancement in the latent space of a neural audio codec",
    "paper_title_zh": "神经音频编解码器潜在空间中的语音增强建模策略",
    "paper_id": "2510.26299",
    "paper_abstract": "Neural audio codecs (NACs) provide compact latent speech representations in the form of sequences of continuous vectors or discrete tokens. In this work, we investigate how these two types of speech representations compare when used as training targets for supervised speech enhancement. We consider both autoregressive and non-autoregressive speech enhancement models based on the Conformer architecture, as well as a simple baseline where the NAC encoder is simply fine-tuned for speech enhancement. Our experiments reveal three key findings: predicting continuous latent representations consistently outperforms discrete token prediction; autoregressive models achieve higher quality but at the expense of intelligibility and efficiency, making non-autoregressive models more attractive in practice; and encoder fine-tuning yields the strongest enhancement metrics overall, though at the cost of degraded codec reconstruction. The code and audio samples are available online.",
    "paper_abstract_zh": "神经音频编解码器（NAC）以连续向量序列或离散令牌的形式提供紧凑的语音表示。在这项工作中，我们研究了当这两种语音表示用作监督语音增强的训练目标时，它们之间的比较。我们考虑了基于Conformer架构的自回归和非自回归语音增强模型，以及一个简单的基线模型，其中NAC编码器仅针对语音增强进行微调。我们的实验揭示了三个关键发现：预测连续潜在表示始终优于离散令牌预测；自回归模型实现了更高的质量，但以可懂度和效率为代价，这使得非自回归模型在实践中更具吸引力；而编码器微调在整体上产生了最强的增强指标，尽管代价是编解码器重建性能的下降。代码和音频样本可在网上获取。",
    "subjects": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2025-10-31",
    "paper_authors": "Sofiene Kammoun, Xavier Alameda-Pineda, Simon Leglaive",
    "topic": [
      "Speech Enhancement",
      "Audio Codec"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio-Language Models",
    "paper_title_zh": "ALMGuard：音频语言模型的安全捷径及作为护栏的发现",
    "paper_id": "2510.26096",
    "paper_abstract": "Recent advances in Audio-Language Models (ALMs) have significantly improved multimodal understanding capabilities. However, the introduction of the audio modality also brings new and unique vulnerability vectors. Previous studies have proposed jailbreak attacks that specifically target ALMs, revealing that defenses directly transferred from traditional audio adversarial attacks or text-based Large Language Model (LLM) jailbreaks are largely ineffective against these ALM-specific threats. To address this issue, we propose ALMGuard, the first defense framework tailored to ALMs. Based on the assumption that safety-aligned shortcuts naturally exist in ALMs, we design a method to identify universal Shortcut Activation Perturbations (SAPs) that serve as triggers that activate the safety shortcuts to safeguard ALMs at inference time. To better sift out effective triggers while preserving the model's utility on benign tasks, we further propose Mel-Gradient Sparse Mask (M-GSM), which restricts perturbations to Mel-frequency bins that are sensitive to jailbreaks but insensitive to speech understanding. Both theoretical analyses and empirical results demonstrate the robustness of our method against both seen and unseen attacks. Overall, \\MethodName reduces the average success rate of advanced ALM-specific jailbreak attacks to 4.6% across four models, while maintaining comparable utility on benign benchmarks, establishing it as the new state of the art. Our code and data are available at this https URL.",
    "paper_abstract_zh": "最近音频语言模型(ALMs)的显著进步极大地提高了多模态理解能力。然而，音频模态的引入也带来了新的独特漏洞向量。先前的研究提出了专门针对ALMs的越狱攻击，揭示出直接从传统音频对抗攻击或基于文本的大型语言模型(LLM)越狱转移而来的防御措施对这些ALM特定威胁 largely无效。为解决这一问题，我们提出了ALMGuard，这是首个专门为ALMs设计的防御框架。基于ALMs中自然存在安全对齐捷径的假设，我们设计了一种方法来识别通用捷径激活扰动(SAPs)，这些扰动作为触发器，在推理时激活安全捷径以保护ALMs。为了在有效筛选触发器的同时保持模型在良性任务上的效用，我们进一步提出了Mel-梯度稀疏掩码(M-GSM)，它将扰动限制在对越狱敏感但对语音理解不敏感的Mel频谱仓上。理论分析和实证结果均证明了我们的方法对已知和未知攻击的鲁棒性。总体而言，MethodName将四种模型上先进ALM特定越狱攻击的平均成功率降低到4.6%，同时在良性基准测试上保持相当的效用，确立了其作为最新技术水平。我们的代码和数据可在提供的https URL获取。",
    "subjects": [
      "Sound (cs.SD)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "update_time": "2025-10-31",
    "paper_authors": "Weifei Jin, Yuxin Cao, Junjie Su, Minhui Xue, Jie Hao, Ke Xu, Jin Song Dong, Derui Wang",
    "topic": [
      "Audio Representation Learning",
      "Speech Recognition"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens",
    "paper_title_zh": "UniTok-Audio：一种通过离散编解码器令牌上的生成建模实现统一音频生成的框架",
    "paper_id": "2510.26372",
    "paper_abstract": "Generative modeling has recently achieved remarkable success across text, image, and audio domains, demonstrating powerful capabilities for unified representation learning. However, audio generation models still face challenges in terms of audio quality and generalization ability across tasks. This fragmentation results in redundant development efforts, inconsistent performance, and limited extensibility. To address these issues, we propose \\textbf{UniTok-Audio}, a scalable and extensible framework for unified audio generation tasks. Specifically, 1) UniTok-Audio extracts continuous feature of conditions to generates discrete tokens of target audio in an autoregressive manner; 2) a special task identifier token unifies different learning patterns of multiple tasks in a single framework; 3) a dual-stream audio codec involving acoustic and semantic branch is developed for high-fidelity waveform reconstruction. Experimental results demonstrate that UniTok-Audio achieves competitive performance in comparation with state-of-the-art task-specific or multi-task systems across five time-aligned tasks: speech restoration, target speaker extraction, speech separation, voice conversion, and language-queried audio source separation. To foster future research, we will open-source our codebase. The demo page of our work can be found here: this https URL.",
    "paper_abstract_zh": "生成建模最近在文本、图像和音频领域取得了显著成功，展示了统一表示学习的强大能力。然而，音频生成模型在音频质量和跨任务泛化能力方面仍面临挑战。这种碎片化导致了冗余的开发工作、不一致的性能和有限的扩展性。为解决这些问题，我们提出了UniTok-Audio，一个可扩展且可扩展的统一音频生成任务框架。具体而言，1) UniTok-Audio以自回归方式提取条件的连续特征，生成目标音频的离散令牌；2) 特殊的任务标识符令牌在单一框架中统一了多种任务的不同学习模式；3) 开发了包含声学和语义分支的双流音频编解码器，用于高保真波形重建。实验结果表明，在五个时间对齐任务（语音修复、目标说话人提取、语音分离、语音转换和语言查询音频源分离）中，UniTok-Audio与最先进的特定任务或多任务系统相比具有竞争力的性能。为促进未来研究，我们将开源我们的代码库。我们作品的演示页面可以在这里找到：this https URL。",
    "subjects": [
      "Sound (cs.SD)"
    ],
    "update_time": "2025-10-31",
    "paper_authors": "Chengwei Liu, Haoyin Yan, Shaofei Xue, Xiaotao Liang, Yinghao Liu, Zheng Xue, Gang Song, Boyang Zhou",
    "topic": [
      "Audio Representation Learning",
      "Speech Enhancement",
      "Audio Codec"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Learning Interpretable Features in Audio Latent Spaces via Sparse Autoencoders",
    "paper_title_zh": "通过稀疏自编码器在音频潜在空间中学习可解释特征",
    "paper_id": "2510.23802",
    "paper_abstract": "While sparse autoencoders (SAEs) successfully extract interpretable features from language models, applying them to audio generation faces unique challenges: audio's dense nature requires compression that obscures semantic meaning, and automatic feature characterization remains limited. We propose a framework for interpreting audio generative models by mapping their latent representations to human-interpretable acoustic concepts. We train SAEs on audio autoencoder latents, then learn linear mappings from SAE features to discretized acoustic properties (pitch, amplitude, and timbre). This enables both controllable manipulation and analysis of the AI music generation process, revealing how acoustic properties emerge during synthesis. We validate our approach on continuous (DiffRhythm-VAE) and discrete (EnCodec, WavTokenizer) audio latent spaces, and analyze DiffRhythm, a state-of-the-art text-to-music model, to demonstrate how pitch, timbre, and loudness evolve throughout generation. While our work is only done on audio modality, our framework can be extended to interpretable analysis of visual latent space generation models.",
    "paper_abstract_zh": "虽然稀疏自编码器(SAEs)成功地从语言模型中提取可解释特征，但将其应用于音频生成面临着独特的挑战：音频的密集特性需要压缩，这会掩盖语义意义，并且自动特征表征仍然有限。我们提出了一种框架，通过将音频生成模型的潜在表示映射到人类可解释的声学概念来解释这些模型。我们在音频自编码器潜在空间上训练SAEs，然后学习从SAE特征到离散化声学属性(音高、振幅和音色)的线性映射。这既实现了对AI音乐生成过程的可控操作，也实现了对其的分析，揭示了声学属性在合成过程中如何出现。我们在连续(DiffRhythm-VAE)和离散(EnCodec, WavTokenizer)音频潜在空间上验证了我们的方法，并分析了DiffRhythm——一个最先进的文本到音乐模型，以展示音高、音色和响度在整个生成过程中的演变。虽然我们的工作仅限于音频模态，但我们的框架可以扩展到视觉潜在空间生成模型的解释性分析。",
    "subjects": [
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)"
    ],
    "update_time": "2025-10-31",
    "paper_authors": "Nathan Paek, Yongyi Zang, Qihui Yang, Randal Leistikow",
    "topic": [
      "Audio Representation Learning",
      "Music Generation"
    ],
    "category": [
      "Music"
    ]
  }
]