[
  {
    "paper_title": "SPEAR: A Unified SSL Framework for Learning Speech and Audio Representations",
    "paper_title_zh": "SPEAR：学习语音与音频表示的统一自监督学习框架",
    "paper_id": "2510.25955",
    "paper_abstract": "Self-Supervised Learning (SSL) excels at learning generic representations of acoustic signals, yet prevailing methods remain domain-specific, tailored to either speech or general audio, hindering the development of a unified representation model with a comprehensive capability over both domains. To address this, we present SPEAR (SPEech and Audio Representations), the first SSL framework to successfully learn unified speech and audio representations from a mixture of speech and audio data. SPEAR proposes a unified pre-training objective based on masked prediction of fine-grained discrete tokens for both speech and general audio. These tokens are derived from continuous speech and audio representations using a Multi-codebook Vector Quantisation (MVQ) method, retaining rich acoustic detail essential for modelling both speech and complex audio events. SPEAR is applied to pre-train both single-domain and unified speech-and-audio SSL models. Our speech-domain model establishes a new state-of-the-art on the SUPERB benchmark, a speech processing benchmark for SSL models, matching or surpassing the highly competitive WavLM Large on 12 out of 15 tasks with the same pre-training corpora and a similar model size. Crucially, our unified model learns complementary features and demonstrates comprehensive capabilities across two major benchmarks, SUPERB and HEAR, for evaluating audio representations. By further scaling up the model size and pre-training data, we present a unified model with 600M parameters that excels in both domains, establishing it as one of the most powerful and versatile open-source SSL models for auditory understanding. The inference code and pre-trained models will be made publicly available.",
    "paper_abstract_zh": "自监督学习（SSL）在学习声学信号的通用表示方面表现出色，但现有方法仍局限于特定领域，仅针对语音或通用音频进行优化，阻碍了能够同时覆盖两个领域的统一表示模型的发展。为解决这一问题，我们提出了SPEAR（语音与音频表示），首个成功从混合语音和音频数据中学习统一表示的SSL框架。SPEAR基于掩码预测细粒度离散令牌的统一预训练目标，适用于语音和通用音频。这些令牌通过多码本向量量化（MVQ）方法从连续语音和音频表示中提取，保留了对建模语音和复杂音频事件至关重要的丰富声学细节。SPEAR应用于预训练单领域和统一语音-音频SSL模型。我们的语音域模型在语音处理基准测试SUPERB上建立了新的最先进水平，使用相同预训练语料库和相似模型规模的情况下，在15项任务中的12项上达到或超越了高度竞争性的WavLM Large模型。关键的是，我们的统一模型学习了互补特征，并在评估音频表示的两个主要基准测试SUPERB和HEAR上展现出全面能力。通过进一步扩大模型规模和预训练数据，我们提出了一个拥有6亿参数的统一模型，在两个领域均表现出色，确立了其作为最强大且多功能的开源听觉理解SSL模型之一的地位。推理代码和预训练模型将公开提供。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2025-10-31",
    "paper_authors": "Xiaoyu Yang, Yifan Yang, Zengrui Jin, Ziyun Cui, Wen Wu, Baoxiang Li, Chao Zhang, Phil Woodland",
    "topic": [
      "Audio Representation Learning"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "POWSM: A Phonetic Open Whisper-Style Speech Foundation Model",
    "paper_title_zh": "POWSM：一种语音学开放式Whisper风格语音基础模型",
    "paper_id": "2510.24992",
    "paper_abstract": "Recent advances in spoken language processing have led to substantial progress in phonetic tasks such as automatic speech recognition (ASR), phone recognition (PR), grapheme-to-phoneme conversion (G2P), and phoneme-to-grapheme conversion (P2G). Despite their conceptual similarity, these tasks have largely been studied in isolation, each relying on task-specific architectures and datasets. In this paper, we introduce POWSM (Phonetic Open Whisper-style Speech Model), the first unified framework capable of jointly performing multiple phone-related tasks. POWSM enables seamless conversion between audio, text (graphemes), and phones, opening up new possibilities for universal and low-resource speech processing. Our model outperforms or matches specialized PR models of similar size (Wav2Vec2Phoneme and ZIPA) while jointly supporting G2P, P2G, and ASR. Our training data, code and models are released to foster open science.",
    "paper_abstract_zh": "近期口语处理技术的进步在语音学任务（如自动语音识别(ASR)、音素识别(PR)、字母到音素转换(G2P)和音素到字母转换(P2G)）方面取得了显著进展。尽管这些任务在概念上相似，但它们大多被孤立研究，各自依赖于特定的架构和数据集。本文介绍了POWSM（Phonetic Open Whisper-style Speech Model），这是首个能够联合执行多种音素相关任务的统一框架。POWSM实现了音频、文本（字母）和音素之间的无缝转换，为通用和低资源语音处理开辟了新的可能性。我们的模型在性能上优于或匹敌类似规模的专用PR模型（如Wav2Vec2Phoneme和ZIPA），同时支持G2P、P2G和ASR。我们已公开发布训练数据、代码和模型，以促进开放科学。",
    "subjects": [
      "Computation and Language (cs.CL)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2025-10-31",
    "paper_authors": "Chin-Jou Li, Kalvin Chang, Shikhar Bharadwaj, Eunjung Yeo, Kwanghee Choi, Jian Zhu, David Mortensen, Shinji Watanabe",
    "topic": [
      "Audio Representation Learning",
      "Speech Recognition"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "SP-MCQA: Evaluating Intelligibility of TTS Beyond the Word Level",
    "paper_title_zh": "SP-MCQA：评估TTS超越词级的可懂度",
    "paper_id": "2510.26190",
    "paper_abstract": "The evaluation of intelligibility for TTS has reached a bottleneck, as existing assessments heavily rely on word-by-word accuracy metrics such as WER, which fail to capture the complexity of real-world speech or reflect human comprehension needs. To address this, we propose Spoken-Passage Multiple-Choice Question Answering, a novel subjective approach evaluating the accuracy of key information in synthesized speech, and release SP-MCQA-Eval, an 8.76-hour news-style benchmark dataset for SP-MCQA evaluation. Our experiments reveal that low WER does not necessarily guarantee high key-information accuracy, exposing a gap between traditional metrics and practical intelligibility. SP-MCQA shows that even state-of-the-art (SOTA) models still lack robust text normalization and phonetic accuracy. This work underscores the urgent need for high-level, more life-like evaluation criteria now that many systems already excel at WER yet may fall short on real-world intelligibility.",
    "paper_abstract_zh": "TTS可懂度的评估已达到瓶颈，因为现有评估严重依赖于逐词准确度指标（如WER），这些指标无法捕捉真实语音的复杂性或反映人类的理解需求。为此，我们提出了语音段落多选题问答（Spoken-Passage Multiple-Choice Question Answering），一种新颖的主观方法，用于评估合成语音中关键信息的准确性，并发布了SP-MCQA-Eval，一个用于SP-MCQA评估的8.76小时新闻风格基准数据集。实验表明，低WER并不一定保证高关键信息准确度，揭示了传统指标与实际可懂度之间的差距。SP-MCQA显示，即使是最先进的模型仍缺乏稳健的文本规范化和音韵准确性。这项工作强调了当前许多系统在WER方面表现出色但在实际可懂度上可能不足的情况下，迫切需要更高级、更贴近生活的评估标准。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)",
      "Computation and Language (cs.CL)"
    ],
    "update_time": "2025-10-31",
    "paper_authors": "Hitomi Jin Ling Tee, Chaoren Wang, Zijie Zhang, Zhizheng Wu",
    "topic": [
      "Speech Synthesis",
      "Speech Recognition"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Modeling strategies for speech enhancement in the latent space of a neural audio codec",
    "paper_title_zh": "基于神经音频编码器潜在空间的语音增强建模策略",
    "paper_id": "2510.26299",
    "paper_abstract": "Neural audio codecs (NACs) provide compact latent speech representations in the form of sequences of continuous vectors or discrete tokens. In this work, we investigate how these two types of speech representations compare when used as training targets for supervised speech enhancement. We consider both autoregressive and non-autoregressive speech enhancement models based on the Conformer architecture, as well as a simple baseline where the NAC encoder is simply fine-tuned for speech enhancement. Our experiments reveal three key findings: predicting continuous latent representations consistently outperforms discrete token prediction; autoregressive models achieve higher quality but at the expense of intelligibility and efficiency, making non-autoregressive models more attractive in practice; and encoder fine-tuning yields the strongest enhancement metrics overall, though at the cost of degraded codec reconstruction. The code and audio samples are available online.",
    "paper_abstract_zh": "神经音频编码器（NAC）以连续向量序列或离散令牌的形式提供紧凑的语音表示。在这项工作中，我们研究了当这两种语音表示被用作监督语音增强的训练目标时，它们之间的比较。我们考虑了基于Conformer架构的自回归和非自回归语音增强模型，以及一个简单的基线方法，即仅对NAC编码器进行微调以进行语音增强。我们的实验揭示了三个关键发现：预测连续潜在表示始终优于离散令牌预测；自回归模型获得更高的质量，但以可懂度和效率为代价，这使得非自回归模型在实践中更具吸引力；编码器微调在整体上产生了最强的增强指标，但代价是编解码器重建质量的下降。代码和音频样本可在网上获取。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "update_time": "2025-10-31",
    "paper_authors": "Sofiene Kammoun, Xavier Alameda-Pineda, Simon Leglaive",
    "topic": [
      "Speech Enhancement",
      "Audio Codec",
      "Audio Representation Learning"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio-Language Models",
    "paper_title_zh": "",
    "paper_id": "2510.26096",
    "paper_abstract": "Recent advances in Audio-Language Models (ALMs) have significantly improved multimodal understanding capabilities. However, the introduction of the audio modality also brings new and unique vulnerability vectors. Previous studies have proposed jailbreak attacks that specifically target ALMs, revealing that defenses directly transferred from traditional audio adversarial attacks or text-based Large Language Model (LLM) jailbreaks are largely ineffective against these ALM-specific threats. To address this issue, we propose ALMGuard, the first defense framework tailored to ALMs. Based on the assumption that safety-aligned shortcuts naturally exist in ALMs, we design a method to identify universal Shortcut Activation Perturbations (SAPs) that serve as triggers that activate the safety shortcuts to safeguard ALMs at inference time. To better sift out effective triggers while preserving the model's utility on benign tasks, we further propose Mel-Gradient Sparse Mask (M-GSM), which restricts perturbations to Mel-frequency bins that are sensitive to jailbreaks but insensitive to speech understanding. Both theoretical analyses and empirical results demonstrate the robustness of our method against both seen and unseen attacks. Overall, \\MethodName reduces the average success rate of advanced ALM-specific jailbreak attacks to 4.6% across four models, while maintaining comparable utility on benign benchmarks, establishing it as the new state of the art. Our code and data are available at this https URL.",
    "paper_abstract_zh": "",
    "subjects": [
      "Sound (cs.SD)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "update_time": "2025-10-31",
    "paper_authors": "Weifei Jin, Yuxin Cao, Junjie Su, Minhui Xue, Jie Hao, Ke Xu, Jin Song Dong, Derui Wang",
    "topic": [],
    "category": []
  },
  {
    "paper_title": "UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens",
    "paper_title_zh": "UniTok-Audio：基于离散编解码器令牌生成建模的统一音频生成框架",
    "paper_id": "2510.26372",
    "paper_abstract": "Generative modeling has recently achieved remarkable success across text, image, and audio domains, demonstrating powerful capabilities for unified representation learning. However, audio generation models still face challenges in terms of audio quality and generalization ability across tasks. This fragmentation results in redundant development efforts, inconsistent performance, and limited extensibility. To address these issues, we propose \\textbf{UniTok-Audio}, a scalable and extensible framework for unified audio generation tasks. Specifically, 1) UniTok-Audio extracts continuous feature of conditions to generates discrete tokens of target audio in an autoregressive manner; 2) a special task identifier token unifies different learning patterns of multiple tasks in a single framework; 3) a dual-stream audio codec involving acoustic and semantic branch is developed for high-fidelity waveform reconstruction. Experimental results demonstrate that UniTok-Audio achieves competitive performance in comparation with state-of-the-art task-specific or multi-task systems across five time-aligned tasks: speech restoration, target speaker extraction, speech separation, voice conversion, and language-queried audio source separation. To foster future research, we will open-source our codebase. The demo page of our work can be found here: this https URL.",
    "paper_abstract_zh": "生成建模近年来在文本、图像和音频领域取得了显著成功，展示了强大的统一表征学习能力。然而，音频生成模型在音频质量和跨任务泛化能力方面仍面临挑战。这种碎片化导致了冗余的研发工作、性能不一致以及可扩展性有限。为解决这些问题，我们提出了**UniTok-Audio**，一个可扩展且可扩展的统一音频生成框架。具体而言：1）UniTok-Audio通过自回归方式提取条件连续特征并生成目标音频的离散令牌；2）通过特殊任务标识令牌在单一框架中统一多种任务的学习模式；3）开发了包含声学与语义分支的双流音频编解码器，实现高质量波形重建。实验结果表明，UniTok-Audio在五个时间对齐任务（语音恢复、目标说话人提取、语音分离、语音转换和语言查询的音频源分离）上均达到了与现有最先进单任务或多任务系统相当的性能。为促进未来研究，我们将开源代码库。项目演示页面：https URL。",
    "subjects": [
      "Sound (cs.SD)"
    ],
    "update_time": "2025-10-31",
    "paper_authors": "Chengwei Liu, Haoyin Yan, Shaofei Xue, Xiaotao Liang, Yinghao Liu, Zheng Xue, Gang Song, Boyang Zhou",
    "topic": [
      "Audio Representation Learning",
      "Speech Enhancement",
      "Audio Codec"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Learning Interpretable Features in Audio Latent Spaces via Sparse Autoencoders",
    "paper_title_zh": "通过稀疏自编码器在音频潜在空间中学习可解释特征",
    "paper_id": "2510.23802",
    "paper_abstract": "While sparse autoencoders (SAEs) successfully extract interpretable features from language models, applying them to audio generation faces unique challenges: audio's dense nature requires compression that obscures semantic meaning, and automatic feature characterization remains limited. We propose a framework for interpreting audio generative models by mapping their latent representations to human-interpretable acoustic concepts. We train SAEs on audio autoencoder latents, then learn linear mappings from SAE features to discretized acoustic properties (pitch, amplitude, and timbre). This enables both controllable manipulation and analysis of the AI music generation process, revealing how acoustic properties emerge during synthesis. We validate our approach on continuous (DiffRhythm-VAE) and discrete (EnCodec, WavTokenizer) audio latent spaces, and analyze DiffRhythm, a state-of-the-art text-to-music model, to demonstrate how pitch, timbre, and loudness evolve throughout generation. While our work is only done on audio modality, our framework can be extended to interpretable analysis of visual latent space generation models.",
    "paper_abstract_zh": "虽然稀疏自编码器（SAEs）在语言模型中成功提取了可解释特征，但在音频生成领域面临独特挑战：音频的密集特性需要压缩，这会掩盖语义含义，且自动特征表征仍存在局限。我们提出了一种通过将音频生成模型的潜在表示映射到人类可理解的声学概念来解析音频生成模型的框架。我们在音频自编码器潜在空间上训练SAEs，然后学习从SAE特征到离散声学属性（音高、振幅和音色）的线性映射。这使得能够对AI音乐生成过程进行可控操作和分析，揭示声学属性在合成过程中的形成机制。我们在连续（DiffRhythm-VAE）和离散（EnCodec, WavTokenizer）音频潜在空间上验证了该方法，并以最先进的文本到音乐模型DiffRhythm为例，展示了音高、音色和响度在整个生成过程中的演变。尽管本研究仅针对音频模态，但该框架可扩展至视觉潜在空间生成模型的可解释性分析。",
    "subjects": [
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)"
    ],
    "update_time": "2025-10-31",
    "paper_authors": "Nathan Paek, Yongyi Zang, Qihui Yang, Randal Leistikow",
    "topic": [
      "Audio Representation Learning",
      "Music Generation"
    ],
    "category": [
      "Music"
    ]
  }
]