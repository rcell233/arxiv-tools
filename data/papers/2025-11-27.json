[
  {
    "paper_title": "Towards Audio Token Compression in Large Audio Language Models",
    "paper_title_zh": "迈向大型音频语言模型中的音频令牌压缩",
    "paper_id": "2511.20973",
    "paper_abstract": "Large Audio Language Models (LALMs) demonstrate impressive performance across diverse tasks, ranging from speech recognition to general audio understanding. However, their scalability is limited by the quadratic complexity of attention and the high token rates of audio signals. These challenges make it difficult to extend LALMs to long-form audio and to deploy them on resource-constrained platforms such as edge devices.\nIn this paper, we explore techniques such as unsupervised segmentation, uniform average pooling, etc., to reduce the number of audio tokens generated by the LALM's audio encoder but before they are consumed by the LLM decoder. To mitigate potential performance degradation introduced by the compressed representations, we employ low-rank adapters to finetune the model. We evaluate our proposed models on two tasks, automatic speech recognition and speech-to-speech translation tasks, that are dependent on effectively uncovering the underlying lexical content of the input signal and study the effect of downsampling on these tasks. Experimental results show that compressed LALMs can achieve performance closer to frame-level LALMs while reducing the input audio token count upto three times before the LLM backbone.",
    "paper_abstract_zh": "大型音频语言模型（LALMs）在从语音识别到通用音频理解等多种任务上表现出令人印象深刻的性能。然而，其扩展性受到注意力机制二次复杂度和音频信号高令牌率的限制。这些挑战使得将LALMs扩展到长音频形式以及在资源受限平台（如边缘设备）上部署变得困难。在本文中，我们探索了无监督分割、均匀平均池化等技术，以减少LALM音频编码器生成但在被LLM解码器消费之前的音频令牌数量。为了缓解压缩表示可能带来的性能下降，我们采用低秩适配器对模型进行微调。我们在两个任务上评估了我们提出的模型：自动语音识别和语音到语音翻译任务，这些任务依赖于有效揭示输入信号的基础词汇内容，并研究了降采样对这些任务的影响。实验结果表明，压缩后的LALMs可以在将LLM主干之前的输入音频令牌数量减少高达三倍的同时，实现接近帧级LALMs的性能。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ],
    "update_time": "2025-11-27",
    "paper_authors": "Saurabhchand Bhati, Samuel Thomas, Hilde Kuehne, Rogerio Feris, James Glass",
    "topic": [
      "Audio Representation Learning",
      "Speech Recognition"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "RosettaSpeech: Zero-Shot Speech-to-Speech Translation from Monolingual Data",
    "paper_title_zh": "RosettaSpeech: 基于单语数据的零样本语音到语音翻译",
    "paper_id": "2511.20974",
    "paper_abstract": "The scarcity of parallel speech corpora critically hampers speech-to-speech translation (S2ST), often forcing reliance on complex, multi-stage pipelines. This paper introduces RosettaSpeech, a novel and simplified framework for zero-shot S2ST that is trained on monolingual speech-text data augmented by machine translation supervision. While our method leverages the linguistic knowledge inherent in text-based NMT models, it strictly eliminates the need for parallel speech-to-speech pairs. Our model uniquely uses text as an intermediate bridge during training but functions as a direct, end-to-end speech-to-speech model at inference. This streamlined approach achieves state-of-the-art results on standard benchmarks. For instance, on the CVSS-C test set, RosettaSpeech outperforms leading systems, achieving an ASR-BLEU score of 25.17 for German-to-English and 29.86 for Spanish-to-English-relative gains of over 27% and 14%, respectively. Furthermore, we demonstrate that a single model can deliver strong many-to-one translation performance (FR/ES/DE -> EN). We also provide a foundational analysis of how training data scaling impacts model performance. By prioritizing reliance on abundant parallel text rather than difficult-to-acquire parallel speech, RosettaSpeech offers a scalable path to creating high-quality, speaker-preserving S2ST for a much broader array of languages.",
    "paper_abstract_zh": "并行语音语料库的稀缺严重阻碍了语音到语音翻译(S2ST)的发展，通常迫使我们依赖复杂的多阶段流水线。本文介绍了RosettaSpeech，这是一种新颖且简化的零样本S2ST框架，它通过机器翻译监督增强的单语语音文本数据进行训练。虽然我们的方法利用了基于文本的NMT模型中固有的语言学知识，但它严格消除了对并行语音到语音对的需求。我们的模型在训练过程中独特地使用文本作为中间桥梁，但在推理时则作为直接的端到端语音到语音模型工作。这种简化的方法在标准基准测试上取得了最先进的结果。例如，在CVSS-C测试集上，RosettaSpeech超越了领先系统，德语到英语的ASR-BLEU得分为25.17，西班牙语到英语的得分为29.86——相对增益分别超过27%和14%。此外，我们证明单个模型可以实现强大的多对一翻译性能(FR/ES/DE -> EN)。我们还提供了关于训练数据扩展如何影响模型性能的基础分析。通过优先依赖丰富的并行文本而非难以获取的并行语音，RosettaSpeech为更广泛的语言创建高质量、保留说话人特征的S2ST提供了一条可扩展的路径。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "update_time": "2025-11-27",
    "paper_authors": "Zhisheng Zheng, Xiaohang Sun, Tuan Dinh, Abhishek Yanamandra, Abhinav Jain, Zhu Liu, Sunil Hadap, Vimal Bhat, Manoj Aggarwal, Gerard Medioni, David Harwath",
    "topic": [
      "Speech Recognition",
      "Speech Synthesis"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Evaluation of an ITD-to-ILD Transformation as a Method to Restore the Spatial Benefit in Speech Intelligibility in Hearing Impaired Listeners",
    "paper_title_zh": "评估ITD到ILD转换作为恢复听力障碍者言语可懂度空间效益的方法",
    "paper_id": "2511.21222",
    "paper_abstract": "To improve speech intelligibility in complex everyday situations, the human auditory system partially relies on Interaural Time Differences (ITDs) and Interaural Level Differences (ILDs). However, hearing impaired (HI) listeners often exhibit limited sensitivity to ITDs, resulting in decreased speech intelligibility performance. This study aimed to investigate whether transforming low-frequency ITDs into ILDs could reintroduce a binaural benefit for HI listeners. We conducted two experiments with HI listeners. The first experiment used binaurally phase-shifted sinusoids at different frequencies to evaluate the HI listeners ITD sensitivity threshold. All subjects had an increased ITD threshold at higher frequencies, with different ITD sensitivities between the subjects in the lower frequencies. In the second experiment, Speech Reception Thresholds (SRTs) were measured in different binaural configurations by manipulating Head-Related Transfer Functions (HRTFs). The results showed that, despite the decreased ITD sensitivity, removing ITDs decreased SRTs by approximately 1 dB compared to the unprocessed baseline, where ITDs and ILDs are available. Furthermore, substituting low-frequency ITDs with ILDs yielded an improvement for a lateral target speaker. Adding the low-frequency ILDs while preserving the ITDs caused a significant improvement for speakers in all directions. These findings suggest that the proposed transformation method could be effective in restoring binaural benefits in HI listeners. The results of this study suggest the use of such transformation techniques to be implemented in hearing aids and cochlear implants, directly benefiting HI listeners.",
    "paper_abstract_zh": "为了提高在复杂日常环境中的言语可懂度，人类听觉系统部分依赖于双耳时间差(ITDs)和双耳强度差(ILDs)。然而，听力障碍(HI)听者通常对ITDs的敏感性有限，导致言语可懂度性能下降。本研究旨在探讨将低频ITDs转换为ILDs是否能为HI听者重新引入双耳效益。我们对HI听者进行了两项实验。第一项实验使用不同频率的双耳相移正弦波来评估HI听者的ITD敏感性阈值。所有受试者在较高频率下ITD阈值增加，且在较低频率下不同受试者之间的ITD敏感性存在差异。在第二项实验中，通过操作头相关传递函数(HRTFs)，在不同双耳配置下测量言语接收阈值(SRTs)。结果表明，尽管ITD敏感性降低，但与未处理的基线(同时提供ITDs和ILDs)相比，移除ITDs使SRTs降低了约1 dB。此外，用ILDs替代低频ITDs对侧向目标说话人有所改善。在保留ITDs的同时添加低频ILDs则对所有方向的说话人都有显著改善。这些研究结果表明，所提出的转换方法可能对恢复HI听者的双耳效益有效。本研究的结果表明，此类转换技术应应用于助听器和人工耳蜗，直接惠及HI听者。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2025-11-27",
    "paper_authors": "Timm-Jonas Bäumer, Johannes W. de Vries, Stephan Töpken, Richard C. Hendriks, Peyman Goli, Steven van de Par",
    "topic": [
      "Speech Enhancement",
      "Speech Recognition"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "The Spheres Dataset: Multitrack Orchestral Recordings for Music Source Separation and Information Retrieval",
    "paper_title_zh": "Spheres数据集：用于音乐源分离和信息检索的多管弦乐队录音",
    "paper_id": "2511.21247",
    "paper_abstract": "This paper introduces The Spheres dataset, multitrack orchestral recordings designed to advance machine learning research in music source separation and related MIR tasks within the classical music domain. The dataset is composed of over one hour recordings of musical pieces performed by the Colibrì Ensemble at The Spheres recording studio, capturing two canonical works - Tchaikovsky's Romeo and Juliet and Mozart's Symphony No. 40 - along with chromatic scales and solo excerpts for each instrument. The recording setup employed 23 microphones, including close spot, main, and ambient microphones, enabling the creation of realistic stereo mixes with controlled bleeding and providing isolated stems for supervised training of source separation models. In addition, room impulse responses were estimated for each instrument position, offering valuable acoustic characterization of the recording space. We present the dataset structure, acoustic analysis, and baseline evaluations using X-UMX based models for orchestral family separation and microphone debleeding. Results highlight both the potential and the challenges of source separation in complex orchestral scenarios, underscoring the dataset's value for benchmarking and for exploring new approaches to separation, localization, dereverberation, and immersive rendering of classical music.",
    "paper_abstract_zh": "本文介绍了Spheres数据集，这是一组多管弦乐队录音，旨在推进古典音乐领域机器学习在音乐源分离和相关音乐信息检索(MIR)任务中的研究。该数据集由Colibrì Ensemble在Spheres录音室录制的超过一小时的音乐作品组成，包含两部经典作品——柴可夫斯基的《罗密欧与朱丽叶》和莫扎特的《第40号交响曲》，以及每种乐器的音阶和独奏片段。录音设置采用了23个麦克风，包括近距离点麦克风、主麦克风和环境麦克风，能够创建具有可控串音的真实立体声混音，并为源分离模型的监督训练提供隔离的音轨。此外，还为每个乐器位置估计了房间脉冲响应，提供了录音空间宝贵的声学特性表征。我们展示了数据集结构、声学分析，以及基于X-UMX模型的管弦乐队家族分离和麦克风去串音的基线评估结果。结果突显了在复杂管弦乐队场景中进行源分离的潜力和挑战，强调了该数据集在基准测试和探索古典音乐分离、定位、去混响和沉浸式渲染新方法方面的价值。",
    "subjects": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)",
      "Machine Learning (cs.LG)"
    ],
    "update_time": "2025-11-27",
    "paper_authors": "Jaime Garcia-Martinez, David Diaz-Guerra, John Anderson, Ricardo Falcon-Perez, Pablo Cabañas-Molero, Tuomas Virtanen, Julio J. Carabias-Orti, Pedro Vera-Candeas",
    "topic": [
      "Music Information Retrieval",
      "Audio Representation Learning"
    ],
    "category": [
      "Music"
    ]
  },
  {
    "paper_title": "Seeing Beyond Sound: Visualization and Abstraction in Audio Data Representation",
    "paper_title_zh": "超越声音：音频数据表示中的可视化与抽象",
    "paper_id": "2511.20658",
    "paper_abstract": "In audio signal processing, the interpretation of complex information using visual representation enhances pattern recognition through its alignment with human perceptual systems. Software tools that carry hidden assumptions inherited from their historical contexts risk misalignment with modern workflows as design origins become obscured. We argue that creating tools that align with emergent needs improves analytical and creative outputs due to an increased affinity for using them. This paper explores the potentials associated with adding dimensionality and interactivity into visualization tools to facilitate complex workflows in audio information research using the Jellyfish Dynamite software.",
    "paper_abstract_zh": "在音频信号处理中，通过视觉表示来解释复杂信息，能够通过与人类感知系统的对齐来增强模式识别。那些继承了历史背景中隐含假设的软件工具，随着设计起源的模糊化，可能会与现代工作流程不匹配。我们认为，创建与新兴需求相一致的工具可以提高分析和创造性产出，因为用户使用这些工具的亲和力会增加。本文探讨了在可视化工具中增加维度和交互性的潜力，以促进音频信息研究中的复杂工作流程，使用了Jellyfish Dynamite软件。",
    "subjects": [
      "Sound (cs.SD)",
      "Human-Computer Interaction (cs.HC)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2025-11-27",
    "paper_authors": "Ashlae Blum'e",
    "topic": [
      "Audio Representation Learning",
      "Other"
    ],
    "category": [
      "Other"
    ]
  },
  {
    "paper_title": "Acoustic neural networks: Identifying design principles and exploring physical feasibility",
    "paper_title_zh": "声学神经网络：确定设计原则并探索物理可行性",
    "paper_id": "2511.21313",
    "paper_abstract": "Wave-guide-based physical systems provide a promising route toward energy-efficient analog computing beyond traditional electronics. Within this landscape, acoustic neural networks represent a promising approach for achieving low-power computation in environments where electronics are inefficient or limited, yet their systematic design has remained largely unexplored. Here we introduce a framework for designing and simulating acoustic neural networks, which perform computation through the propagation of sound waves. Using a digital-twin approach, we train conventional neural network architectures under physically motivated constraints including non-negative signals and weights, the absence of bias terms, and nonlinearities compatible with intensity-based, non-negative acoustic signals. Our work provides a general framework for acoustic neural networks that connects learnable network components directly to physically measurable acoustic properties, enabling the systematic design of realizable acoustic computing systems. We demonstrate that constrained recurrent and hierarchical architectures can perform accurate speech classification, and we propose the SincHSRNN, a hybrid model that combines learnable acoustic bandpass filters with hierarchical temporal processing. The SincHSRNN achieves up to 95% accuracy on the AudioMNIST dataset while remaining compatible with passive acoustic components. Beyond computational performance, the learned parameters correspond to measurable material and geometric properties such as attenuation and transmission. Our results establish general design principles for physically realizable acoustic neural networks and outline a pathway toward low-power, wave-based neural computing.",
    "paper_abstract_zh": "基于波导的物理系统为超越传统电子学的节能模拟计算提供了有前景的途径。在这一领域，声学神经网络为实现低功耗计算提供了一种有前景的方法，特别是在电子设备效率低下或受限的环境中，然而其系统设计在很大程度上仍未被探索。在这里，我们介绍了一个用于设计和模拟声学神经网络的框架，这些网络通过声波的传播来执行计算。采用数字孪生方法，我们在物理约束条件下训练传统的神经网络架构，包括非负信号和权重、不存在偏置项，以及与基于强度、非负声信号兼容的非线性。我们的工作为声学神经网络提供了一个通用框架，将可学习的网络组件直接与可测量的声学特性联系起来，从而实现了可实现声学计算系统的系统设计。我们证明，受约束的循环和分层架构可以执行准确的语音分类，并且我们提出了SincHSRNN，这是一种结合了可学习声学带通滤波器与分层时间处理的混合模型。SincHSRNN在AudioMNIST数据集上实现了高达95%的准确率，同时保持与无源声学组件的兼容性。除了计算性能外，学习到的参数对应于可测量的材料和几何特性，如衰减和传输。我们的结果为可实现物理声学神经网络建立了通用设计原则，并概述了向低功耗、基于波的神经计算的发展路径。",
    "subjects": [
      "Sound (cs.SD)",
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "Audio and Speech Processing (eess.AS)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Applied Physics (physics.app-ph)"
    ],
    "update_time": "2025-11-27",
    "paper_authors": "Ivan Kalthoff, Marcel Rey, Raphael Wittkowski",
    "topic": [
      "Audio Classification",
      "Speech Recognition"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Musical Score Understanding Benchmark: Evaluating Large Language Models' Comprehension of Complete Musical Scores",
    "paper_title_zh": "乐谱理解基准：评估大型语言模型对完整乐谱的理解能力",
    "paper_id": "2511.20697",
    "paper_abstract": "Understanding complete musical scores requires reasoning over symbolic structures such as pitch, rhythm, harmony, and form. Despite the rapid progress of Large Language Models (LLMs) and Vision-Language Models (VLMs) in natural language and multimodal tasks, their ability to comprehend musical notation remains underexplored. We introduce Musical Score Understanding Benchmark (MSU-Bench), the first large-scale, human-curated benchmark for evaluating score-level musical understanding across both textual (ABC notation) and visual (PDF) modalities. MSU-Bench comprises 1,800 generative question-answer (QA) pairs drawn from works spanning Bach, Beethoven, Chopin, Debussy, and others, organised into four progressive levels of comprehension: Onset Information, Notation & Note, Chord & Harmony, and Texture & Form. Through extensive zero-shot and fine-tuned evaluations of over 15+ state-of-the-art (SOTA) models, we reveal sharp modality gaps, fragile level-wise success rates, and the difficulty of sustaining multilevel correctness. Fine-tuning markedly improves performance in both modalities while preserving general knowledge, establishing MSU-Bench as a rigorous foundation for future research at the intersection of Artificial Intelligence (AI), musicological, and multimodal reasoning.",
    "paper_abstract_zh": "理解完整的乐谱需要对音高、节奏、和声和形式等符号结构进行推理。尽管大型语言模型（LLMs）和视觉-语言模型（VLMs）在自然语言和多模态任务中取得了快速进展，但它们对乐谱符号的理解能力仍未得到充分探索。我们提出了乐谱理解基准（MSU-Bench），这是首个大规模、人工策划的基准，用于评估在文本（ABC记谱法）和视觉（PDF）两种模态下的乐谱级音乐理解能力。MSU-Bench包含1800个生成式问答（QA）对，这些问答对来源于巴赫、贝多芬、肖邦、德彪西等人的作品，并组织为四个递进的理解层次：起始信息、记谱与音符、和弦与和声、以及织体与形式。通过对15多种最先进（SOTA）模型进行广泛的零样本和微调评估，我们揭示了明显的模态差距、脆弱的层次成功率以及维持多级正确性的困难。微调显著提高了两种模态的性能，同时保留了通用知识，从而确立了MSU-Bench作为人工智能（AI）、音乐学和多模态推理交叉领域未来研究的严谨基础。",
    "subjects": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)"
    ],
    "update_time": "2025-11-27",
    "paper_authors": "Congren Dai, Yue Yang, Krinos Li, Huichi Zhou, Shijie Liang, Zhang Bo, Enyang Liu, Ge Jin, Hongran An, Haosen Zhang, Peiyuan Jing, KinHei Lee, Zhenxuan Zhang, Xiaobing Li, Maosong Sun",
    "topic": [
      "Music Information Retrieval",
      "Other"
    ],
    "category": [
      "Music"
    ]
  },
  {
    "paper_title": "SingingSDS: A Singing-Capable Spoken Dialogue System for Conversational Roleplay Applications",
    "paper_title_zh": "SingingSDS：一种用于角色扮演应用的歌曲对话系统",
    "paper_id": "2511.20972",
    "paper_abstract": "With recent advances in automatic speech recognition (ASR), large language models (LLMs), and text-to-speech (TTS) technologies, spoken dialogue systems (SDS) have become widely accessible. However, most existing SDS are limited to conventional spoken responses. We present SingingSDS, a cascaded SDS that responds through singing rather than speaking, fostering more affective, memorable, and pleasurable interactions in character-based roleplay and interactive entertainment scenarios. SingingSDS employs a modular ASR-LLM-SVS pipeline and supports a wide range of configurations across character personas, ASR and LLM backends, SVS models, melody sources, and voice profiles, tailored to different needs in terms of latency, quality, and musical style. SingingSDS is available as a plug-and-play web demo, featuring modular, open-source code that supports customization and extension. Demo: this https URL. Code: this https URL.",
    "paper_abstract_zh": "随着自动语音识别（ASR）、大型语言模型（LLMs）和文本转语音（TTS）技术的最新进展，语音对话系统（SDS）已变得广泛可用。然而，大多数现有的SDS仅限于传统的语音响应。我们提出了SingingSDS，一种级联式SDS，它通过唱歌而非说话进行回应，在基于角色的角色扮演和互动娱乐场景中促进更具情感性、记忆性和愉悦性的交互。SingingSDS采用模块化的ASR-LLM-SVS流水线，并支持广泛的配置，包括角色人格、ASR和LLM后端、SVS模型、旋律来源和语音配置文件，以满足不同场景在延迟、质量和音乐风格方面的需求。SingingSDS可作为即插即用的网络演示提供，具有模块化、开源的代码，支持定制和扩展。演示：this https URL。代码：this https URL。",
    "subjects": [
      "Sound (cs.SD)"
    ],
    "update_time": "2025-11-27",
    "paper_authors": "Jionghao Han, Jiatong Shi, Masao Someki, Yuxun Tang, Lan Liu, Yiwen Zhao, Wenhao Feng, Shinji Watanabe",
    "topic": [
      "Speech Recognition",
      "Speech Synthesis",
      "Music Generation"
    ],
    "category": [
      "Music"
    ]
  },
  {
    "paper_title": "CartoonSing: Unifying Human and Nonhuman Timbres in Singing Generation",
    "paper_title_zh": "CartoonSing：统一人类和非人类音色的歌唱生成",
    "paper_id": "2511.21045",
    "paper_abstract": "Singing voice synthesis (SVS) and singing voice conversion (SVC) have achieved remarkable progress in generating natural-sounding human singing. However, existing systems are restricted to human timbres and have limited ability to synthesize voices outside the human range, which are increasingly demanded in creative applications such as video games, movies, and virtual characters. We introduce Non-Human Singing Generation (NHSG), covering non-human singing voice synthesis (NHSVS) and non-human singing voice conversion (NHSVC), as a novel machine learning task for generating musically coherent singing with non-human timbral characteristics. NHSG is particularly challenging due to the scarcity of non-human singing data, the lack of symbolic alignment, and the wide timbral gap between human and non-human voices. To address these challenges, we propose CartoonSing, a unified framework that integrates singing voice synthesis and conversion while bridging human and non-human singing generation. CartoonSing employs a two-stage pipeline: a score representation encoder trained with annotated human singing and a timbre-aware vocoder that reconstructs waveforms for both human and non-human audio. Experiments demonstrate that CartoonSing successfully generates non-human singing voices, generalizes to novel timbres, and extends conventional SVS and SVC toward creative, non-human singing generation.",
    "paper_abstract_zh": "歌唱语音合成（SVS）和歌唱语音转换（SVC）在生成自然的人类歌唱声音方面取得了显著进展。然而，现有系统仅限于人类音色，且在合成人类音域之外的声音方面能力有限，而这些声音在视频游戏、电影和虚拟角色等创意应用中需求日益增长。我们引入了非人类歌唱生成（NHSG），涵盖非人类歌唱语音合成（NHSVS）和非人类歌唱语音转换（NHSVC），作为一个新颖的机器学习任务，用于生成具有非人类音色特征的音乐连贯歌唱。由于非人类歌唱数据的稀缺、符号对齐的缺失以及人类和非人类声音之间的广泛音色差距，NHSG尤其具有挑战性。为应对这些挑战，我们提出了CartoonSing，一个统一框架，整合了歌唱语音合成和转换，同时架起了人类和非人类歌唱生成的桥梁。CartoonSing采用两阶段流程：一个使用标注的人类歌唱训练的乐谱表示编码器，以及一个能够重建人类和非人类音频波形的音色感知声码器。实验证明，CartoonSing成功生成了非人类歌唱声音，能够泛化到新的音色，并将传统的SVS和SVC扩展到创意性的非人类歌唱生成。",
    "subjects": [
      "Sound (cs.SD)"
    ],
    "update_time": "2025-11-27",
    "paper_authors": "Jionghao Han, Jiatong Shi, Zhuoyan Tao, Yuxun Tang, Yiwen Zhao, Gus Xia, Shinji Watanabe",
    "topic": [
      "Music Generation",
      "Speech Synthesis"
    ],
    "category": [
      "Music"
    ]
  },
  {
    "paper_title": "Multi-Reward GRPO for Stable and Prosodic Single-Codebook TTS LLMs at Scale",
    "paper_title_zh": "用于大规模稳定且韵律良好的单码本TTS LLM的多奖励GRPO",
    "paper_id": "2511.21270",
    "paper_abstract": "Recent advances in Large Language Models (LLMs) have transformed text-to-speech (TTS) synthesis, inspiring autoregressive frameworks that represent speech as sequences of discrete codec tokens. Among them, single-codebook TTS LLMs have emerged as compact and streamable architectures that jointly model semantic and acoustic integration. However, despite their efficiency, these models often exhibit unstable prosody, speaker drift, and degraded naturalness. To address these issues, we propose a multi-reward Group Relative Policy Optimization (GRPO) framework that directly optimizes the token generation policy of single-codebook TTS LLMs. Beyond standard intelligibility and speaker similarity objectives, our design integrates three rule-based rewards: a length penalty for duration consistency, an entropy regularization reward for decoding stability, and an LLM-annotated prosody alignment reward that explicitly supervises rhythm. In this prosody reward, an external reasoning LLM predicts multiple plausible pause structures via in-context learning, providing a human-preference-aligned supervisory signal for GRPO training. To assess universality, we further attach a flow-matching (FM) decoder on top of the GRPO-optimized AR backbone and observe consistent additional gains, indicating that our reinforcement optimization enhances the intrinsic AR policy. We further conduct a scalability analysis across data sizes and model scales, revealing that the proposed method consistently enhances prosodic stability, speaker similarity, and overall speech naturalness in single-codebook TTS LLMs.",
    "paper_abstract_zh": "大型语言模型（LLM）的最新进展改变了文本到语音（TTS）合成技术，启发了将语音表示为离散编解码器令牌序列的自回归框架。其中，单码本TTS LLM已成为紧凑且可流式传输的架构，能够联合建模语义和声学集成。然而，尽管这些模型效率高，但常常表现出不稳定的韵律、说话人漂移和自然度下降等问题。为解决这些问题，我们提出了一种多奖励组相对策略优化（GRPO）框架，直接优化单码本TTS LLM的令牌生成策略。除了标准的可懂度和说话人相似性目标外，我们的设计还集成了三种基于规则的奖励：用于持续时间一致性的长度惩罚、用于解码稳定性的熵正则化奖励，以及显式监督节奏的LLM标注韵律对齐奖励。在该韵律奖励中，外部推理LLM通过上下文学习预测多种可能的停顿结构，为GRPO训练提供与人类偏好一致的监督信号。为评估通用性，我们在GRPO优化的自回归主干上附加了一个流匹配（FM）解码器，并观察到一致的性能提升，表明我们的强化优化增强了内在的自回归策略。我们还进行了数据量和模型规模的扩展性分析，揭示该方法在单码本TTS LLM中持续提升了韵律稳定性、说话人相似性和整体语音自然度。",
    "subjects": [
      "Sound (cs.SD)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "update_time": "2025-11-27",
    "paper_authors": "Yicheng Zhong, Peiji Yang, Zhisheng Wang",
    "topic": [
      "Speech Synthesis",
      "Audio Codec"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "SONAR: Spectral-Contrastive Audio Residuals for Generalizable Deepfake Detection",
    "paper_title_zh": "SONAR：用于通用深度伪造检测的谱对比音频残差",
    "paper_id": "2511.21325",
    "paper_abstract": "Deepfake (DF) audio detectors still struggle to generalize to out of distribution inputs. A central reason is spectral bias, the tendency of neural networks to learn low-frequency structure before high-frequency (HF) details, which both causes DF generators to leave HF artifacts and leaves those same artifacts under-exploited by common detectors. To address this gap, we propose Spectral-cONtrastive Audio Residuals (SONAR), a frequency-guided framework that explicitly disentangles an audio signal into complementary representations. An XLSR encoder captures the dominant low-frequency content, while the same cloned path, preceded by learnable SRM, value-constrained high-pass filters, distills faint HF residuals. Frequency cross-attention reunites the two views for long- and short-range frequency dependencies, and a frequency-aware Jensen-Shannon contrastive loss pulls real content-noise pairs together while pushing fake embeddings apart, accelerating optimization and sharpening decision boundaries. Evaluated on the ASVspoof 2021 and in-the-wild benchmarks, SONAR attains state-of-the-art performance and converges four times faster than strong baselines. By elevating faint high-frequency residuals to first-class learning signals, SONAR unveils a fully data-driven, frequency-guided contrastive framework that splits the latent space into two disjoint manifolds: natural-HF for genuine audio and distorted-HF for synthetic audio, thereby sharpening decision boundaries. Because the scheme operates purely at the representation level, it is architecture-agnostic and, in future work, can be seamlessly integrated into any model or modality where subtle high-frequency cues are decisive.",
    "paper_abstract_zh": "深度伪造(DF)音频检测器仍然难以对分布外输入进行泛化。一个核心原因是频谱偏差，即神经网络倾向于先学习低频结构，后学习高频(HF)细节，这导致DF生成器留下高频伪影，而常见检测器则未充分利用这些伪影。为解决这一差距，我们提出了谱对比音频残差(Spectral-cONtrastive Audio Residuals, SONAR)，这是一个频率引导的框架，明确地将音频信号解耦为互补表示。XLSR编码器捕获主导的低频内容，而相同的克隆路径前接可学习的SRM和值约束的高通滤波器，则提炼出微弱的高频残差。频率交叉注意力重新连接两个视图，以处理长程和短程频率依赖关系，频率感知的Jensen-Shannon对比损失将真实内容-噪声对拉近，同时将假嵌入推开，从而加速优化并锐化决策边界。在ASVspoof 2021和野外基准测试中评估，SONAR取得了最先进的性能，并且比强基线收敛速度快四倍。通过将微弱的高频残差提升为一级学习信号，SONAR揭示了一个完全数据驱动的、频率引导的对比框架，该框架将潜在空间划分为两个不相交的流形：自然-HF用于真实音频，失真-HF用于合成音频，从而锐化决策边界。由于该方案完全在表示级别运行，它与架构无关，并在未来工作中可以无缝集成到任何模型或模态中，其中微妙的高频线索是决定性的。",
    "subjects": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)"
    ],
    "update_time": "2025-11-27",
    "paper_authors": "Ido Nitzan HIdekel, Gal lifshitz, Khen Cohen, Dan Raviv",
    "topic": [
      "Audio Representation Learning",
      "Audio Classification"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Generating Separated Singing Vocals Using a Diffusion Model Conditioned on Music Mixtures",
    "paper_title_zh": "使用基于音乐混合条件扩散模型生成分离的人声",
    "paper_id": "2511.21342",
    "paper_abstract": "Separating the individual elements in a musical mixture is an essential process for music analysis and practice. While this is generally addressed using neural networks optimized to mask or transform the time-frequency representation of a mixture to extract the target sources, the flexibility and generalization capabilities of generative diffusion models are giving rise to a novel class of solutions for this complicated task. In this work, we explore singing voice separation from real music recordings using a diffusion model which is trained to generate the solo vocals conditioned on the corresponding mixture. Our approach improves upon prior generative systems and achieves competitive objective scores against non-generative baselines when trained with supplementary data. The iterative nature of diffusion sampling enables the user to control the quality-efficiency trade-off, and also refine the output when needed. We present an ablation study of the sampling algorithm, highlighting the effects of the user-configurable parameters.",
    "paper_abstract_zh": "分离音乐混合中的单个元素是音乐分析和实践中的基本过程。虽然这通常使用优化的神经网络来解决，通过掩码或转换混合的时间频率表示来提取目标源，但生成式扩散模型的灵活性和泛化能力为这一复杂任务带来了新的解决方案。在这项工作中，我们探索使用扩散模型从真实音乐录音中分离人声，该模型经过训练以在相应混合条件下生成独唱人声。我们的方法改进了先前的生成系统，并在使用补充数据训练时与非生成基线方法实现了具有竞争力的客观评分。扩散采样的迭代性质使用户能够控制质量与效率之间的权衡，并在需要时优化输出。我们展示了采样算法的消融研究，突出了用户可配置参数的影响。",
    "subjects": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)"
    ],
    "update_time": "2025-11-27",
    "paper_authors": "Genís Plaja-Roglans, Yun-Ning Hung, Xavier Serra, Igor Pereira",
    "topic": [
      "Music Information Retrieval",
      "Audio Representation Learning"
    ],
    "category": [
      "Music"
    ]
  },
  {
    "paper_title": "HarmonicAttack: An Adaptive Cross-Domain Audio Watermark Removal",
    "paper_title_zh": "HarmonicAttack：一种自适应跨域音频水印去除方法",
    "paper_id": "2511.21577",
    "paper_abstract": "The availability of high-quality, AI-generated audio raises security challenges such as misinformation campaigns and voice-cloning fraud. A key defense against the misuse of AI-generated audio is by watermarking it, so that it can be easily distinguished from genuine audio. As those seeking to misuse AI-generated audio may thus seek to remove audio watermarks, studying effective watermark removal techniques is critical to being able to objectively evaluate the robustness of audio watermarks against removal. Previous watermark removal schemes either assume impractical knowledge of the watermarks they are designed to remove or are computationally expensive, potentially generating a false sense of confidence in current watermark schemes.\nWe introduce HarmonicAttack, an efficient audio watermark removal method that only requires the basic ability to generate the watermarks from the targeted scheme and nothing else. With this, we are able to train a general watermark removal model that is able to remove the watermarks generated by the targeted scheme from any watermarked audio sample. HarmonicAttack employs a dual-path convolutional autoencoder that operates in both temporal and frequency domains, along with GAN-style training, to separate the watermark from the original audio. When evaluated against state-of-the-art watermark schemes AudioSeal, WavMark, and Silentcipher, HarmonicAttack demonstrates greater watermark removal ability than previous watermark removal methods with near real-time performance. Moreover, while HarmonicAttack requires training, we find that it is able to transfer to out-of-distribution samples with minimal degradation in performance.",
    "paper_abstract_zh": "高质量AI生成音频的可用性带来了诸如虚假信息传播和语音克隆欺诈等安全挑战。对抗AI生成音频滥用的一个关键防御手段是通过水印技术，使其能够轻易与真实音频区分开来。然而，那些试图滥用AI生成音频的人可能会寻求去除音频水印，因此研究有效的水印去除技术对于客观评估音频水印对去除的鲁棒性至关重要。现有的水印去除方案要么假设对目标水印拥有不切实际的知识，要么计算成本高昂，这可能导致对当前水印方案产生虚假的安全感。\n我们提出了HarmonicAttack，一种高效的音频水印去除方法，仅需具备从目标方案生成水印的基本能力即可。基于此，我们能够训练一个通用的水印去除模型，能够从任何带水印的音频样本中去除目标方案生成的水印。HarmonicAttack采用双路径卷积自编码器，同时在时域和频域运行，并结合GAN风格训练，以将水印与原始音频分离。在与最先进的水印方案AudioSeal、WavMark和Silentcipher的对比评估中，HarmonicAttack展现出比先前水印去除方法更强的水印去除能力，并接近实时性能。此外，尽管HarmonicAttack需要训练，但我们发现它能够迁移到分布外样本，且性能下降最小。",
    "subjects": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)"
    ],
    "update_time": "2025-11-27",
    "paper_authors": "Kexin Li, Xiao Hu, Ilya Grishchenko, David Lie",
    "topic": [
      "Audio Representation Learning",
      "Other"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Harmonic-Percussive Disentangled Neural Audio Codec for Bandwidth Extension",
    "paper_title_zh": "用于带宽扩展的谐波-打击乐分离神经音频编解码器",
    "paper_id": "2511.21580",
    "paper_abstract": "Bandwidth extension, the task of reconstructing the high-frequency components of an audio signal from its low-pass counterpart, is a long-standing problem in audio processing. While traditional approaches have evolved alongside the broader trends in signal processing, recent advances in neural architectures have significantly improved performance across a wide range of audio tasks, In this work, we extend these advances by framing bandwidth extension as an audio token prediction problem. Specifically, we train a transformer-based language model on the discrete representations produced by a disentangled neural audio codec, where the disentanglement is guided by a Harmonic-Percussive decomposition of the input signals, highlighting spectral structures particularly relevant for bandwidth extension. Our approach introduces a novel codec design that explicitly accounts for the downstream token prediction task, enabling a more effective coupling between codec structure and transformer modeling. This joint design yields high-quality reconstructions of the original signal, as measured by both objective metrics and subjective evaluations. These results highlight the importance of aligning codec disentanglement and representation learning with the generative modeling stage, and demonstrate the potential of global, representation-aware design for advancing bandwidth extension.",
    "paper_abstract_zh": "带宽扩展是从音频信号的低通分量重建其高频分量的任务，是音频处理中的一个长期存在的问题。虽然传统方法随着信号处理的总体趋势而发展，但神经架构的最新进展已显著提高了各种音频任务的性能。在这项工作中，我们将带宽扩展扩展为音频令牌预测问题，从而推进了这些进展。具体而言，我们在由分离神经音频编解码器产生的离散表示上训练基于transformer的语言模型，其中分离过程由输入信号谐波-打击乐分解引导，突出了对带宽扩展特别相关的频谱结构。我们的方法引入了一种新颖的编解码器设计，明确考虑了下游令牌预测任务，实现了编解码器结构与transformer建模之间的更有效耦合。这种联合设计通过客观指标和主观评估测量，产生了原始信号的高质量重建。这些结果强调了将编解码器分离和表示学习与生成建模阶段对齐的重要性，并展示了全局、感知表示设计在推进带宽扩展方面的潜力。",
    "subjects": [
      "Sound (cs.SD)"
    ],
    "update_time": "2025-11-27",
    "paper_authors": "Benoît Giniès, Xiaoyu Bie, Olivier Fercoq, Gaël Richard",
    "topic": [
      "Audio Codec",
      "Audio Representation Learning"
    ],
    "category": [
      "Music"
    ]
  },
  {
    "paper_title": "ASR Error Correction in Low-Resource Burmese with Alignment-Enhanced Transformers using Phonetic Features",
    "paper_title_zh": "基于音素特征的增强对齐Transformer在低资源缅甸语ASR错误纠正中的应用",
    "paper_id": "2511.21088",
    "paper_abstract": "This paper investigates sequence-to-sequence Transformer models for automatic speech recognition (ASR) error correction in low-resource Burmese, focusing on different feature integration strategies including IPA and alignment information. To our knowledge, this is the first study addressing ASR error correction specifically for Burmese. We evaluate five ASR backbones and show that our ASR Error Correction (AEC) approaches consistently improve word- and character-level accuracy over baseline outputs. The proposed AEC model, combining IPA and alignment features, reduced the average WER of ASR models from 51.56 to 39.82 before augmentation (and 51.56 to 43.59 after augmentation) and improving chrF++ scores from 0.5864 to 0.627, demonstrating consistent gains over the baseline ASR outputs without AEC. Our results highlight the robustness of AEC and the importance of feature design for improving ASR outputs in low-resource settings.",
    "paper_abstract_zh": "本文研究了针对低资源缅甸语自动语音识别(ASR)错误的序列到序列Transformer模型纠正方法，重点关注包括IPA(国际音标)和对齐信息在内的不同特征集成策略。据我们所知，这是首个专门针对缅甸语ASR错误纠正的研究。我们评估了五种ASR骨干模型，并表明我们的ASR错误纠正(AEC)方法在单词和字符级准确率上始终优于基线输出。结合IPA和对齐特征的AEC模型将ASR模型的平均词错误率(WER)从51.56降低到39.82(数据增强后为51.56到43.59)，并将chrF++分数从0.5864提高到0.627，这表明在不使用AEC的基线ASR输出上实现了持续的性能提升。我们的研究结果突显了AEC的鲁棒性以及特征设计在改善低资源环境下ASR输出的重要性。",
    "subjects": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)"
    ],
    "update_time": "2025-11-27",
    "paper_authors": "Ye Bhone Lin, Thura Aung, Ye Kyaw Thu, Thazin Myint Oo",
    "topic": [
      "Speech Recognition",
      "Audio Representation Learning"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "AV-Edit: Multimodal Generative Sound Effect Editing via Audio-Visual Semantic Joint Control",
    "paper_title_zh": "AV-Edit：基于音频-视觉语义联合控制的多模态生成音效编辑",
    "paper_id": "2511.21146",
    "paper_abstract": "Sound effect editing-modifying audio by adding, removing, or replacing elements-remains constrained by existing approaches that rely solely on low-level signal processing or coarse text prompts, often resulting in limited flexibility and suboptimal audio quality. To address this, we propose AV-Edit, a generative sound effect editing framework that enables fine-grained editing of existing audio tracks in videos by jointly leveraging visual, audio, and text semantics. Specifically, the proposed method employs a specially designed contrastive audio-visual masking autoencoder (CAV-MAE-Edit) for multimodal pre-training, learning aligned cross-modal representations. These representations are then used to train an editorial Multimodal Diffusion Transformer (MM-DiT) capable of removing visually irrelevant sounds and generating missing audio elements consistent with video content through a correlation-based feature gating training strategy. Furthermore, we construct a dedicated video-based sound editing dataset as an evaluation benchmark. Experiments demonstrate that the proposed AV-Edit generates high-quality audio with precise modifications based on visual content, achieving state-of-the-art performance in the field of sound effect editing and exhibiting strong competitiveness in the domain of audio generation.",
    "paper_abstract_zh": "音效编辑——通过添加、移除或替换元素来修改音频——仍然受限于现有方法，这些方法仅依赖低级信号处理或粗略的文本提示，通常导致灵活性和音频质量有限。为解决这一问题，我们提出了AV-Edit，一种生成式音效编辑框架，通过联合利用视觉、音频和文本语义，实现对视频中现有音轨的细粒度编辑。具体而言，该方法采用专门设计的对比音频-视觉掩码自编码器（CAV-MAE-Edit）进行多模态预训练，学习对齐的跨模态表示。这些表示随后用于训练编辑式多模态扩散Transformer（MM-DiT），通过基于相关性的特征门控训练策略，能够移除视觉无关的声音并生成与视频内容一致的缺失音频元素。此外，我们构建了一个专用的基于视频的音效编辑数据集作为评估基准。实验表明，所提出的AV-Edit能够根据视觉内容生成高质量且精确修改的音频，在音效编辑领域取得了最先进的性能，并在音频生成领域展现出强大的竞争力。",
    "subjects": [
      "Multimedia (cs.MM)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Sound (cs.SD)"
    ],
    "update_time": "2025-11-27",
    "paper_authors": "Xinyue Guo, Xiaoran Yang, Lipan Zhang, Jianxuan Yang, Zhao Wang, Jian Luan",
    "topic": [
      "Audio Representation Learning",
      "Video Generation"
    ],
    "category": [
      "Image&Video"
    ]
  }
]