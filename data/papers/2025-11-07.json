[
  {
    "paper_title": "CardioPHON: Quality assessment and self-supervised pretraining for screening of cardiac function based on phonocardiogram recordings",
    "paper_title_zh": "CardioPHON：基于心音记录的心脏功能筛查的质量评估与自监督预训练",
    "paper_id": "2511.04533",
    "paper_abstract": "Remote monitoring of cardiovascular diseases plays an essential role in early detection of abnormal cardiac function, enabling timely intervention, improved preventive care, and personalized patient treatment. Abnormalities in the heart sounds can be detected automatically via computer-assisted decision support systems, and used as the first-line screening tool for detection of cardiovascular problems, or for monitoring the effects of treatments and interventions. We propose in this paper CardioPHON, an integrated heart sound quality assessment and classification tool that can be used for screening of abnormal cardiac function from phonocardiogram recordings. The model is pretrained in a self-supervised fashion on a collection of six small- and mid-sized heart sound datasets, enables automatic removal of low quality recordings to ensure that subtle sounds of heart abnormalities are not misdiagnosed, and provides a state-of-the-art performance for the heart sound classification task. The multimodal model that combines audio and socio-demographic features demonstrated superior performance, achieving the best ranking on the official leaderboard of the 2022 George B. Moody PhysioNet heart sound challenge, whereas the unimodal model, that is based only on phonocardiogram recordings, holds the first position among the unimodal approaches (a total rank 4), surpassing the models utilizing multiple modalities. CardioPHON is the first publicly released pretrained model in the domain of heart sound recordings, facilitating the development of data-efficient artificial intelligence models that can generalize to various downstream tasks in cardiovascular diagnostics.",
    "paper_abstract_zh": "心血管疾病的远程监测在早期发现异常心脏功能方面发挥着关键作用，能够实现及时干预、改善预防和个性化患者治疗。心脏声音的异常可以通过计算机辅助决策支持系统自动检测，并作为检测心血管问题的第一线筛查工具，或用于监测治疗和干预的效果。本文提出了CardioPHON，一个集成的声音质量评估和分类工具，可用于从心音记录中筛查异常心脏功能。该模型在六个中小型心音数据集上以自监督方式进行预训练，能够自动移除低质量记录，确保细微的心脏异常声音不会被误诊，并在心音分类任务上提供了最先进的性能。结合音频和社会人口统计特征的多模态模型表现出优越的性能，在2022年George B. Moody PhysioNet心音挑战赛的官方排行榜上获得了最佳排名，而仅基于心音记录的单模态模型在单模态方法中排名第一（总排名第4），超越了利用多模态的模型。CardioPHON是心音领域首个公开发布的预训练模型，促进了数据高效人工智能模型的发展，这些模型可以泛化到心血管诊断的多种下游任务。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2025-11-07",
    "paper_authors": "Vladimir Despotovic, Peter Pocta, Andrej Zgank",
    "topic": [
      "Audio Representation Learning",
      "Audio Classification"
    ],
    "category": [
      "Other"
    ]
  },
  {
    "paper_title": "MusRec: Zero-Shot Text-to-Music Editing via Rectified Flow and Diffusion Transformers",
    "paper_title_zh": "MusRec: 通过修正流和扩散变换器的零样本文本到音乐编辑",
    "paper_id": "2511.04376",
    "paper_abstract": "Music editing has emerged as an important and practical area of artificial intelligence, with applications ranging from video game and film music production to personalizing existing tracks according to user preferences. However, existing models face significant limitations, such as being restricted to editing synthesized music generated by their own models, requiring highly precise prompts, or necessitating task-specific retraining, thus lacking true zero-shot capability. Leveraging recent advances in rectified flow and diffusion transformers, we introduce MusRec, the first zero-shot text-to-music editing model capable of performing diverse editing tasks on real-world music efficiently and effectively. Experimental results demonstrate that our approach outperforms existing methods in preserving musical content, structural consistency, and editing fidelity, establishing a strong foundation for controllable music editing in real-world scenarios.",
    "paper_abstract_zh": "音乐编辑已成为人工智能中一个重要且实用的领域，应用范围从游戏和电影音乐制作到根据用户偏好个性化现有曲目。然而，现有模型面临显著限制，例如仅限于编辑由自身模型生成的合成音乐，需要高度精确的提示，或需要针对特定任务重新训练，因此缺乏真正的零样本能力。利用修正流和扩散变换器的最新进展，我们引入了MusRec，这是首个能够高效有效地对现实世界音乐执行多样化编辑任务的零样本文本到音乐编辑模型。实验结果表明，我们的方法在保留音乐内容、结构一致性和编辑保真度方面优于现有方法，为现实场景中的可控音乐编辑奠定了坚实基础。",
    "subjects": [
      "Artificial Intelligence (cs.AI)",
      "Audio and Speech Processing (eess.AS)",
      "Multimedia (cs.MM)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)"
    ],
    "update_time": "2025-11-07",
    "paper_authors": "Ali Boudaghi, Hadi Zare",
    "topic": [
      "Music Generation",
      "Music Information Retrieval"
    ],
    "category": [
      "Music"
    ]
  },
  {
    "paper_title": "PromptSep: Generative Audio Separation via Multimodal Prompting",
    "paper_title_zh": "PromptSep: 通过多模态提示生成式音频分离",
    "paper_id": "2511.04623",
    "paper_abstract": "Recent breakthroughs in language-queried audio source separation (LASS) have shown that generative models can achieve higher separation audio quality than traditional masking-based approaches. However, two key limitations restrict their practical use: (1) users often require operations beyond separation, such as sound removal; and (2) relying solely on text prompts can be unintuitive for specifying sound sources. In this paper, we propose PromptSep to extend LASS into a broader framework for general-purpose sound separation. PromptSep leverages a conditional diffusion model enhanced with elaborated data simulation to enable both audio extraction and sound removal. To move beyond text-only queries, we incorporate vocal imitation as an additional and more intuitive conditioning modality for our model, by incorporating Sketch2Sound as a data augmentation strategy. Both objective and subjective evaluations on multiple benchmarks demonstrate that PromptSep achieves state-of-the-art performance in sound removal and vocal-imitation-guided source separation, while maintaining competitive results on language-queried source separation.",
    "paper_abstract_zh": "最近在语言查询音频源分离(LASS)方面的突破表明，生成模型可以实现比传统基于掩码的方法更高的音频分离质量。然而，两个关键限制限制了它们的实际应用：(1)用户通常需要分离以外的操作，如声音去除；(2)仅依赖文本提示来指定声音源可能不够直观。在本文中，我们提出PromptSep，将LASS扩展为一个更通用的声音分离框架。PromptSep利用增强条件扩散模型，通过精心设计的数据模拟实现音频提取和声音去除。为了超越纯文本查询，我们将声音模仿作为额外的、更直观的调节模态纳入模型，采用Sketch2Sound作为数据增强策略。在多个基准测试上的客观和主观评估表明，PromptSep在声音去除和声音模仿引导的源分离方面取得了最先进的性能，同时在语言查询的源分离上保持了具有竞争力的结果。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "update_time": "2025-11-07",
    "paper_authors": "Yutong Wen, Ke Chen, Prem Seetharaman, Oriol Nieto, Jiaqi Su, Rithesh Kumar, Minje Kim, Paris Smaragdis, Zeyu Jin, Justin Salamon",
    "topic": [
      "Audio Representation Learning",
      "Speech Enhancement",
      "Other"
    ],
    "category": [
      "Other"
    ]
  },
  {
    "paper_title": "MIDI-LLM: Adapting Large Language Models for Text-to-MIDI Music Generation",
    "paper_title_zh": "MIDI-LLM：适配大型语言模型用于文本到MIDI音乐生成",
    "paper_id": "2511.03942",
    "paper_abstract": "We present MIDI-LLM, an LLM for generating multitrack MIDI music from free-form text prompts. Our approach expands a text LLM's vocabulary to include MIDI tokens, and uses a two-stage training recipe to endow text-to-MIDI abilities. By preserving the original LLM's parameter structure, we can directly leverage the vLLM library for accelerated inference. Experiments show that MIDI-LLM achieves higher quality, better text control, and faster inference compared to the recent Text2midi model. Live demo at this https URL.",
    "paper_abstract_zh": "我们提出了MIDI-LLM，这是一种用于根据自由形式文本提示生成多轨道MIDI音乐的大型语言模型。我们的方法扩展了文本LLM的词汇表以包含MIDI标记，并采用两阶段训练方案赋予文本到MIDI的能力。通过保留原始LLM的参数结构，我们可以直接利用vLLM库进行加速推理。实验表明，与最近的Text2midi模型相比，MIDI-LLM实现了更高的质量、更好的文本控制和更快的推理速度。实时演示请访问此https URL。",
    "subjects": [
      "Sound (cs.SD)",
      "Computation and Language (cs.CL)",
      "Multimedia (cs.MM)"
    ],
    "update_time": "2025-11-07",
    "paper_authors": "Shih-Lun Wu, Yoon Kim, Cheng-Zhi Anna Huang",
    "topic": [
      "Music Generation",
      "Music Information Retrieval"
    ],
    "category": [
      "Music"
    ]
  },
  {
    "paper_title": "CantoASR: Prosody-Aware ASR-LALM Collaboration for Low-Resource Cantonese",
    "paper_title_zh": "CantoASR：面向低资源粤语的韵律感知ASR-LALM协作",
    "paper_id": "2511.04139",
    "paper_abstract": "Automatic speech recognition (ASR) is critical for language accessibility, yet low-resource Cantonese remains challenging due to limited annotated data, six lexical tones, tone sandhi, and accent variation. Existing ASR models, such as Whisper, often suffer from high word error rates. Large audio-language models (LALMs), in contrast, can leverage broader contextual reasoning but still require explicit tonal and prosodic acoustic cues. We introduce CantoASR, a collaborative ASR-LALM error correction framework that integrates forced alignment for acoustic feature extraction, a LoRA-finetuned Whisper for improved tone discrimination, and an instruction-tuned Qwen-Audio for prosody-aware correction. Evaluations on spontaneous Cantonese data show substantial CER gains over Whisper-Large-V3. These findings suggest that integrating acoustic cues with LALM reasoning provides a scalable strategy for low-resource tonal and dialectal ASR.",
    "paper_abstract_zh": "自动语音识别(ASR)对语言可访问性至关重要，但由于标注数据有限、六个声调、连变调和口音变化，低资源粤语仍然具有挑战性。现有的ASR模型（如Whisper）通常存在较高的词错误率。相比之下，大型音频语言模型(LALM)可以利用更广泛的上下文推理，但仍需要明确的声调和韵律声学线索。我们介绍了CantoASR，这是一个协作的ASR-LALM错误校正框架，集成了强制对齐进行声学特征提取，使用LoRA微调的Whisper改进声调辨别能力，以及指令微调的Qwen-Audio进行韵律感知校正。在粤语自发数据的评估中，与Whisper-Large-V3相比，CER显著提高。这些发现表明，将声学线索与LALM推理相结合，为低资源声调和方言ASR提供了一种可扩展的策略。",
    "subjects": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)"
    ],
    "update_time": "2025-11-07",
    "paper_authors": "Dazhong Chen, Yi-Cheng Lin, Yuchen Huang, Ziwei Gong, Di Jiang, Zeying Xie, Yi R., Fung",
    "topic": [
      "Speech Recognition",
      "Audio Representation Learning"
    ],
    "category": [
      "Speech"
    ]
  }
]