[
  {
    "paper_title": "Discriminating real and synthetic super-resolved audio samples using embedding-based classifiers",
    "paper_title_zh": "基于嵌入分类器的真实与合成超分辨率音频样本鉴别",
    "paper_id": "2601.03443",
    "paper_abstract": "Generative adversarial networks (GANs) and diffusion models have recently achieved state-of-the-art performance in audio super-resolution (ADSR), producing perceptually convincing wideband audio from narrowband inputs. However, existing evaluations primarily rely on signal-level or perceptual metrics, leaving open the question of how closely the distributions of synthetic super-resolved and real wideband audio match. Here we address this problem by analyzing the separability of real and super-resolved audio in various embedding spaces. We consider both middle-band ($4\\to 16$~kHz) and full-band ($16\\to 48$~kHz) upsampling tasks for speech and music, training linear classifiers to distinguish real from synthetic samples based on multiple types of audio embeddings. Comparisons with objective metrics and subjective listening tests reveal that embedding-based classifiers achieve near-perfect separation, even when the generated audio attains high perceptual quality and state-of-the-art metric scores. This behavior is consistent across datasets and models, including recent diffusion-based approaches, highlighting a persistent gap between perceptual quality and true distributional fidelity in ADSR models.",
    "paper_abstract_zh": "生成对抗网络(GANs)和扩散模型最近在音频超分辨率(ADSR)方面取得了最先进的性能，能够从窄带输入生成感知上令人信服的宽带音频。然而，现有的评估主要依赖于信号级或感知指标，这留下了合成超分辨率音频与真实宽带音频的分布匹配程度如何的问题。我们通过分析各种嵌入空间中真实和超分辨率音频的可分性来解决这个问题。我们考虑了语音和音乐的中频段(4→16 kHz)和全频段(16→48 kHz)上采样任务，训练线性分类器基于多种类型的音频嵌入来区分真实和合成样本。与客观指标和主观听音测试的比较表明，基于嵌入的分类器实现了近乎完美的分离，即使生成的音频获得了高感知质量和最先进的指标分数。这种行为在数据集和模型中是一致的，包括最近的基于扩散的方法，这突显了ADSR模型中感知质量和真实分布保真度之间的持续差距。",
    "subjects": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Audio and Speech Processing (eess.AS)",
      "Signal Processing (eess.SP)"
    ],
    "update_time": "2026-01-08",
    "paper_authors": "Mikhail Silaev, Konstantinos Drossos, Tuomas Virtanen",
    "topic": [
      "Audio Representation Learning",
      "Audio Classification"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Learning from Limited Labels: Transductive Graph Label Propagation for Indian Music Analysis",
    "paper_title_zh": "从有限标签中学习：用于印度音乐分析的图标签传播",
    "paper_id": "2601.03626",
    "paper_abstract": "Supervised machine learning frameworks rely on extensive labeled datasets for robust performance on real-world tasks. However, there is a lack of large annotated datasets in audio and music domains, as annotating such recordings is resource-intensive, laborious, and often require expert domain knowledge. In this work, we explore the use of label propagation (LP), a graph-based semi-supervised learning technique, for automatically labeling the unlabeled set in an unsupervised manner. By constructing a similarity graph over audio embeddings, we propagate limited label information from a small annotated subset to a larger unlabeled corpus in a transductive, semi-supervised setting. We apply this method to two tasks in Indian Art Music (IAM): Raga identification and Instrument classification. For both these tasks, we integrate multiple public datasets along with additional recordings we acquire from Prasar Bharati Archives to perform LP. Our experiments demonstrate that LP significantly reduces labeling overhead and produces higher-quality annotations compared to conventional baseline methods, including those based on pretrained inductive models. These results highlight the potential of graph-based semi-supervised learning to democratize data annotation and accelerate progress in music information retrieval.",
    "paper_abstract_zh": "监督机器学习框架依赖于大量标记数据集，以在实际任务上获得稳健的性能。然而，在音频和音乐领域缺乏大型标注数据集，因为标注此类录音资源密集、劳动量大，并且通常需要专家领域知识。在这项工作中，我们探索了标签传播（LP）的使用，这是一种基于图的半监督学习技术，用于以无监督方式自动标记未标记集。通过在音频嵌入上构建相似性图，我们在传递式、半监督设置中，将有限的标签信息从小型标注子集传播到更大的未标记语料库。我们将此方法应用于印度艺术音乐（IAM）中的两个任务：Raga识别和乐器分类。对于这两个任务，我们整合了多个公共数据集以及我们从Prasar Bharati档案中获取的额外录音来执行LP。我们的实验表明，与包括基于预训练归纳模型的基线方法在内的传统基线方法相比，LP显著减少了标记开销并产生了更高质量的标注。这些结果突显了基于图的半监督学习在数据标注民主化和加速音乐信息检索进展方面的潜力。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)",
      "Machine Learning (cs.LG)"
    ],
    "update_time": "2026-01-08",
    "paper_authors": "Parampreet Singh, Akshay Raina, Sayeedul Islam Sheikh, Vipul Arora",
    "topic": [
      "Audio Representation Learning",
      "Music Information Retrieval",
      "Audio Classification"
    ],
    "category": [
      "Music"
    ]
  },
  {
    "paper_title": "ReStyle-TTS: Relative and Continuous Style Control for Zero-Shot Speech Synthesis",
    "paper_title_zh": "ReStyle-TTS: 零样本语音合成中相对和连续的风格控制",
    "paper_id": "2601.03632",
    "paper_abstract": "Zero-shot text-to-speech models can clone a speaker's timbre from a short reference audio, but they also strongly inherit the speaking style present in the reference. As a result, synthesizing speech with a desired style often requires carefully selecting reference audio, which is impractical when only limited or mismatched references are available. While recent controllable TTS methods attempt to address this issue, they typically rely on absolute style targets and discrete textual prompts, and therefore do not support continuous and reference-relative style control. We propose ReStyle-TTS, a framework that enables continuous and reference-relative style control in zero-shot TTS. Our key insight is that effective style control requires first reducing the model's implicit dependence on reference style before introducing explicit control mechanisms. To this end, we introduce Decoupled Classifier-Free Guidance (DCFG), which independently controls text and reference guidance, reducing reliance on reference style while preserving text fidelity. On top of this, we apply style-specific LoRAs together with Orthogonal LoRA Fusion to enable continuous and disentangled multi-attribute control, and introduce a Timbre Consistency Optimization module to mitigate timbre drift caused by weakened reference guidance. Experiments show that ReStyle-TTS enables user-friendly, continuous, and relative control over pitch, energy, and multiple emotions while maintaining intelligibility and speaker timbre, and performs robustly in challenging mismatched reference-target style scenarios.",
    "paper_abstract_zh": "零样本文本到语音模型能够从简短的参考音频中克隆说话人的音色，但它们也会强烈继承参考音频中存在的说话风格。因此，当只有有限或不匹配的参考音频可用时，合成具有期望风格的语音通常需要仔细选择参考音频，这是不切实际的。虽然最近的可控TTS方法试图解决这个问题，但它们通常依赖于绝对风格目标和离散的文本提示，因此不支持连续和参考相对的风格控制。我们提出了ReStyle-TTS，一个能够在零样本TTS中实现连续和参考相对风格控制的框架。我们的关键见解是，有效的风格控制首先需要减少模型对参考风格的隐式依赖，然后再引入显式控制机制。为此，我们引入了解耦的无分类器指导（DCFG），它可以独立控制文本和参考指导，减少对参考风格的依赖，同时保持文本保真度。在此基础上，我们应用了特定风格的LoRA和正交LoRA融合，以实现连续且解耦的多属性控制，并引入了音色一致性优化模块，以减轻因参考指导减弱而导致的音色漂移。实验表明，ReStyle-TTS能够在保持可理解性和说话人音色的同时，实现对音高、能量和多种情绪的用户友好、连续和相对控制，并在具有挑战性的不匹配参考-目标风格场景中表现稳健。",
    "subjects": [
      "Artificial Intelligence (cs.AI)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2026-01-08",
    "paper_authors": "Haitao Li, Chunxiang Jin, Chenglin Li, Wenhao Guan, Zhengxing Huang, Xie Chen",
    "topic": [
      "Speech Synthesis",
      "Audio Representation Learning"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "TellWhisper: Tell Whisper Who Speaks When",
    "paper_title_zh": "TellWhisper: 告诉Whisper谁在何时说话",
    "paper_id": "2601.03712",
    "paper_abstract": "Multi-speaker automatic speech recognition (MASR) aims to predict ''who spoke when and what'' from multi-speaker speech, a key technology for multi-party dialogue understanding. However, most existing approaches decouple temporal modeling and speaker modeling when addressing ''when'' and ''who'': some inject speaker cues before encoding (e.g., speaker masking), which can cause irreversible information loss; others fuse identity by mixing speaker posteriors after encoding, which may entangle acoustic content with speaker identity. This separation is brittle under rapid turn-taking and overlapping speech, often leading to degraded performance. To address these limitations, we propose TellWhisper, a unified framework that jointly models speaker identity and temporal within the speech encoder. Specifically, we design TS-RoPE, a time-speaker rotary positional encoding: time coordinates are derived from frame indices, while speaker coordinates are derived from speaker activity and pause cues. By applying region-specific rotation angles, the model explicitly captures per-speaker continuity, speaker-turn transitions, and state dynamics, enabling the attention mechanism to simultaneously attend to ''when'' and ''who''. Moreover, to estimate frame-level speaker activity, we develop Hyper-SD, which casts speaker classification in hyperbolic space to enhance inter-class separation and refine speaker-activity estimates. Extensive experiments demonstrate the effectiveness of the proposed approach.",
    "paper_abstract_zh": "多说话人自动语音识别（MASR）旨在从多说话人语音中预测'谁在何时说了什么'，这是多方对话理解的关键技术。然而，大多数现有方法在处理'何时'和'谁'时将时间建模和说话人建模解耦：一些在编码前注入说话人线索（如说话人掩码），这可能导致不可逆的信息损失；另一些在编码后通过混合说话人后验来融合身份，这可能会将声学内容与说话人身份纠缠在一起。这种分离在快速轮流发言和重叠语音情况下很脆弱，常常导致性能下降。为解决这些局限性，我们提出了TellWhisper，一个在语音编码器中联合建模说话人身份和时间的统一框架。具体来说，我们设计了TS-RoPE，一种时间-说话人旋转位置编码：时间坐标来自帧索引，而说话人坐标来自说话人活动和暂停线索。通过应用特定区域的旋转角度，模型明确捕获每个说话人的连续性、说话人转换和状态动态，使注意力机制能够同时关注'何时'和'谁'。此外，为了估计帧级说话人活动，我们开发了Hyper-SD，它在双曲空间中进行说话人分类，以增强类间分离并细化说话人活动估计。大量实验证明了所提方法的有效性。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2026-01-08",
    "paper_authors": "Yifan Hu, Peiji Yang, Zhisheng Wang, Yicheng Zhong, Rui Liu",
    "topic": [
      "Speech Recognition",
      "Audio Representation Learning"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Sound Event Detection with Boundary-Aware Optimization and Inference",
    "paper_title_zh": "具有边界感知优化和推理的声音事件检测",
    "paper_id": "2601.04178",
    "paper_abstract": "Temporal detection problems appear in many fields including time-series estimation, activity recognition and sound event detection (SED). In this work, we propose a new approach to temporal event modeling by explicitly modeling event onsets and offsets, and by introducing boundary-aware optimization and inference strategies that substantially enhance temporal event detection. The presented methodology incorporates new temporal modeling layers - Recurrent Event Detection (RED) and Event Proposal Network (EPN) - which, together with tailored loss functions, enable more effective and precise temporal event detection. We evaluate the proposed method in the SED domain using a subset of the temporally-strongly annotated portion of AudioSet. Experimental results show that our approach not only outperforms traditional frame-wise SED models with state-of-the-art post-processing, but also removes the need for post-processing hyperparameter tuning, and scales to achieve new state-of-the-art performance across all AudioSet Strong classes.",
    "paper_abstract_zh": "时间检测问题出现在许多领域，包括时间序列估计、活动识别和声音事件检测（SED）。在这项工作中，我们提出了一种新的时间事件建模方法，通过显式建模事件的开始和结束时间，并引入边界感知的优化和推理策略，显著提高了时间事件检测的性能。所提出的方法集成了新的时间建模层——循环事件检测（RED）和事件提议网络（EPN），这些层与定制的损失函数一起，能够实现更有效和精确的时间事件检测。我们使用AudioSet时间强标注子集在SED领域评估了所提出的方法。实验结果表明，我们的方法不仅优于使用最先进后处理的传统帧级SED模型，而且消除了后处理超参数调优的需要，并扩展到在所有AudioSet Strong类别上实现新的最先进性能。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "update_time": "2026-01-08",
    "paper_authors": "Florian Schmid, Chi Ian Tang, Sanjeel Parekh, Vamsi Krishna Ithapu, Juan Azcarreta Ortiz, Giacomo Ferroni, Yijun Qian, Arnoldas Jasonas, Cosmin Frateanu, Camilla Clark, Gerhard Widmer, Çağdaş Bilen",
    "topic": [
      "Audio Classification"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Investigation into respiratory sound classification for an imbalanced data set using hybrid LSTM-KAN architectures",
    "paper_title_zh": "使用混合LSTM-KAN架构处理不平衡数据集的呼吸音分类研究",
    "paper_id": "2601.03610",
    "paper_abstract": "Respiratory sounds captured via auscultation contain critical clues for diagnosing pulmonary conditions. Automated classification of these sounds faces challenges due to subtle acoustic differences and severe class imbalance in clinical datasets. This study investigates respiratory sound classification with a focus on mitigating pronounced class imbalance. We propose a hybrid deep learning model that combines a Long Short-Term Memory (LSTM) network for sequential feature encoding with a Kolmogorov-Arnold Network (KAN) for classification. The model is integrated with a comprehensive feature extraction pipeline and targeted imbalance mitigation strategies. Experiments were conducted on a public respiratory sound database comprising six classes with a highly skewed distribution. Techniques such as focal loss, class-specific data augmentation, and Synthetic Minority Over-sampling Technique (SMOTE) were employed to enhance minority class recognition. The proposed Hybrid LSTM-KAN model achieves an overall accuracy of 94.6 percent and a macro-averaged F1 score of 0.703, despite the dominant COPD class accounting for over 86 percent of the data. Improved detection performance is observed for minority classes compared to baseline approaches, demonstrating the effectiveness of the proposed architecture for imbalanced respiratory sound classification.",
    "paper_abstract_zh": "通过听诊捕获的呼吸音包含诊断肺部状况的关键线索。由于临床数据集中细微的声学差异和严重的类别不平衡问题，这些声音的自动分类面临挑战。本研究重点研究呼吸音分类，以减轻显著的类别不平衡问题。我们提出了一种混合深度学习模型，该模型结合了用于序列特征编码的长短期记忆（LSTM）网络和用于分类的柯尔莫哥洛夫-阿诺德网络（KAN）。该模型与全面特征提取流程和针对性的不平衡缓解策略集成。实验在一个包含六个类别且分布高度倾斜的公共呼吸音数据库上进行。采用了焦点损失、类别特定的数据增强和合成少数类过采样技术（SMOTE）等技术，以增强少数类别的识别能力。尽管占主导地位的COPD类别占数据总量的86%以上，所提出的混合LSTM-KAN模型仍实现了94.6%的整体准确率和0.703的宏平均F1分数。与基线方法相比，少数类别的检测性能有所提高，证明了所提出的架构在不平衡呼吸音分类中的有效性。",
    "subjects": [
      "Artificial Intelligence (cs.AI)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2026-01-08",
    "paper_authors": "Nithinkumar K.V, Anand R",
    "topic": [
      "Audio Classification"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Mathematical Foundations of Polyphonic Music Generation via Structural Inductive Bias",
    "paper_title_zh": "基于结构归纳偏置的复调音乐生成数学基础",
    "paper_id": "2601.03612",
    "paper_abstract": "This monograph introduces a novel approach to polyphonic music generation by addressing the \"Missing Middle\" problem through structural inductive bias. Focusing on Beethoven's piano sonatas as a case study, we empirically verify the independence of pitch and hand attributes using normalized mutual information (NMI=0.167) and propose the Smart Embedding architecture, achieving a 48.30% reduction in parameters. We provide rigorous mathematical proofs using information theory (negligible loss bounded at 0.153 bits), Rademacher complexity (28.09% tighter generalization bound), and category theory to demonstrate improved stability and generalization. Empirical results show a 9.47% reduction in validation loss, confirmed by SVD analysis and an expert listening study (N=53). This dual theoretical and applied framework bridges gaps in AI music generation, offering verifiable insights for mathematically grounded deep learning.",
    "paper_abstract_zh": "本专著通过结构归纳偏置解决“缺失中间层”问题，介绍了一种复调音乐生成的新方法。以贝多芬钢琴奏鸣曲为案例研究，我们利用归一化互信息（NMI=0.167）实证验证了音高与手部属性的独立性，并提出了Smart Embedding架构，实现了48.30%的参数削减。我们利用信息论（损失上限可忽略不计，仅为0.153比特）、Rademacher复杂度（泛化边界收紧了28.09%）和范畴论提供了严格的数学证明，以论证模型在稳定性和泛化能力上的提升。实证结果显示验证损失降低了9.47%，这一结果得到了SVD分析和专家听感测试（N=53）的证实。这一双重理论与应用框架弥合了AI音乐生成的差距，为基于数学的深度学习提供了可验证的见解。",
    "subjects": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2026-01-08",
    "paper_authors": "Joonwon Seo",
    "topic": [
      "Music Generation"
    ],
    "category": [
      "Music"
    ]
  },
  {
    "paper_title": "Analyzing Reasoning Shifts in Audio Deepfake Detection under Adversarial Attacks: The Reasoning Tax versus Shield Bifurcation",
    "paper_title_zh": "分析对抗攻击下音频深度伪造检测中的推理转变：推理税与屏蔽分化",
    "paper_id": "2601.03615",
    "paper_abstract": "Audio Language Models (ALMs) offer a promising shift towards explainable audio deepfake detections (ADDs), moving beyond \\textit{black-box} classifiers by providing some level of transparency into their predictions via reasoning traces. This necessitates a new class of model robustness analysis: robustness of the predictive reasoning under adversarial attacks, which goes beyond existing paradigm that mainly focuses on the shifts of the final predictions (e.g., fake v.s. real). To analyze such reasoning shifts, we introduce a forensic auditing framework to evaluate the robustness of ALMs' reasoning under adversarial attacks in three inter-connected dimensions: acoustic perception, cognitive coherence, and cognitive dissonance. Our systematic analysis reveals that explicit reasoning does not universally enhance robustness. Instead, we observe a bifurcation: for models exhibiting robust acoustic perception, reasoning acts as a defensive \\textit{``shield''}, protecting them from adversarial attacks. However, for others, it imposes a performance \\textit{``tax''}, particularly under linguistic attacks which reduce cognitive coherence and increase attack success rate. Crucially, even when classification fails, high cognitive dissonance can serve as a \\textit{silent alarm}, flagging potential manipulation. Overall, this work provides a critical evaluation of the role of reasoning in forensic audio deepfake analysis and its vulnerabilities.",
    "paper_abstract_zh": "音频语言模型(ALMs)为可解释的音频深度伪造检测(ADDs)提供了有前景的转变，通过推理轨迹提供一定程度的透明度，超越了黑盒分类器。这需要一类新的模型鲁棒性分析：对抗攻击下预测推理的鲁棒性，超越了现有主要关注最终预测(如伪造与真实)转变的范式。为了分析这种推理转变，我们引入了一个法证审计框架，从三个相互关联的维度评估ALMs在对抗攻击下推理的鲁棒性：听觉感知、认知一致性和认知失调。我们的系统分析表明，显式推理并不普遍增强鲁棒性。相反，我们观察到一种分化：对于表现出鲁棒听觉感知的模型，推理充当防御性'屏蔽'，保护它们免受对抗攻击。然而，对于其他模型，推理 imposes 性能'税'，特别是在语言学攻击下，这些攻击降低认知一致性并提高攻击成功率。关键的是，即使分类失败，高认知失调也可以作为'无声警报'，标记潜在的操纵。总体而言，这项工作对推理在法证音频深度伪造分析中的作用及其脆弱性进行了关键评估。",
    "subjects": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)",
      "Computation and Language (cs.CL)"
    ],
    "update_time": "2026-01-08",
    "paper_authors": "Binh Nguyen, Thai Le",
    "topic": [
      "Audio Classification",
      "Other"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Objective comparison of auditory profiles using manifold learning and intrinsic measures",
    "paper_title_zh": "使用流形学习和内在度量进行听觉特征的客观比较",
    "paper_id": "2601.03827",
    "paper_abstract": "Assigning individuals with hearing impairment to auditory profiles can support a better understanding of the causes and consequences of hearing loss and facilitate profile-based hearing-aid fitting. However, the factors influencing auditory profile generation remain insufficiently understood, and existing profiling frameworks have rarely been compared systematically. This study therefore investigated the impact of two key factors - the clustering method and the number of profiles - on auditory profile generation. In addition, eight established auditory profiling frameworks were systematically reviewed and compared using intrinsic statistical measures and manifold learning techniques. Frameworks were evaluated with respect to internal consistency (i.e., grouping similar individuals) and cluster separation (i.e., clear differentiation between groups). To ensure comparability, all analyses were conducted on a common open-access dataset, the extended Oldenburg Hearing Health Record (OHHR), comprising 1,127 participants (mean age = 67.2 years, SD = 12.0). Results showed that both the clustering method and the chosen number of profiles substantially influenced the resulting auditory profiles. Among purely audiogram-based approaches, the Bisgaard auditory profiles demonstrated the strongest clustering performance, whereas audiometric phenotypes performed worst. Among frameworks incorporating supra-threshold information in addition to the audiogram, the Hearing4All auditory profiles were advantageous, combining a near-optimal number of profile classes (N = 13) with high clustering quality, as indicated by a low Davies-Bouldin index. In conclusion, manifold learning and intrinsic measures enable systematic comparison of auditory profiling frameworks and identify the Hearing4All auditory profile as a promising approach for future research.",
    "paper_abstract_zh": "将听力障碍个体分配到不同的听觉特征有助于更好地理解听力损失的成因和后果，并促进基于特征的助听器适配。然而，影响听觉特征生成的因素尚未得到充分理解，现有的特征框架也鲜有系统性的比较。因此，本研究探讨了两种关键因素——聚类方法和特征数量——对听觉特征生成的影响。此外，研究还使用内在统计度量和流形学习技术对八个既定的听觉特征框架进行了系统性回顾和比较。框架评估基于内部一致性（即相似个体的分组）和聚类分离度（即组间清晰区分）。为确保可比性，所有分析均在同一公开数据集（扩展版奥尔登堡听力健康记录，OHHR）上进行，该数据集包含1,127名参与者（平均年龄=67.2岁，标准差=12.0）。结果表明，聚类方法和所选特征数量均显著影响最终的听觉特征。在纯听力图方法中，比斯加德听觉特征表现出最强的聚类性能，而听力表型表现最差。在结合听力图和阈上信息的框架中，Hearing4All听觉特征具有优势，其特征类别数量（N=13）接近最优，且聚类质量高（戴维斯-鲍尔丁指数低）。总之，流形学习和内在度量能够系统性地比较听觉特征框架，并将Hearing4All听觉特征确定为未来研究的有前景的方法。",
    "subjects": [
      "Medical Physics (physics.med-ph)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2026-01-08",
    "paper_authors": "Chen Xu, Birger Kollmeier, Lena Schell-Majoor",
    "topic": [
      "Audio Classification",
      "Other"
    ],
    "category": [
      "Other"
    ]
  },
  {
    "paper_title": "Domain Adaptation of the Pyannote Diarization Pipeline for Conversational Indonesian Audio",
    "paper_title_zh": "针对对话印尼音频的Pyannote说话人分离流程的领域自适应",
    "paper_id": "2601.03684",
    "paper_abstract": "This study presents a domain adaptation approach for speaker diarization targeting conversational Indonesian audio. We address the challenge of adapting an English-centric diarization pipeline to a low-resource language by employing synthetic data generation using neural Text-to-Speech technology. Experiments were conducted with varying training configurations, a small dataset (171 samples) and a large dataset containing 25 hours of synthetic speech. Results demonstrate that the baseline \\texttt{pyannote/segmentation-3.0} model, trained on the AMI Corpus, achieves a Diarization Error Rate (DER) of 53.47\\% when applied zero-shot to Indonesian. Domain adaptation significantly improves performance, with the small dataset models reducing DER to 34.31\\% (1 epoch) and 34.81\\% (2 epochs). The model trained on the 25-hour dataset achieves the best performance with a DER of 29.24\\%, representing a 13.68\\% absolute improvement over the baseline while maintaining 99.06\\% Recall and 87.14\\% F1-Score.",
    "paper_abstract_zh": "本研究提出了一种针对对话印尼音频的说话人分离领域自适应方法。我们通过使用神经文本到语音技术生成合成数据，解决了将以英语为中心的分离流程适应到低资源语言的挑战。实验采用了不同的训练配置，包括一个小型数据集（171个样本）和一个包含25小时合成语音的大型数据集。结果表明，在AMI语料库上训练的基线模型pyannote/segmentation-3.0在零样本应用于印尼语时，说话人分离错误率（DER）为53.47%。领域自适应显著提高了性能，小型数据集模型将DER降低到34.31%（1个周期）和34.81%（2个周期）。在25小时数据集上训练的模型实现了最佳性能，DER为29.24%，比基线模型绝对提高了13.68%，同时保持了99.06%的召回率和87.14%的F1分数。",
    "subjects": [
      "Sound (cs.SD)"
    ],
    "update_time": "2026-01-08",
    "paper_authors": "Muhammad Daffa'i Rafi Prasetyo, Ramadhan Andika Putra, Zaidan Naufal Ilmi, Kurniawati Azizah",
    "topic": [
      "Speech Recognition",
      "Speech Synthesis"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "IndexTTS 2.5 Technical Report",
    "paper_title_zh": "IndexTTS 2.5 技术报告",
    "paper_id": "2601.03888",
    "paper_abstract": "In prior work, we introduced IndexTTS 2, a zero-shot neural text-to-speech foundation model comprising two core components: a transformer-based Text-to-Semantic (T2S) module and a non-autoregressive Semantic-to-Mel (S2M) module, which together enable faithful emotion replication and establish the first autoregressive duration-controllable generative paradigm. Building upon this, we present IndexTTS 2.5, which significantly enhances multilingual coverage, inference speed, and overall synthesis quality through four key improvements: 1) Semantic Codec Compression: we reduce the semantic codec frame rate from 50 Hz to 25 Hz, halving sequence length and substantially lowering both training and inference costs; 2) Architectural Upgrade: we replace the U-DiT-based backbone of the S2M module with a more efficient Zipformer-based modeling architecture, achieving notable parameter reduction and faster mel-spectrogram generation; 3) Multilingual Extension: We propose three explicit cross-lingual modeling strategies, boundary-aware alignment, token-level concatenation, and instruction-guided generation, establishing practical design principles for zero-shot multilingual emotional TTS that supports Chinese, English, Japanese, and Spanish, and enables robust emotion transfer even without target-language emotional training data; 4) Reinforcement Learning Optimization: we apply GRPO in post-training of the T2S module, improving pronunciation accuracy and natrualness. Experiments show that IndexTTS 2.5 not only supports broader language coverage but also replicates emotional prosody in unseen languages under the same zero-shot setting. IndexTTS 2.5 achieves a 2.28 times improvement in RTF while maintaining comparable WER and speaker similarity to IndexTTS 2.",
    "paper_abstract_zh": "在先前的工作中，我们介绍了IndexTTS 2，这是一个零样本神经文本转语音基础模型，包含两个核心组件：基于transformer的文本到语义（T2S）模块和非自回归的语义到梅尔谱（S2M）模块，它们共同实现了真实的情感复制，并建立了第一个自回归时长可控的生成范式。基于此，我们提出了IndexTTS 2.5，通过四个关键改进显著提高了多语言覆盖范围、推理速度和整体合成质量：1）语义编解码压缩：我们将语义编解码的帧率从50 Hz降低到25 Hz，使序列长度减半，并大幅降低了训练和推理成本；2）架构升级：我们用更高效的基于Zipformer的建模架构替代了S2M模块的基于U-DiT的主干网络，实现了显著的参数减少和更快的梅尔谱生成；3）多语言扩展：我们提出了三种明确的跨语言建模策略，边界感知对齐、标记级连接和指令引导生成，为支持中文、英文、日文和西班牙文的零样本多语言情感TTS建立了实用的设计原则，并实现了即使在没有目标语言情感训练数据的情况下也能进行稳健的情感迁移；4）强化学习优化：我们在T2S模块的后训练中应用GRPO，提高了发音准确度和自然度。实验表明，IndexTTS 2.5不仅支持更广泛的语言覆盖，而且在相同的零样本设置下，能够在未见过的语言中复制情感韵律。IndexTTS 2.5实现了2.28倍的RTF提升，同时保持了与IndexTTS 2相当的WER和说话人相似度。",
    "subjects": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)"
    ],
    "update_time": "2026-01-08",
    "paper_authors": "Yunpei Li, Xun Zhou, Jinchao Wang, Lu Wang, Yong Wu, Siyi Zhou, Yiquan Zhou, Jingchen Shu",
    "topic": [
      "Speech Synthesis",
      "Audio Codec"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Lightweight and perceptually-guided voice conversion for electro-laryngeal speech",
    "paper_title_zh": "轻量级且感知引导的喉发音语音转换",
    "paper_id": "2601.03892",
    "paper_abstract": "Electro-laryngeal (EL) speech is characterized by constant pitch, limited prosody, and mechanical noise, reducing naturalness and intelligibility. We propose a lightweight adaptation of the state-of-the-art StreamVC framework to this setting by removing pitch and energy modules and combining self-supervised pretraining with supervised fine-tuning on parallel EL and healthy (HE) speech data, guided by perceptual and intelligibility losses. Objective and subjective evaluations across different loss configurations confirm their influence: the best model variant, based on WavLM features and human-feedback predictions (+WavLM+HF), drastically reduces character error rate (CER) of EL inputs, raises naturalness mean opinion score (nMOS) from 1.1 to 3.3, and consistently narrows the gap to HE ground-truth speech in all evaluated metrics. These findings demonstrate the feasibility of adapting lightweight voice conversion architectures to EL voice rehabilitation while also identifying prosody generation and intelligibility improvements as the main remaining bottlenecks.",
    "paper_abstract_zh": "喉发音(EL)语音的特点是音调恒定、韵律有限和机械噪声，这降低了自然度和可懂度。我们通过移除音调和能量模块，并结合自监督预训练和监督微调，对最先进的StreamVC框架进行了轻量级适配，使用并行的EL和健康(HE)语音数据，并在感知和可懂度损失的指导下进行。不同损失配置下的客观和主观评估证实了它们的影响：基于WavLM特征和人类反馈预测的最佳模型变体(+WavLM+HF)显著降低了EL输入的字符错误率(CER)，将自然度平均意见评分(nMOS)从1.1提高到3.3，并在所有评估指标上一致地缩小了与HE真实语音的差距。这些研究结果证明了将轻量级语音转换架构适配到EL语音康复中的可行性，同时也确定了韵律生成和可懂度改进为主要剩余瓶颈。",
    "subjects": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)"
    ],
    "update_time": "2026-01-08",
    "paper_authors": "Benedikt Mayrhofer, Franz Pernkopf, Philipp Aichinger, Martin Hagmüller",
    "topic": [
      "Speech Enhancement",
      "Speech Synthesis"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Muse: Towards Reproducible Long-Form Song Generation with Fine-Grained Style Control",
    "paper_title_zh": "Muse：面向可重现长篇歌曲生成与细粒度风格控制的系统",
    "paper_id": "2601.03973",
    "paper_abstract": "Recent commercial systems such as Suno demonstrate strong capabilities in long-form song generation, while academic research remains largely non-reproducible due to the lack of publicly available training data, hindering fair comparison and progress. To this end, we release a fully open-source system for long-form song generation with fine-grained style conditioning, including a licensed synthetic dataset, training and evaluation pipelines, and Muse, an easy-to-deploy song generation model. The dataset consists of 116k fully licensed synthetic songs with automatically generated lyrics and style descriptions paired with audio synthesized by SunoV5. We train Muse via single-stage supervised finetuning of a Qwen-based language model extended with discrete audio tokens using MuCodec, without task-specific losses, auxiliary objectives, or additional architectural components. Our evaluations find that although Muse is trained with a modest data scale and model size, it achieves competitive performance on phoneme error rate, text--music style similarity, and audio aesthetic quality, while enabling controllable segment-level generation across different musical structures. All data, model weights, and training and evaluation pipelines will be publicly released, paving the way for continued progress in controllable long-form song generation research. The project repository is available at this https URL.",
    "paper_abstract_zh": "最近的商业系统如Suno展示了长篇歌曲生成的强大能力，但学术研究由于缺乏公开可用的训练数据而 largely 不可重现，阻碍了公平比较和进展。为此，我们发布了一个完全开源的长篇歌曲生成系统，具有细粒度风格控制条件，包括一个许可的合成数据集、训练和评估流程，以及Muse，一个易于部署的歌曲生成模型。该数据集包含116k个完全许可的合成歌曲，配有自动生成的歌词和风格描述，以及由SunoV5合成的音频对。我们通过基于Qwen的语言模型的单阶段监督微调来训练Muse，该模型使用MuCodec扩展了离散音频标记，无需特定任务的损失、辅助目标或额外的架构组件。我们的评估发现，尽管Muse使用适中的数据规模和模型大小进行训练，但在音素错误率、文本-音乐风格相似性和音频美学质量方面取得了具有竞争力的性能，同时能够在不同的音乐结构上实现可控的段落级生成。所有数据、模型权重以及训练和评估流程都将公开发布，为可控长篇歌曲生成研究的持续进展铺平道路。项目仓库可通过此https URL获取。",
    "subjects": [
      "Sound (cs.SD)",
      "Computation and Language (cs.CL)"
    ],
    "update_time": "2026-01-08",
    "paper_authors": "Changhao Jiang, Jiahao Chen, Zhenghao Xiang, Zhixiong Yang, Hanchen Wang, Jiabao Zhuang, Xinmeng Che, Jiajun Sun, Hui Li, Yifei Cao, Shihan Dou, Ming Zhang, Junjie Ye, Tao Ji, Tao Gui, Qi Zhang, Xuanjing Huang",
    "topic": [
      "Music Generation",
      "Audio Representation Learning"
    ],
    "category": [
      "Music"
    ]
  },
  {
    "paper_title": "Listen to Rhythm, Choose Movements: Autoregressive Multimodal Dance Generation via Diffusion and Mamba with Decoupled Dance Dataset",
    "paper_title_zh": "聆听节奏，选择动作：基于解耦舞蹈数据集的扩散与Mamba多模态舞蹈自回归生成",
    "paper_id": "2601.03323",
    "paper_abstract": "Advances in generative models and sequence learning have greatly promoted research in dance motion generation, yet current methods still suffer from coarse semantic control and poor coherence in long sequences. In this work, we present Listen to Rhythm, Choose Movements (LRCM), a multimodal-guided diffusion framework supporting both diverse input modalities and autoregressive dance motion generation. We explore a feature decoupling paradigm for dance datasets and generalize it to the Motorica Dance dataset, separating motion capture data, audio rhythm, and professionally annotated global and local text descriptions. Our diffusion architecture integrates an audio-latent Conformer and a text-latent Cross-Conformer, and incorporates a Motion Temporal Mamba Module (MTMM) to enable smooth, long-duration autoregressive synthesis. Experimental results indicate that LRCM delivers strong performance in both functional capability and quantitative metrics, demonstrating notable potential in multimodal input scenarios and extended sequence generation. We will release the full codebase, dataset, and pretrained models publicly upon acceptance.",
    "paper_abstract_zh": "生成模型和序列学习的进步极大地促进了舞蹈动作生成的研究，然而当前方法仍存在语义控制粗糙和长序列连贯性差的问题。在这项工作中，我们提出了'聆听节奏，选择动作'(LRCM)，这是一个多模态引导的扩散框架，支持多样化的输入模态和舞蹈动作的自回归生成。我们探索了一种舞蹈数据集的特征解耦范式，并将其推广到Motorica舞蹈数据集，将动作捕捉数据、音频节奏以及专业标注的全局和局部文本描述进行分离。我们的扩散架构集成了音频潜在Conformer和文本潜在Cross-Conformer，并引入了动作时序Mamba模块(MTMM)，以实现平滑、长持续时间的自回归合成。实验结果表明，LRCM在功能能力和定量指标方面均表现出色，展示了在多模态输入场景和扩展序列生成方面的显著潜力。我们将在论文被接收后公开完整的代码库、数据集和预训练模型。",
    "subjects": [
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)"
    ],
    "update_time": "2026-01-08",
    "paper_authors": "Oran Duan, Yinghua Shen, Yingzhu Lv, Luyang Jie, Yaxin Liu, Qiong Wu",
    "topic": [
      "Audio Representation Learning",
      "Music Generation",
      "Video Generation"
    ],
    "category": [
      "Image&Video"
    ]
  },
  {
    "paper_title": "ASVspoof 5: Evaluation of Spoofing, Deepfake, and Adversarial Attack Detection Using Crowdsourced Speech",
    "paper_title_zh": "ASVspoof 5：使用众包语音评估欺骗、深度伪造和对抗攻击检测",
    "paper_id": "2601.03944",
    "paper_abstract": "ASVspoof 5 is the fifth edition in a series of challenges which promote the study of speech spoofing and deepfake detection solutions. A significant change from previous challenge editions is a new crowdsourced database collected from a substantially greater number of speakers under diverse recording conditions, and a mix of cutting-edge and legacy generative speech technology. With the new database described elsewhere, we provide in this paper an overview of the ASVspoof 5 challenge results for the submissions of 53 participating teams. While many solutions perform well, performance degrades under adversarial attacks and the application of neural encoding/compression schemes. Together with a review of post-challenge results, we also report a study of calibration in addition to other principal challenges and outline a road-map for the future of ASVspoof.",
    "paper_abstract_zh": "ASVspoof 5是系列挑战赛的第五版，旨在促进语音欺骗和深度伪造检测解决方案的研究。与以往挑战赛版本相比，一个显著变化是新的众包数据库，该数据库从更多样化的说话者群体中收集，涵盖多样化的录音条件，并融合了前沿和传统的生成语音技术。除了描述新数据库外，本文还概述了ASVspoof 5挑战赛的结果，涉及53支参赛团队的提交。尽管许多解决方案表现良好，但在对抗攻击和神经编码/压缩方案应用下，性能有所下降。结合挑战赛后的结果回顾，我们还报告了一项校准研究，以及其他主要挑战，并概述了ASVspoof的未来路线图。",
    "subjects": [
      "Signal Processing (eess.SP)",
      "Sound (cs.SD)"
    ],
    "update_time": "2026-01-08",
    "paper_authors": "Xin Wang, Héctor Delgado, Nicholas Evans, Xuechen Liu, Tomi Kinnunen, Hemlata Tak, Kong Aik Lee, Ivan Kukanov, Md Sahidullah, Massimiliano Todisco, Junichi Yamagishi",
    "topic": [
      "Audio Classification",
      "Speech Recognition"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Klear: Unified Multi-Task Audio-Video Joint Generation",
    "paper_title_zh": "Klear: 统一的多任务音视频联合生成",
    "paper_id": "2601.04151",
    "paper_abstract": "Audio-video joint generation has progressed rapidly, yet substantial challenges still remain. Non-commercial approaches still suffer audio-visual asynchrony, poor lip-speech alignment, and unimodal degradation, which can be stemmed from weak audio-visual correspondence modeling, limited generalization, and scarce high-quality dense-caption data. To address these issues, we introduce Klear and delve into three axes--model architecture, training strategy, and data curation. Architecturally, we adopt a single-tower design with unified DiT blocks and an Omni-Full Attention mechanism, achieving tight audio-visual alignment and strong scalability. Training-wise, we adopt a progressive multitask regime--random modality masking to joint optimization across tasks, and a multistage curriculum, yielding robust representations, strengthening A-V aligned world knowledge, and preventing unimodal collapse. For datasets, we present the first large-scale audio-video dataset with dense captions, and introduce a novel automated data-construction pipeline which annotates and filters millions of diverse, high-quality, strictly aligned audio-video-caption triplets. Building on this, Klear scales to large datasets, delivering high-fidelity, semantically and temporally aligned, instruction-following generation in both joint and unimodal settings while generalizing robustly to out-of-distribution scenarios. Across tasks, it substantially outperforms prior methods by a large margin and achieves performance comparable to Veo 3, offering a unified, scalable path toward next-generation audio-video synthesis.",
    "paper_abstract_zh": "音视频联合生成已取得快速发展，但仍存在重大挑战。非商业方法仍存在音视频异步、唇语-语音对齐不良和单模态退化问题，这些问题源于弱音视频对应建模、泛化能力有限以及高质量密集标注数据稀缺。为解决这些问题，我们引入了Klear，并从模型架构、训练策略和数据构建三个维度进行了深入研究。在架构上，我们采用单塔设计，使用统一的DiT块和Omni-Full Attention机制，实现了紧密的音视频对齐和强大的可扩展性。在训练方面，我们采用渐进式多任务策略——随机模态掩码以实现跨任务联合优化，以及多阶段课程学习，从而获得鲁棒表示，增强A-V对齐的世界知识，并防止单模态崩溃。对于数据集，我们首次推出了带有密集标注的大规模音视频数据集，并引入了一种新颖的自动化数据构建流程，该流程能够标注和过滤数百万种多样化、高质量、严格对齐的音视频-文本三元组。基于此，Klear能够扩展到大型数据集，在联合和单模态设置下生成高保真、语义和时间对齐、遵循指令的内容，并能鲁棒地泛化到分布外场景。在各项任务中，Klear显著优于先前方法，且性能与Veo 3相当，为下一代音视频合成提供了一条统一、可扩展的路径。",
    "subjects": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Multimedia (cs.MM)",
      "Sound (cs.SD)"
    ],
    "update_time": "2026-01-08",
    "paper_authors": "Jun Wang, Chunyu Qiang, Yuxin Guo, Yiran Wang, Xijuan Zeng, Chen Zhang, Pengfei Wan",
    "topic": [
      "Video Generation",
      "Image Generation",
      "Speech Synthesis",
      "Music Generation"
    ],
    "category": [
      "Image&Video"
    ]
  }
]