[
  {
    "paper_title": "WaveSP-Net: Learnable Wavelet-Domain Sparse Prompt Tuning for Speech Deepfake Detection",
    "paper_title_zh": "WaveSP-Net：一种基于可学习小波域稀疏提示调谐的语音深度伪造检测方法",
    "paper_id": "2510.05305",
    "paper_abstract": "Modern front-end design for speech deepfake detection relies on full fine-tuning of large pre-trained models like XLSR. However, this approach is not parameter-efficient and may lead to suboptimal generalization to realistic, in-the-wild data types. To address these limitations, we introduce a new family of parameter-efficient front-ends that fuse prompt-tuning with classical signal processing transforms. These include FourierPT-XLSR, which uses the Fourier Transform, and two variants based on the Wavelet Transform: WSPT-XLSR and Partial-WSPT-XLSR. We further propose WaveSP-Net, a novel architecture combining a Partial-WSPT-XLSR front-end and a bidirectional Mamba-based back-end. This design injects multi-resolution features into the prompt embeddings, which enhances the localization of subtle synthetic artifacts without altering the frozen XLSR parameters. Experimental results demonstrate that WaveSP-Net outperforms several state-of-the-art models on two new and challenging benchmarks, Deepfake-Eval-2024 and SpoofCeleb, with low trainable parameters and notable performance gains. The code and models are available at this https URL.",
    "paper_abstract_zh": "现代语音深度伪造检测的前端设计通常依赖于对大型预训练模型（如XLSR）进行全面微调。然而，这种方法参数效率低下，且可能导致对真实、野外数据类型的泛化能力不佳。为了解决这些问题，我们提出了一族参数高效的前端方案，将提示调优技术与经典信号处理变换相结合。具体包括：基于傅里叶变换的FourierPT-XLSR，以及两种基于小波变换的变体——WSPT-XLSR和Partial-WSPT-XLSR。我们进一步提出了WaveSP-Net，这是一种新颖的架构，结合了Partial-WSPT-XLSR前端和基于双向Mamba的后端。该设计将多分辨率特征注入到提示嵌入中，从而在不改变冻结的XLSR参数的情况下，增强了对细微合成伪影的定位能力。实验结果表明，WaveSP-Net在两个新且具有挑战性的基准测试（Deepfake-Eval-2024和SpookCeleb）上均优于多种先进模型，同时具有较低的可训练参数和显著的性能提升。相关代码和模型已在如下网址提供：https://example.com。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)",
      "Computation and Language (cs.CL)",
      "Signal Processing (eess.SP)"
    ],
    "update_time": "2025-10-08",
    "paper_authors": "Xi Xuan, Xuechen Liu, Wenxin Zhang, Yi-Cheng Lin, Xiaojian Lin, Tomi Kinnunen",
    "topic": [
      "Speech Synthesis",
      "Audio Classification"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "AQA-TTRL: Self-Adaptation in Audio Question Answering with Test-Time Reinforcement Learning",
    "paper_title_zh": "AQA-TTRL：基于测试时强化学习的自适应音频问答系统",
    "paper_id": "2510.05478",
    "paper_abstract": "Large Audio Language Models (LALMs) demonstrate impressive general audio understanding, but once deployed, they are static and fail to improve with new real-world audio data. As traditional supervised fine-tuning is costly, we introduce a novel framework for test-time audio understanding, AQA-TTRL, where an LALM evolves on-the-fly using only unlabeled test data. It first generates pseudo-labels from the prediction via majority voting, then optimizes the model via reinforcement learning. To handle the inherent noise in these self-generated labels, we introduce a confidence-based weighting method to adjust training signals. Furthermore, a multiple-attempt sampling operation mitigates advantage collapse and stabilizes training. On the MMAU (test-mini/test), MMAR, and MMSU benchmarks, AQA-TTRL achieves significant average improvements of 4.42% for the Qwen2.5-Omni 7B model and 11.04% for the 3B model. Notably, the adapted 3B model consistently outperforms the direct inference of the unadapted 7B model, highlighting the effectiveness of previously unexplored test-time adaptations in audio understanding.",
    "paper_abstract_zh": "大型音频语言模型（LALMs）在通用音频理解方面表现出色，但一旦部署，它们是静态的，无法随新的真实音频数据改进。由于传统的监督微调成本高昂，我们引入了一个新颖的测试时音频理解框架AQA-TTRL，其中LALM仅使用未标记的测试数据进行实时演化。它首先通过多数投票从预测中生成伪标签，然后通过强化学习优化模型。为处理这些自生成标签中固有的噪声，我们引入了一种基于置信度的加权方法来调整训练信号。此外，一种多尝试采样操作缓解了优势崩溃问题并稳定了训练。在MMAU（测试迷你版/测试版）、MMAR和MMSU基准测试中，AQA-TTRL实现了显著的平均提升：Qwen2.5-Omni 7B模型提升了4.42%，3B模型提升了11.04%。值得注意的是，优化后的3B模型持续优于未优化的7B模型的直接推理结果，突显了先前未探索的测试时自适应在音频理解中的有效性。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2025-10-08",
    "paper_authors": "Haoyu Zhang, Jiaxian Guo, Yusuke Iwasawa, Yutaka Matsuo",
    "topic": [
      "Audio Representation Learning",
      "Speech Recognition",
      "Audio Classification"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Teaching Machines to Speak Using Articulatory Control",
    "paper_title_zh": "通过发音控制让机器说话",
    "paper_id": "2510.05619",
    "paper_abstract": "Current speech production systems predominantly rely on large transformer models that operate as black boxes, providing little interpretability or grounding in the physical mechanisms of human speech. We address this limitation by proposing a new framework: speech generation through explicit articulatory control. This reframes speech as a motor control task similar to robotic manipulation. Our approach uses reinforcement learning to train a policy that directly controls the movements of vocal tract articulators, such as the tongue, lips, and jaw, to produce syllable-level speech. Specifically, we employ the Proximal Policy Optimization algorithm to learn optimal articulatory movements based on acoustic feedback provided by our audio perceiver, Sylber. The resulting articulatory trajectories are decoded into audio using SPARC, a pre-trained articulatory-to-speech decoder. We train this framework on six target syllables, and it demonstrates successful convergence, with similarity scores between the policy-generated audio and the target syllables exceeding 0.85. Accurate human transcription of the audio for syllables such as \"please\", \"loot\", and \"cat\" demonstrates the intelligibility of this framework.",
    "paper_abstract_zh": "当前的语言生成系统主要依赖于大型Transformer模型，这些模型作为黑箱运行，几乎无法提供可解释性或对人类语音物理机制的明确关联。为解决这一局限，我们提出了一个新框架：通过显式的发音控制来生成语音。这一方法将语音重新定义为一种类似于机器人操作的运动控制任务。我们的方法使用强化学习来训练一个策略，该策略能直接控制声道发音器官（如舌头、嘴唇和下颌）的运动，以生成音节级别的语音。具体而言，我们采用近端策略优化（PPO）算法，根据我们的音频感知器Sylber提供的听觉反馈，学习最优的发音动作轨迹。生成的发音轨迹通过预训练的发音-语音解码器SPARC转换为音频。我们在六个目标音节上训练这一框架，并取得了成功收敛——生成音频与目标音节之间的相似度得分超过0.85。对人类听写的准确转录（例如对“please”、“loot”和“cat”等音节的识别）证明了该框架的可理解性。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2025-10-08",
    "paper_authors": "Akshay Anand, Chenxu Guo, Cheol Jun Cho, Jiachen Lian, Gopala Anumanchipalli",
    "topic": [
      "Speech Synthesis",
      "Audio Representation Learning"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Investigation of perception inconsistency in speaker embedding for asynchronous voice anonymization",
    "paper_title_zh": "针对异步语音匿名化中说话人嵌入的感知不一致性研究",
    "paper_id": "2510.05718",
    "paper_abstract": "Given the speech generation framework that represents the speaker attribute with an embedding vector, asynchronous voice anonymization can be achieved by modifying the speaker embedding derived from the original speech. However, the inconsistency between machine and human perceptions of the speaker attribute within the speaker embedding remains unexplored, limiting its performance in asynchronous voice anonymization. To this end, this study investigates this inconsistency via modifications to speaker embedding in the speech generation process. Experiments conducted on the FACodec and Diff-HierVC speech generation models discover a subspace whose removal alters machine perception while preserving its human perception of the speaker attribute in the generated speech. With these findings, an asynchronous voice anonymization is developed, achieving 100% human perception preservation rate while obscuring the machine perception. Audio samples can be found in this https URL.",
    "paper_abstract_zh": "考虑到使用嵌入向量表示说话人属性的语音生成框架，通过修改从原始语音中提取的说话人嵌入可以实现异步语音匿名化。然而，机器与人类对说话人属性在嵌入向量中的感知不一致性尚未得到充分探索，这限制了其在异步语音匿名化中的性能。为此，本研究通过修改说话人嵌入在语音生成过程中的表现来调查这种不一致性。在FACodec和Diff-HierVC语音生成模型上进行的实验发现，存在一个子空间，移除该子空间会改变机器的感知，同时保留人类对说话人属性的感知。基于这些发现，开发了一种异步语音匿名化方法，在实现100%人类感知保留率的同时混淆了机器感知。音频样本可在提供的https URL中找到。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2025-10-08",
    "paper_authors": "Rui Wang, Liping Chen, Kong Aik Lee, Zhengpeng Zha, Zhenhua Ling",
    "topic": [
      "Audio Representation Learning",
      "Speech Synthesis"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Neural Forward Filtering for Speaker-Image Separation",
    "paper_title_zh": "用于人像语音分离的神经正向滤波",
    "paper_id": "2510.05757",
    "paper_abstract": "We address monaural multi-speaker-image separation in reverberant conditions, aiming at separating mixed speakers but preserving the reverberation of each speaker. A straightforward approach for this task is to directly train end-to-end DNN systems to predict the reverberant speech of each speaker based on the input mixture. Although effective, this approach does not explicitly exploit the physical constraint that reverberant speech can be reproduced by convolving the direct-path signal with a linear filter. To address this, we propose CxNet, a two-DNN system with a neural forward filtering module in between. The first DNN is trained to jointly predict the direct-path signal and reverberant speech. Based on the direct-path estimate, the neural forward filtering module estimates the linear filter, and the estimated filter is then convolved with the direct-path estimate to obtain another estimate of reverberant speech, which is utilized as a discriminative feature to help the second DNN better estimate the reverberant speech. By explicitly modeling the linear filter, CxNet could leverage the physical constraint between the direct-path signal and reverberant speech to capture crucial information about reverberation tails. Evaluation results on the SMS-WSJ dataset show the effectiveness of the proposed algorithms.",
    "paper_abstract_zh": "我们致力于解决混响条件下的单声道多说话人-图像分离问题，目标是在分离混合说话人的同时保留各自的混响效果。对此任务的一种直接方法是直接训练端到端的深度神经网络系统，基于输入混合预测每个说话人的混响语音。尽管有效，但该方法并未明确利用混响语音可通过直接路径信号与线性滤波器的卷积来再现这一物理约束。为解决此问题，我们提出CxNet，这是一个双DNN系统，中间包含一个神经正向滤波模块。第一个DNN被训练来联合预测直接路径信号和混响语音。基于直接路径估计，神经正向滤波模块估计线性滤波器，并将估计的滤波器与直接路径估计进行卷积以获得混响语音的另一估计，该估计被用作判别特征以帮助第二个DNN更好地估计混响语音。通过显式建模线性滤波器，CxNet能够利用直接路径信号和混响语音之间的物理约束来捕捉混响尾音的关键信息。在SMS-WSJ数据集上的评估结果显示了所提算法的有效性。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2025-10-08",
    "paper_authors": "Jingqi Sun, Shulin He, Ruizhe Pang, Zhong-Qiu Wang",
    "topic": [
      "Audio Representation Learning",
      "Speech Enhancement"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Revisiting MFCCs: Evidence for Spectral-Prosodic Coupling",
    "paper_title_zh": "重新审视梅尔倒谱系数：关于频谱-韵律耦合的证据",
    "paper_id": "2510.05922",
    "paper_abstract": "Mel-frequency cepstral coefficients (MFCCs) are an important feature in speech processing. A deeper understanding of their properties can contribute to the work that is being done with both classical and deep learning models. This study challenges the long-held assumption that MFCCs lack relevant temporal information by investigating their relationship with speech prosody. Using a null hypothesis significance testing framework, a systematic assessment is made about the statistical independence between MFCCs and the three prosodic features: energy, fundamental frequency (F0), and voicing. The results demonstrate that it is statistically implausible that the MFCCs are independent of any of these three prosodic features. This finding suggests that MFCCs inherently carry valuable prosodic information, which can inform the design of future models in speech analysis and recognition.",
    "paper_abstract_zh": "梅尔倒谱系数（MFCCs）是语音处理中的一项重要特征。对其特性的更深入理解可以促进基于经典模型和深度学习模型的研究工作。本研究挑战了长期以来认为梅尔倒谱系数缺乏时序信息的假设，通过研究其与语音韵律的关系。使用零假设显著性检验框架，系统性地评估了梅尔倒谱系数与三种韵律特征——能量（energy）、基频（F0）和浊音性（voicing）——之间的统计独立性。结果表明，梅尔倒谱系数与这三种韵律特征中的任何一种都统计上显著相关，这一发现表明梅尔倒谱系数本身携带了有价值的韵律信息，这可以为未来语音分析和识别模型的设计提供参考。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2025-10-08",
    "paper_authors": "Vitor Magno de O. S. Bezerra, Gabriel F. A. Bastos, Jugurta Montalvão",
    "topic": [
      "Audio Representation Learning",
      "Speech Recognition"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Revisiting Modeling and Evaluation Approaches in Speech Emotion Recognition: Considering Subjectivity of Annotators and Ambiguity of Emotions",
    "paper_title_zh": "重温语音情感识别中的建模与评估方法：考虑标注者的主观性与情感的模糊性",
    "paper_id": "2510.05934",
    "paper_abstract": "Over the past two decades, speech emotion recognition (SER) has received growing attention. To train SER systems, researchers collect emotional speech databases annotated by crowdsourced or in-house raters who select emotions from predefined categories. However, disagreements among raters are common. Conventional methods treat these disagreements as noise, aggregating labels into a single consensus target. While this simplifies SER as a single-label task, it ignores the inherent subjectivity of human emotion perception. This dissertation challenges such assumptions and asks: (1) Should minority emotional ratings be discarded? (2) Should SER systems learn from only a few individuals' perceptions? (3) Should SER systems predict only one emotion per sample?\nPsychological studies show that emotion perception is subjective and ambiguous, with overlapping emotional boundaries. We propose new modeling and evaluation perspectives: (1) Retain all emotional ratings and represent them with soft-label distributions. Models trained on individual annotator ratings and jointly optimized with standard SER systems improve performance on consensus-labeled tests. (2) Redefine SER evaluation by including all emotional data and allowing co-occurring emotions (e.g., sad and angry). We propose an ``all-inclusive rule'' that aggregates all ratings to maximize diversity in label representation. Experiments on four English emotion databases show superior performance over majority and plurality labeling. (3) Construct a penalization matrix to discourage unlikely emotion combinations during training. Integrating it into loss functions further improves performance. Overall, embracing minority ratings, multiple annotators, and multi-emotion predictions yields more robust and human-aligned SER systems.",
    "paper_abstract_zh": "在过去的二十年里，语音情感识别（SER）受到了越来越多的关注。为了训练SER系统，研究人员收集了由众包或内部评分员标注的情感语音数据库，这些标注者从预定义的类别中选择情感标签。然而，标注者之间的意见分歧很常见。传统方法将这些分歧视为噪声，并将标签聚合成单一共识目标。虽然这将SER简化为单标签任务，但它忽略了人类情感感知固有的主观性。本论文挑战了这些假设，并提出以下问题：（1）是否应该丢弃少数派的情感评分？（2）SER系统是否应该仅基于少数个体的感知进行学习？（3）SER系统是否应该为每个样本仅预测一种情感？心理学研究表明，情感感知是主观且模糊的，具有重叠的情感边界。我们提出了新的建模和评估视角：（1）保留所有情感评分，并用软标签分布表示它们。基于个体标注者评分训练的模型，并与标准SER系统联合优化，在共识标签测试集上表现出更好的性能。（2）重新定义SER评估，纳入所有情感数据并允许多种情感共现（例如，悲伤和愤怒）。我们提出了一种“全包含规则”，通过聚合所有评分来最大化标签表示的多样性。在四个英语情感数据库上的实验显示，其性能优于多数投票和相对多数标签方法。（3）构建一个惩罚矩阵，以在训练中抑制不太可能的情感组合。将其整合到损失函数中可进一步提升性能。总体而言，拥抱少数派评分、多标注者数据和多情感预测能够产生更鲁棒且更符合人类感知的SER系统。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2025-10-08",
    "paper_authors": "Huang-Cheng Chou, Chi-Chun Lee",
    "topic": [
      "Audio Representation Learning",
      "Audio Classification"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "TokenChain: A Discrete Speech Chain via Semantic Token Modeling",
    "paper_title_zh": "TokenChain：通过语义标记建模实现的离散语音链",
    "paper_id": "2510.06201",
    "paper_abstract": "Machine Speech Chain, simulating the human perception-production loop, proves effective in jointly improving ASR and TTS. We propose TokenChain, a fully discrete speech chain coupling semantic-token ASR with a two-stage TTS: an autoregressive text-to-semantic model co-trained with ASR and a masked-generative semantic-to-acoustic model for synthesis only. End-to-end feedback across the text interface is enabled with straight-through argmax/Gumbel-Softmax and balanced with supervised ASR via dynamic weight averaging. Ablations examine optimal temperature schedules for in- and cross-domain transfer. Evaluation reveals TokenChain surpasses baseline accuracy 2-6 epochs earlier and yields 5-13% lower equal-epoch error with stable T2S on LibriSpeech, and reduces relative ASR WER by 56% and T2S WER by 31% on TED-LIUM with minimal forgetting, showing that chain learning remains effective with token interfaces and models.",
    "paper_abstract_zh": "机器语音链通过模拟人类感知-生产的循环，被证明能有效联合提升自动语音识别（ASR）和文本到语音（TTS）的性能。本文提出TokenChain，一种全离散的语音链，通过语义标记耦合ASR与两阶段TTS：一个与ASR共同训练的自回归文本到语义模型，以及一个仅用于合成的掩码生成式语义到声学模型。通过直通式argmax/Gumbel-Softmax实现跨文本接口的端到端反馈，并通过动态权重平均与监督式ASR进行平衡。消融实验研究了最优的温度调度策略以用于域内和跨域迁移。评估显示，TokenChain在LibriSpeech上以2-6个epoch的优势超越基线准确率，并获得5-13%更低的等错误率，同时保持稳定的文本到语音（T2S）性能；在TED-LIUM上，它将ASR的词错误率（WER）相对降低56%，T2S的WER降低31%，且遗忘程度最小，这表明链式学习在标记接口和模型下仍然有效。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)"
    ],
    "update_time": "2025-10-08",
    "paper_authors": "Mingxuan Wang, Satoshi Nakamura",
    "topic": [
      "Speech Recognition",
      "Speech Synthesis"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Advancing Automated Spatio-Semantic Analysis in Picture Description Using Language Models",
    "paper_title_zh": "利用语言模型推进图画描述中的自动化空间语义分析",
    "paper_id": "2510.05128",
    "paper_abstract": "Current methods for automated assessment of cognitive-linguistic impairment via picture description often neglect the visual narrative path - the sequence and locations of elements a speaker described in the picture. Analyses of spatio-semantic features capture this path using content information units (CIUs), but manual tagging or dictionary-based mapping is labor-intensive. This study proposes a BERT-based pipeline, fine tuned with binary cross-entropy and pairwise ranking loss, for automated CIU extraction and ordering from the Cookie Theft picture description. Evaluated by 5-fold cross-validation, it achieves 93% median precision, 96% median recall in CIU detection, and 24% sequence error rates. The proposed method extracts features that exhibit strong Pearson correlations with ground truth, surpassing the dictionary-based baseline in external validation. These features also perform comparably to those derived from manual annotations in evaluating group differences via ANCOVA. The pipeline is shown to effectively characterize visual narrative paths for cognitive impairment assessment, with the implementation and models open-sourced to public.",
    "paper_abstract_zh": "当前通过图画描述评估认知语言障碍的自动化方法往往忽视了视觉叙事路径——即描述者所描述的元素的顺序和位置。空间语义特征分析通过内容信息单元（CIUs）捕捉这一路径，但手动标注或基于词典的映射方法费时费力。本研究提出了一种基于BERT的流程，通过二元交叉熵和成对排序损失进行微调，用于从'饼干失窃'图画描述中自动化提取和排序CIUs。经5折交叉验证评估，其在CIU检测中取得了93%的中位数精确率、96%的中位数召回率，以及24%的序列错误率。该方法提取的特征与真实标注表现出强烈的皮尔逊相关性，并在外部验证中超越了基于词典的基线方法。这些特征在通过ANCOVA评估群体差异时，与手动注释得出的特征表现相当。该流程被证明能有效表征视觉叙事路径以评估认知障碍，其实现和模型已开源。",
    "subjects": [
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2025-10-08",
    "paper_authors": "Si-Ioi Ng, Pranav S. Ambadi, Kimberly D. Mueller, Julie Liss, Visar Berisha",
    "topic": [
      "Speech Recognition",
      "Other"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Sci-Phi: A Large Language Model Spatial Audio Descriptor",
    "paper_title_zh": "Sci-Phi：一种大语言模型空间音频描述符",
    "paper_id": "2510.05542",
    "paper_abstract": "Acoustic scene perception involves describing the type of sounds, their timing, their direction and distance, as well as their loudness and reverberation. While audio language models excel in sound recognition, single-channel input fundamentally limits spatial understanding. This work presents Sci-Phi, a spatial audio large language model with dual spatial and spectral encoders that estimates a complete parameter set for all sound sources and the surrounding environment. Learning from over 4,000 hours of synthetic first-order Ambisonics recordings including metadata, Sci-Phi enumerates and describes up to four directional sound sources in one pass, alongside non-directional background sounds and room characteristics. We evaluate the model with a permutation-invariant protocol and 15 metrics covering content, location, timing, loudness, and reverberation, and analyze its robustness across source counts, signal-to-noise ratios, reverberation levels, and challenging mixtures of acoustically, spatially, or temporally similar sources. Notably, Sci-Phi generalizes to real room impulse responses with only minor performance degradation. Overall, this work establishes the first audio LLM capable of full spatial-scene description, with strong potential for real-world deployment. Demo: this https URL",
    "paper_abstract_zh": "声学场景感知涉及描述声音的类型、时间、方向与距离，以及响度与混响。虽然音频语言模型在声音识别方面表现出色，但单声道输入从根本上限制了空间理解。本研究提出了Sci-Phi，一种具有双空间和频谱编码器的大语言模型，用于估计所有声源和周围环境的完整参数集。通过学习超过4000小时的合成一阶Ambisonics录音（包括元数据），Sci-Phi能够一次性枚举并描述多达4个方向性声源，同时包括非方向性背景声音和房间特性。我们通过排列不变性协议和涵盖内容、位置、时间、响度及混响的15项指标对模型进行评估，并分析其在声源数量、信噪比、混响水平以及声学、空间或时间相似源混合等挑战下的鲁棒性。值得注意的是，Sci-Phi仅需微小的性能下降即可泛化至真实房间脉冲响应。总体而言，这项工作建立了首个能够完整描述空间场景的音频大语言模型，具有强大的实际部署潜力。演示：此HTTPS URL。",
    "subjects": [
      "Computation and Language (cs.CL)",
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "update_time": "2025-10-08",
    "paper_authors": "Xilin Jiang, Hannes Gamper, Sebastian Braun",
    "topic": [
      "Audio Representation Learning",
      "Audio Classification"
    ],
    "category": [
      "Other"
    ]
  },
  {
    "paper_title": "Transcribing Rhythmic Patterns of the Guitar Track in Polyphonic Music",
    "paper_title_zh": "多音音乐中吉他轨道的节奏模式转录",
    "paper_id": "2510.05756",
    "paper_abstract": "Whereas chord transcription has received considerable attention during the past couple of decades, far less work has been devoted to transcribing and encoding the rhythmic patterns that occur in a song. The topic is especially relevant for instruments such as the rhythm guitar, which is typically played by strumming rhythmic patterns that repeat and vary over time. However, in many cases one cannot objectively define a single \"right\" rhythmic pattern for a given song section. To create a dataset with well-defined ground-truth labels, we asked expert musicians to transcribe the rhythmic patterns in 410 popular songs and record cover versions where the guitar tracks followed those transcriptions. To transcribe the strums and their corresponding rhythmic patterns, we propose a three-step framework. Firstly, we perform approximate stem separation to extract the guitar part from the polyphonic mixture. Secondly, we detect individual strums within the separated guitar audio, using a pre-trained foundation model (MERT) as a backbone. Finally, we carry out a pattern-decoding process in which the transcribed sequence of guitar strums is represented by patterns drawn from an expert-curated vocabulary. We show that it is possible to transcribe the rhythmic patterns of the guitar track in polyphonic music with quite high accuracy, producing a representation that is human-readable and includes automatically detected bar lines and time signature markers. We perform ablation studies and error analysis and propose a set of evaluation metrics to assess the accuracy and readability of the predicted rhythmic pattern sequence.",
    "paper_abstract_zh": "尽管和弦转录在过去几十年中受到了相当大的关注，但对歌曲中节奏模式的转录和编码的研究却少得多。这一主题对于节奏吉他等乐器尤为重要，这些乐器通常通过弹奏重复且随时间变化的节奏模式来演奏。然而，在许多情况下，无法客观地定义一个单一“正确”的节奏模式用于歌曲段落。为了创建一个具有明确定义的真实标签的数据集，我们邀请了专业音乐家转录了410首流行歌曲中的节奏模式，并录制了遵循这些转录的翻唱版本中的吉他部分。为了转录这些弹奏及其相应的节奏模式，我们提出了一个三步框架。首先，我们执行近似的音轨分离以从多音混合中提取吉他部分。其次，我们在分离出的吉他音频中使用预训练的基干模型（MERT）检测单个弹奏。最后，我们进行模式解码处理，其中转录出的吉他弹奏序列由专家策划的词汇表中的模式表示。我们证明了以相当高的准确率转录多音音乐中吉他轨道的节奏模式是可能的，产生的表示是人类可读的，并包含自动检测到的小节线和拍号标记。我们进行了消融研究和错误分析，并提出了一套评估指标来评估预测节奏模式序列的准确性和可读性。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)"
    ],
    "update_time": "2025-10-08",
    "paper_authors": "Aleksandr Lukoianov, Anssi Klapuri",
    "topic": [
      "Music Information Retrieval",
      "Audio Representation Learning"
    ],
    "category": [
      "Music"
    ]
  },
  {
    "paper_title": "StereoSync: Spatially-Aware Stereo Audio Generation from Video",
    "paper_title_zh": "StereoSync: 基于视频的空间感知立体声生成",
    "paper_id": "2510.05828",
    "paper_abstract": "Although audio generation has been widely studied over recent years, video-aligned audio generation still remains a relatively unexplored frontier. To address this gap, we introduce StereoSync, a novel and efficient model designed to generate audio that is both temporally synchronized with a reference video and spatially aligned with its visual context. Moreover, StereoSync also achieves efficiency by leveraging pretrained foundation models, reducing the need for extensive training while maintaining high-quality synthesis. Unlike existing methods that primarily focus on temporal synchronization, StereoSync introduces a significant advancement by incorporating spatial awareness into video-aligned audio generation. Indeed, given an input video, our approach extracts spatial cues from depth maps and bounding boxes, using them as cross-attention conditioning in a diffusion-based audio generation model. Such an approach allows StereoSync to go beyond simple synchronization, producing stereo audio that dynamically adapts to the spatial structure and movement of a video scene. We evaluate StereoSync on Walking The Maps, a curated dataset comprising videos from video games that feature animated characters walking through diverse environments. Experimental results demonstrate the ability of StereoSync to achieve both temporal and spatial alignment, advancing the state of the art in video-to-audio generation and resulting in a significantly more immersive and realistic audio experience.",
    "paper_abstract_zh": "尽管音频生成在近年来已被广泛研究，但视频对齐的音频生成仍然是一个相对未被探索的领域。为了填补这一空白，我们引入了 StereoSync，这是一种新颖高效的模型，旨在生成既与参考视频时间同步又与其视觉内容空间对齐的音频。此外，StereoSync 通过利用预训练的基础模型实现了高效性，在保持高质量合成的同时减少了对大量训练的需求。与现有主要关注时间同步的方法不同，StereoSync 通过将空间感知引入视频对齐音频生成取得了显著进展。具体来说，给定输入视频，我们的方法从深度图和边界框中提取空间线索，并将它们用作基于扩散的音频生成模型中的交叉注意力条件。这种方法使 StereoSync 能够超越简单的同步，生成能够动态适应视频场景空间结构和运动的立体声音频。我们在 Walking The Maps 数据集上评估 StereoSync，这是一个包含电子游戏中角色在不同环境中行走视频的精选数据集。实验结果表明，StereoSync 能够实现时间和空间的双重对齐，推动了视频到音频生成领域的技术前沿，并实现了显著更沉浸和真实的音频体验。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Multimedia (cs.MM)",
      "Sound (cs.SD)"
    ],
    "update_time": "2025-10-08",
    "paper_authors": "Christian Marinoni, Riccardo Fosco Gramaccioni, Kazuki Shimada, Takashi Shibuya, Yuki Mitsufuji, Danilo Comminiello",
    "topic": [
      "Other"
    ],
    "category": [
      "Image&Video"
    ]
  },
  {
    "paper_title": "FoleyGRAM: Video-to-Audio Generation with GRAM-Aligned Multimodal Encoders",
    "paper_title_zh": "FoleyGRAM：基于GRAM对齐多模态编码器的视频到音频生成方法",
    "paper_id": "2510.05829",
    "paper_abstract": "In this work, we present FoleyGRAM, a novel approach to video-to-audio generation that emphasizes semantic conditioning through the use of aligned multimodal encoders. Building on prior advancements in video-to-audio generation, FoleyGRAM leverages the Gramian Representation Alignment Measure (GRAM) to align embeddings across video, text, and audio modalities, enabling precise semantic control over the audio generation process. The core of FoleyGRAM is a diffusion-based audio synthesis model conditioned on GRAM-aligned embeddings and waveform envelopes, ensuring both semantic richness and temporal alignment with the corresponding input video. We evaluate FoleyGRAM on the Greatest Hits dataset, a standard benchmark for video-to-audio models. Our experiments demonstrate that aligning multimodal encoders using GRAM enhances the system's ability to semantically align generated audio with video content, advancing the state of the art in video-to-audio synthesis.",
    "paper_abstract_zh": "本研究中，我们提出了FoleyGRAM，这是一种视频到音频生成的新方法，其核心是通过对齐的多模态编码器实现语义条件控制。FoleyGRAM基于先前视频到音频生成技术的进展，利用格拉米安表示对齐度量（GRAM）来对齐视频、文本和音频模态的嵌入表示，从而实现对音频生成过程的精确语义控制。FoleyGRAM的核心是一个基于扩散的音频合成模型，该模型以GRAM对齐的嵌入和波形包络为条件，确保了语义丰富性以及与输入视频的时间对齐。我们在Greatest Hits数据集（视频到音频模型的标准基准）上评估了FoleyGRAM。实验表明，使用GRAM对齐多模态编码器能够增强系统在语义上对齐生成音频与视频内容的能力，推动了视频到音频合成技术的前沿。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Multimedia (cs.MM)",
      "Sound (cs.SD)"
    ],
    "update_time": "2025-10-08",
    "paper_authors": "Riccardo Fosco Gramaccioni, Christian Marinoni, Eleonora Grassucci, Giordano Cicchetti, Aurelio Uncini, Danilo Comminiello",
    "topic": [],
    "category": []
  },
  {
    "paper_title": "Segment-Factorized Full-Song Generation on Symbolic Piano Music",
    "paper_title_zh": "符号钢琴音乐中的分段因子化全曲生成",
    "paper_id": "2510.05881",
    "paper_abstract": "We propose the Segmented Full-Song Model (SFS) for symbolic full-song generation. The model accepts a user-provided song structure and an optional short seed segment that anchors the main idea around which the song is developed. By factorizing a song into segments and generating each one through selective attention to related segments, the model achieves higher quality and efficiency compared to prior work. To demonstrate its suitability for human-AI interaction, we further wrap SFS into a web application that enables users to iteratively co-create music on a piano roll with customizable structures and flexible ordering.",
    "paper_abstract_zh": "我们提出了分段全曲模型（SFS）用于符号全曲生成。该模型接受用户提供的歌曲结构和一个可选的短种子片段，该片段锚定了歌曲发展的主要思想。通过将歌曲分解为多个分段，并通过选择性关注相关分段来生成每一段，该模型实现了与先前工作相比更高的质量和效率。为了展示其在人机交互中的适用性，我们进一步将SFS包装成一个网络应用程序，使用户能够通过可自定义结构和灵活顺序在钢琴卷帘上迭代协同创作音乐。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)",
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Multimedia (cs.MM)",
      "Sound (cs.SD)"
    ],
    "update_time": "2025-10-08",
    "paper_authors": "Ping-Yi Chen, Chih-Pin Tan, Yi-Hsuan Yang",
    "topic": [
      "Music Generation",
      "Music Information Retrieval"
    ],
    "category": [
      "Music"
    ]
  },
  {
    "paper_title": "ECTSpeech: Enhancing Efficient Speech Synthesis via Easy Consistency Tuning",
    "paper_title_zh": "ECTSpeech：通过简易一致性调优增强高效语音合成",
    "paper_id": "2510.05984",
    "paper_abstract": "Diffusion models have demonstrated remarkable performance in speech synthesis, but typically require multi-step sampling, resulting in low inference efficiency. Recent studies address this issue by distilling diffusion models into consistency models, enabling efficient one-step generation. However, these approaches introduce additional training costs and rely heavily on the performance of pre-trained teacher models. In this paper, we propose ECTSpeech, a simple and effective one-step speech synthesis framework that, for the first time, incorporates the Easy Consistency Tuning (ECT) strategy into speech synthesis. By progressively tightening consistency constraints on a pre-trained diffusion model, ECTSpeech achieves high-quality one-step generation while significantly reducing training complexity. In addition, we design a multi-scale gate module (MSGate) to enhance the denoiser's ability to fuse features at different scales. Experimental results on the LJSpeech dataset demonstrate that ECTSpeech achieves audio quality comparable to state-of-the-art methods under single-step sampling, while substantially reducing the model's training cost and complexity.",
    "paper_abstract_zh": "扩散模型在语音合成领域已展现出卓越的性能，但通常需要多步采样，导致推理效率低下。近期研究通过将扩散模型蒸馏为一致性模型来解决这一问题，实现高效的一步生成。然而，这些方法引入了额外的训练成本，并严重依赖于预训练教师模型的性能。本文提出 ECTSpeech，一个简单有效的一步语音合成框架，首次将简易一致性调优（ECT）策略引入语音合成。通过在预训练的扩散模型上逐步收紧一致性约束，ECTSpeech 实现了高质量的一步生成，同时显著降低了训练复杂度。此外，我们设计了一个多尺度门控模块（MSGate）以增强去噪器在不同尺度上融合特征的能力。在 LJSpeech 数据集上的实验结果表明，ECTSpeech 在单步采样下实现了与最先进方法相当的音频质量，同时显著降低了模型的训练成本和复杂度。",
    "subjects": [
      "Artificial Intelligence (cs.AI)",
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "update_time": "2025-10-08",
    "paper_authors": "Tao Zhu, Yinfeng Yu, Liejun Wang, Fuchun Sun, Wendong Zheng",
    "topic": [
      "Speech Synthesis"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Latent Speech-Text Transformer",
    "paper_title_zh": "潜在语音-文本转换器",
    "paper_id": "2510.06195",
    "paper_abstract": "Auto-regressive speech-text models are typically pre-trained on a large number of interleaved sequences of text tokens and raw speech encoded as speech tokens using vector quantization. These models have demonstrated state-of-the-art performance in speech-to-speech understanding and generation benchmarks, together with promising scaling laws, primarily enabled by the representational alignment between text and speech. Nevertheless, they suffer from shortcomings, partly owing to the disproportionately longer sequences of speech tokens in contrast to textual tokens. This results in a large compute imbalance between modalities during pre-training as well as during inference, and a potential hindrance to effectively aligning speech and text, ultimately translating to several orders of magnitude slower scaling laws. We introduce the Latent Speech-Text Transformer (LST), which makes pre-training speech-text models more data-efficient by dynamically and inexpensively aggregating speech tokens into latent speech patches. These patches serve as higher-level units that can either align with corresponding textual units to aid capability transfer or even encapsulate common speech sequences like silences to be more compute-efficient. We show that LST outperforms vanilla approaches on speech-to-speech as well as text-to-text benchmarks in both data- and compute-controlled settings, the former indicating more effective representational alignment and the latter indicating steeper scaling laws for speech-text models. On HellaSwag story completion, LST achieves 6.5% absolute gain in speech accuracy under compute-controlled training and 5.3% under data-controlled training, while also improving text performance. We will release our models, code, and the evaluation data to facilitate further research.",
    "paper_abstract_zh": "自回归语音-文本模型通常在大规模交织的文本标记序列和通过向量量化编码为语音标记的原始语音上进行预训练。这些模型在语音到语音的理解和生成基准测试中展现了最先进的性能，并伴随着有前景的缩放规律，这主要得益于语音与文本之间的表示对齐。然而，它们也存在一些缺陷，部分原因是语音标记序列相比文本标记序列过长。这导致了在预训练和推理过程中模态间的计算不均衡，以及对有效对齐语音和文本的潜在阻碍，最终转化为数量级更慢的缩放规律。我们引入了潜在语音-文本转换器（LST），它通过动态且低成本地将语音标记聚合成潜在的语音片段，从而使语音-文本模型的预训练更加数据高效。这些片段作为更高级别的单元，既可以与相应的文本单元对齐以实现能力迁移，也可以封装常见的语音序列（如静音）以提高计算效率。我们证明，LST在语音到语音以及文本到文本的基准测试中均优于传统方法，无论是在数据控制还是计算控制的设置下，前者表明了更有效的表示对齐，而后者表明了语音-文本模型更陡峭的缩放规律。在HellaSwag故事补全任务上，LST在计算控制训练下实现了语音准确率6.5%的绝对提升，在数据控制训练下实现了5.3%的提升，同时改进了文本性能。我们将发布我们的模型、代码和评估数据以促进进一步研究。",
    "subjects": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2025-10-08",
    "paper_authors": "Yen-Ju Lu, Yashesh Gaur, Wei Zhou, Benjamin Muller, Jesus Villalba, Najim Dehak, Luke Zettlemoyer, Gargi Ghosh, Mike Lewis, Srinivasan Iyer, Duc Le",
    "topic": [
      "Audio Representation Learning",
      "Speech Recognition"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Modulation Discovery with Differentiable Digital Signal Processing",
    "paper_title_zh": "具有可微分数字信号处理的调制发现",
    "paper_id": "2510.06204",
    "paper_abstract": "Modulations are a critical part of sound design and music production, enabling the creation of complex and evolving audio. Modern synthesizers provide envelopes, low frequency oscillators (LFOs), and more parameter automation tools that allow users to modulate the output with ease. However, determining the modulation signals used to create a sound is difficult, and existing sound-matching / parameter estimation systems are often uninterpretable black boxes or predict high-dimensional framewise parameter values without considering the shape, structure, and routing of the underlying modulation curves. We propose a neural sound-matching approach that leverages modulation extraction, constrained control signal parameterizations, and differentiable digital signal processing (DDSP) to discover the modulations present in a sound. We demonstrate the effectiveness of our approach on highly modulated synthetic and real audio samples, its applicability to different DDSP synth architectures, and investigate the trade-off it incurs between interpretability and sound-matching accuracy. We make our code and audio samples available and provide the trained DDSP synths in a VST plugin.",
    "paper_abstract_zh": "调制是声音设计和音乐制作的关键部分，能够创造出复杂且不断变化的音频。现代合成器提供了包络、低频振荡器（LFO）等参数自动化工具，使用户能够轻松调制输出。然而，确定用于创建声音的调制信号是困难的，现有的声音匹配/参数估计系统往往是不透明的黑盒，或预测高维度的逐帧参数值，而不考虑底层调制曲线的形状、结构和路由。我们提出了一种神经声音匹配方法，利用调制提取、约束控制信号参数化和可微分数字信号处理（DDSP）来发现声音中的调制。我们在高度调制的合成和真实音频样本上展示了我们方法的有效性，其对不同DDSP合成器架构的适用性，并研究了其在可解释性与声音匹配准确性之间的权衡。我们公开了我们的代码和音频样本，并以VST插件的形式提供了训练好的DDSP合成器。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)"
    ],
    "update_time": "2025-10-08",
    "paper_authors": "Christopher Mitcheltree, Hao Hao Tan, Joshua D. Reiss",
    "topic": [
      "Audio Representation Learning",
      "Music Information Retrieval"
    ],
    "category": [
      "Music"
    ]
  },
  {
    "paper_title": "Provable Speech Attributes Conversion via Latent Independence",
    "paper_title_zh": "可证明的语音属性转换：通过潜在独立性实现",
    "paper_id": "2510.05191",
    "paper_abstract": "While signal conversion and disentangled representation learning have shown promise for manipulating data attributes across domains such as audio, image, and multimodal generation, existing approaches, especially for speech style conversion, are largely empirical and lack rigorous theoretical foundations to guarantee reliable and interpretable control. In this work, we propose a general framework for speech attribute conversion, accompanied by theoretical analysis and guarantees under reasonable assumptions. Our framework builds on a non-probabilistic autoencoder architecture with an independence constraint between the predicted latent variable and the target controllable variable. This design ensures a consistent signal transformation, conditioned on an observed style variable, while preserving the original content and modifying the desired attribute. We further demonstrate the versatility of our method by evaluating it on speech styles, including speaker identity and emotion. Quantitative evaluations confirm the effectiveness and generality of the proposed approach.",
    "paper_abstract_zh": "尽管信号转换和解耦表示学习在音频、图像和多模态生成等领域的数据属性操控方面展现出潜力，但现有方法，特别是针对语音风格转换的，大多基于经验且缺乏严格的理论基础，难以保证可靠和可解释的控制。本工作提出一个语音属性转换的通用框架，并附带理论分析和合理假设下的保障。我们的框架基于一个非概率自编码器架构，其特点是在预测的潜在变量与目标可控变量之间施加独立性约束。这一设计确保了在给定观察到的风格变量条件下，能够实现一致的信号转换，同时保持原始内容不变并修改目标属性。我们进一步通过在语音风格（包括说话人身份和情感）上的评估，展示了方法的通用性。量化评估确认了所提方法的有效性和普适性。",
    "subjects": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)"
    ],
    "update_time": "2025-10-08",
    "paper_authors": "Jonathan Svirsky, Ofir Lindenbaum, Uri Shaham",
    "topic": [
      "Speech Synthesis",
      "Audio Representation Learning"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "AUREXA-SE: Audio-Visual Unified Representation Exchange Architecture with Cross-Attention and Squeezeformer for Speech Enhancement",
    "paper_title_zh": "AUREXA-SE：基于跨注意力和Squeezeformer的视听统一表征交换架构及其在语音增强中的应用",
    "paper_id": "2510.05295",
    "paper_abstract": "In this paper, we propose AUREXA-SE (Audio-Visual Unified Representation Exchange Architecture with Cross-Attention and Squeezeformer for Speech Enhancement), a progressive bimodal framework tailored for audio-visual speech enhancement (AVSE). AUREXA-SE jointly leverages raw audio waveforms and visual cues by employing a U-Net-based 1D convolutional encoder for audio and a Swin Transformer V2 for efficient and expressive visual feature extraction. Central to the architecture is a novel bidirectional cross-attention mechanism, which facilitates deep contextual fusion between modalities, enabling rich and complementary representation learning. To capture temporal dependencies within the fused embeddings, a stack of lightweight Squeezeformer blocks combining convolutional and attention modules is introduced. The enhanced embeddings are then decoded via a U-Net-style decoder for direct waveform reconstruction, ensuring perceptually consistent and intelligible speech output. Experimental evaluations demonstrate the effectiveness of AUREXA-SE, achieving significant performance improvements over noisy baselines, with STOI of 0.516, PESQ of 1.323, and SI-SDR of -4.322 dB. The source code of AUREXA-SE is available at this https URL.",
    "paper_abstract_zh": "本文提出AUREXA-SE（基于跨注意力和Squeezeformer的视听统一表征交换架构用于语音增强），这是一个专为视听语音增强（AVSE）设计的渐进式双模态框架。AUREXA-SE联合利用原始音频波形和视觉线索，采用基于U-Net的一维卷积编码器处理音频，以及Swin Transformer V2进行高效且富有表现力的视觉特征提取。该架构的核心是一个新颖的双向跨注意力机制，它促进了模态间的深度上下文融合，实现了丰富且互补的表征学习。为了捕捉融合嵌入中的时间依赖性，引入了一组结合了卷积和注意力模块的轻量级Squeezeformer块。增强后的嵌入随后通过一个U-Net风格的解码器进行解码，以实现波形重建，确保输出语音在感知上保持连贯且清晰可懂。实验评估证明了AUREXA-SE的有效性，相比嘈杂基线取得了显著性能提升，其STOI为0.516，PESQ为1.323，SI-SDR为-4.322 dB。AUREXA-SE的源代码可在以下网址获取：https://example.com（实际URL已省略）。",
    "subjects": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Multimedia (cs.MM)"
    ],
    "update_time": "2025-10-08",
    "paper_authors": "M. Sajid, Deepanshu Gupta, Yash Modi, Sanskriti Jain, Harshith Jai Surya Ganji, A. Rahaman, Harshvardhan Choudhary, Nasir Saleem, Amir Hussain, M. Tanveer",
    "topic": [
      "Speech Enhancement",
      "Audio Representation Learning"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Sparse deepfake detection promotes better disentanglement",
    "paper_title_zh": "稀疏伪造检测促进更好的解耦",
    "paper_id": "2510.05696",
    "paper_abstract": "Due to the rapid progress of speech synthesis, deepfake detection has become a major concern in the speech processing community. Because it is a critical task, systems must not only be efficient and robust, but also provide interpretable explanations. Among the different approaches for explainability, we focus on the interpretation of latent representations. In such paper, we focus on the last layer of embeddings of AASIST, a deepfake detection architecture. We use a TopK activation inspired by SAEs on this layer to obtain sparse representations which are used in the decision process. We demonstrate that sparse deepfake detection can improve detection performance, with an EER of 23.36% on ASVSpoof5 test set, with 95% of sparsity. We then show that these representations provide better disentanglement, using completeness and modularity metrics based on mutual information. Notably, some attacks are directly encoded in the latent space.",
    "paper_abstract_zh": "由于语音合成的快速发展，深度伪造检测已成为语音处理领域的一个主要关注点。由于这是一项关键任务，系统不仅需要高效和鲁棒，还需要提供可解释的解释。在不同可解释性方法中，我们专注于潜在表示的解释。在本文中，我们重点关注AASIST（一种深度伪造检测架构）的最后一层嵌入。我们使用受SAE（稀疏自编码器）启发的TopK激活机制来获取稀疏表示，并将其用于决策过程。我们证明稀疏深度伪造检测可以提高检测性能，在ASVSpoof5测试集上达到23.36%的等错误率（EER），稀疏度达到95%。然后我们展示这些表示通过基于互信息性的完整性和模块化性度量提供了更好的解耦。值得注意的是，某些攻击直接在潜在空间中被编码。",
    "subjects": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "update_time": "2025-10-08",
    "paper_authors": "Antoine Teissier, Marie Tahon, Nicolas Dugué, Aghilas Sini",
    "topic": [
      "Audio Representation Learning",
      "Speech Synthesis"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "MSF-SER: Enriching Acoustic Modeling with Multi-Granularity Semantics for Speech Emotion Recognition",
    "paper_title_zh": "MSF-SER: 通过多粒度语义增强语音情感识别中的声学建模",
    "paper_id": "2510.05749",
    "paper_abstract": "Continuous dimensional speech emotion recognition captures affective variation along valence, arousal, and dominance, providing finer-grained representations than categorical approaches. Yet most multimodal methods rely solely on global transcripts, leading to two limitations: (1) all words are treated equally, overlooking that emphasis on different parts of a sentence can shift emotional meaning; (2) only surface lexical content is represented, lacking higher-level interpretive cues. To overcome these issues, we propose MSF-SER (Multi-granularity Semantic Fusion for Speech Emotion Recognition), which augments acoustic features with three complementary levels of textual semantics--Local Emphasized Semantics (LES), Global Semantics (GS), and Extended Semantics (ES). These are integrated via an intra-modal gated fusion and a cross-modal FiLM-modulated lightweight Mixture-of-Experts (FM-MOE). Experiments on MSP-Podcast and IEMOCAP show that MSF-SER consistently improves dimensional prediction, demonstrating the effectiveness of enriched semantic fusion for SER.",
    "paper_abstract_zh": "连续维度语音情感识别捕捉了情感在效价、唤醒度和支配度方面的变化，提供了比分类方法更细粒度的表示。然而，大多数多模态方法仅依赖全局转录文本，导致两个局限：(1) 所有单词被平等对待，忽略了强调句子不同部分会改变情感含义；(2) 仅表示表面词汇内容，缺乏更高层次的解释性线索。为解决这些问题，我们提出MSF-SER（多粒度语义融合语音情感识别），它通过三个互补级别的文本语义来增强声学特征——局部强调语义(LES)、全局语义(GS)和扩展语义(ES)。这些通过模态内门控融合和跨模态FiLM调制轻量级专家混合模型(FM-MOE)进行集成。在MSP-Podcast和IEMOCAP上的实验表明，MSF-SER持续改进维度预测，证明了 enriched semantic fusion 对语音情感识别的有效性。",
    "subjects": [
      "Sound (cs.SD)"
    ],
    "update_time": "2025-10-08",
    "paper_authors": "Haoxun Li, Yuqing Sun, Hanlei Shi, Yu Liu, Leyuan Qu, Taihao Li",
    "topic": [
      "Audio Representation Learning",
      "Speech Recognition"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "EMORL-TTS: Reinforcement Learning for Fine-Grained Emotion Control in LLM-based TTS",
    "paper_title_zh": "EMORL-TTS：基于强化学习的细粒度情感控制LLM语音合成系统",
    "paper_id": "2510.05758",
    "paper_abstract": "Recent LLM-based TTS systems achieve strong quality and zero-shot ability, but lack fine-grained emotional control due to their reliance on discrete speech tokens. Existing approaches either limit emotions to categorical labels or cannot generalize to LLM-based architectures. We propose EMORL-TTS (Fine-grained Emotion-controllable TTS with Reinforcement Learning), a framework that unifies global intensity control in the VAD space with local emphasis regulation. Our method combines supervised fine-tuning with reinforcement learning guided by task-specific rewards for emotion category, intensity, and emphasis. Moreover, we further investigate how emphasis placement modulates fine-grained emotion intensity. Experiments show that EMORL-TTS improves emotion accuracy, intensity differentiation, and emphasis clarity, while preserving synthesis quality comparable to strong LLM-based baselines.",
    "paper_abstract_zh": "近期基于大语言模型(LLM)的语音合成(TTS)系统虽实现了较强的语音质量和零样本能力，但由于依赖离散的语音标记，难以实现细粒度的情感控制。现有方法要么将情感限制在分类标签中，要么无法适配于基于LLM的架构。我们提出了EMORL-TTS(基于强化学习的细粒度情感可控语音合成框架)，该框架统一了VAD(Valence-Arousal-Dominance)空间中的全局强度控制和局部强调调节。我们的方法结合了监督微调与由任务特定奖励(针对情感类别、强度和强调程度)引导的强化学习。此外，我们还研究了强调位置如何影响细粒度的情感强度。实验表明，EMORL-TTS在情感准确性、强度区分度和强调清晰度方面均有提升，同时保持了与主流基于LLM的基线相当的合成质量。",
    "subjects": [
      "Sound (cs.SD)"
    ],
    "update_time": "2025-10-08",
    "paper_authors": "Haoxun Li, Yu Liu, Yuqing Sun, Hanlei Shi, Leyuan Qu, Taihao Li",
    "topic": [
      "Speech Synthesis",
      "Audio Representation Learning"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "LARA-Gen: Enabling Continuous Emotion Control for Music Generation Models via Latent Affective Representation Alignment",
    "paper_title_zh": "LARA-Gen：通过潜在情感表征对齐实现音乐生成模型的连续情感控制",
    "paper_id": "2510.05875",
    "paper_abstract": "Recent advances in text-to-music models have enabled coherent music generation from text prompts, yet fine-grained emotional control remains unresolved. We introduce LARA-Gen, a framework for continuous emotion control that aligns the internal hidden states with an external music understanding model through Latent Affective Representation Alignment (LARA), enabling effective training. In addition, we design an emotion control module based on a continuous valence-arousal space, disentangling emotional attributes from textual content and bypassing the bottlenecks of text-based prompting. Furthermore, we establish a benchmark with a curated test set and a robust Emotion Predictor, facilitating objective evaluation of emotional controllability in music generation. Extensive experiments demonstrate that LARA-Gen achieves continuous, fine-grained control of emotion and significantly outperforms baselines in both emotion adherence and music quality. Generated samples are available at this https URL.",
    "paper_abstract_zh": "近年来，文本到音乐模型的进展使得从文本提示生成连贯的音乐成为可能，但细粒度的情感控制仍未解决。我们提出了LARA-Gen框架，通过潜在情感表征对齐（LARA）将内部隐藏状态与外部音乐理解模型对齐，从而实现有效的连续情感控制训练。此外，我们设计了一个基于连续效价-唤醒度空间的情感控制模块，将情感属性从文本内容中解耦，并绕过了基于文本提示的瓶颈。此外，我们建立了一个包含精选测试集和鲁棒情感预测器的基准测试，以促进对音乐生成中情感可控性的客观评估。大量实验表明，LARA-Gen实现了连续、细粒度的情感控制，并在情感一致性和音乐质量方面显著优于基线方法。生成样本可在该https URL查看。",
    "subjects": [
      "Sound (cs.SD)"
    ],
    "update_time": "2025-10-08",
    "paper_authors": "Jiahao Mei, Xuenan Xu, Zeyu Xie, Zihao Zheng, Ye Tao, Yue Ding, Mengyue Wu",
    "topic": [
      "Music Generation",
      "Audio Representation Learning"
    ],
    "category": [
      "Music"
    ]
  },
  {
    "paper_title": "EmoHRNet: High-Resolution Neural Network Based Speech Emotion Recognition",
    "paper_title_zh": "EmoHRNet: 基于高分辨率神经网络的语音情感识别",
    "paper_id": "2510.06072",
    "paper_abstract": "Speech emotion recognition (SER) is pivotal for enhancing human-machine interactions. This paper introduces \"EmoHRNet\", a novel adaptation of High-Resolution Networks (HRNet) tailored for SER. The HRNet structure is designed to maintain high-resolution representations from the initial to the final layers. By transforming audio samples into spectrograms, EmoHRNet leverages the HRNet architecture to extract high-level features. EmoHRNet's unique architecture maintains high-resolution representations throughout, capturing both granular and overarching emotional cues from speech signals. The model outperforms leading models, achieving accuracies of 92.45% on RAVDESS, 80.06% on IEMOCAP, and 92.77% on EMOVO. Thus, we show that EmoHRNet sets a new benchmark in the SER domain.",
    "paper_abstract_zh": "语音情感识别(SER)对于增强人机交互至关重要。本文提出了“EmoHRNet”，一种专为SER定制的高分辨率网络(HRNet)的变体。HRNet结构旨在从初始层到最终层都保持高分辨率表示。通过将音频样本转换为频谱图，EmoHRNet利用HRNet架构提取高级特征。EmoHRNet的独特架构在整个过程中保持高分辨率表示，捕捉语音信号中细粒度和整体性的情感线索。该模型性能优于领先模型，在RAVDESS上达到92.45%的准确率，在IEMOCAP上达到80.06%，在EMOVO上达到92.77%。因此，我们证明EmoHRNet为该领域设立了新标杆。",
    "subjects": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)"
    ],
    "update_time": "2025-10-08",
    "paper_authors": "Akshay Muppidi, Martin Radfar",
    "topic": [
      "Audio Classification",
      "Speech Recognition"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Data-efficient Targeted Token-level Preference Optimization for LLM-based Text-to-Speech",
    "paper_title_zh": "数据高效的目标令牌级偏好优化用于基于LLM的文本到语音合成",
    "paper_id": "2510.05799",
    "paper_abstract": "Aligning text-to-speech (TTS) system outputs with human feedback through preference optimization has been shown to effectively improve the robustness and naturalness of language model-based TTS models. Current approaches primarily require paired desirable and undesirable samples at the utterance level. However, such pairs are often limited in TTS output data, and utterance-level formulation prevents fine-grained token-level optimization needed for accurate pronunciation alignment. In this study, we propose TKTO that eliminates the need for paired data, enabling a more data-efficient training paradigm, and directly targets token-level units, automatically providing fine-grained alignment signals without token-level annotations. TKTO improves the challenging Japanese TTS accuracy by 39% and reduces CER by 54%, automatically assigning 12.8 times stronger reward to targeted tokens.",
    "paper_abstract_zh": "通过偏好优化将文本到语音（TTS）系统输出与人类反馈对齐，已被证明能有效提高基于语言模型的TTS模型的鲁棒性和自然性。当前方法主要需要在话语层面具备理想和非理想样本对。然而，这类配对在TTS输出数据中往往有限，且话语级别的制定阻碍了实现准确发音对齐所需的细粒度令牌级优化。在本研究中，我们提出了TKTO，它消除了对配对数据的需求，实现了更高效的数据驱动训练范式，并直接针对令牌级单位，自动提供细粒度的对齐信号而无需令牌级标注。TKTO将日语TTS的准确率提高了39%，CER降低了54%，并为目标令牌自动分配了12.8倍更强的奖励。",
    "subjects": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Sound (cs.SD)"
    ],
    "update_time": "2025-10-08",
    "paper_authors": "Rikuto Kotoge, Yuichi Sasaki",
    "topic": [
      "Speech Synthesis",
      "Speech Recognition"
    ],
    "category": [
      "Speech"
    ]
  }
]