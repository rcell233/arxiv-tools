[
  {
    "paper_title": "Contextualized Token Discrimination for Speech Search Query Correction",
    "paper_title_zh": "面向语音搜索查询纠错的上下文 Token 判别方法",
    "paper_id": "2509.04393v1",
    "paper_abstract": "Query spelling correction is an important function of modern search engines since it effectively helps users express their intentions clearly. With the growing popularity of speech search driven by Automated Speech Recognition (ASR) systems, this paper introduces a novel method named Contextualized Token Discrimination (CTD) to conduct effective speech query correction. In CTD, we first employ BERT to generate token-level contextualized representations and then construct a composition layer to enhance semantic information. Finally, we produce the correct query according to the aggregated token representation, correcting the incorrect tokens by comparing the original token representations and the contextualized representations. Extensive experiments demonstrate the superior performance of our proposed method across all metrics, and we further present a new benchmark dataset with erroneous ASR transcriptions to offer comprehensive evaluations for audio query correction.",
    "paper_abstract_zh": "查询拼写纠正是现代搜索引擎的重要功能，可帮助用户准确表达意图。随着自动语音识别（ASR）驱动的语音搜索日益普及，本文提出一种名为“上下文 Token 判别（CTD）”的新方法，用于高效纠正语音查询。CTD 首先利用 BERT 生成 Token 级上下文表示，随后构建组合层以增强语义信息，最后通过对比原始与上下文表示来纠正错误 Token，输出正确查询。大量实验表明，该方法在所有指标上均优于现有方法；我们还发布了一个包含 ASR 错误转录的新基准数据集，为音频查询纠错提供全面评测。",
    "primary_category": "cs.SD",
    "update_time": "2025-09-04",
    "paper_authors": "Junyu Lu, Di Jiang, Mengze Hong, Victor Junqiu Wei, Qintian Guo, Zhiyang Su",
    "topic": [
      "Speech Recognition"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Denoising GER: A Noise-Robust Generative Error Correction with LLM for Speech Recognition",
    "paper_title_zh": "Denoising GER：面向语音识别的噪声鲁棒生成式纠错框架",
    "paper_id": "2509.04392v1",
    "paper_abstract": "In recent years, large language models (LLM) have made significant progress in the task of generation error correction (GER) for automatic speech recognition (ASR) post-processing. However, in complex noisy environments, they still face challenges such as poor adaptability and low information utilization, resulting in limited effectiveness of GER. To address these issues, this paper proposes a noise-robust multi-modal GER framework (Denoising GER). The framework enhances the model's adaptability to different noisy scenarios through a noise-adaptive acoustic encoder and optimizes the integration of multi-modal information via a heterogeneous feature compensation dynamic fusion (HFCDF) mechanism, improving the LLM's utilization of multi-modal information. Additionally, reinforcement learning (RL) training strategies are introduced to enhance the model's predictive capabilities. Experimental results demonstrate that Denoising GER significantly improves accuracy and robustness in noisy environments and exhibits good generalization abilities in unseen noise scenarios.",
    "paper_abstract_zh": "近年来，大语言模型（LLM）在自动语音识别（ASR）后处理中的生成式纠错（GER）任务上取得显著进展。但在复杂噪声环境下，其适应性差、信息利用率低，导致 GER 效果受限。本文提出一种噪声鲁棒的多模态 GER 框架——Denoising GER。该框架通过噪声自适应声学编码器提升对不同噪声场景的适应性，并借助异构特征补偿动态融合（HFCDF）机制优化多模态信息整合，提高 LLM 对多模态信息的利用。此外，引入强化学习（RL）训练策略进一步增强模型预测能力。实验结果表明，Denoising GER 在噪声环境下显著提升了准确率与鲁棒性，并在未见噪声场景中展现出良好的泛化能力。",
    "primary_category": "cs.SD",
    "update_time": "2025-09-04",
    "paper_authors": "Yanyan Liu, Minqiang Xu, Yihao Chen, Liang He, Lei Fang, Sian Fang, Lin Liu",
    "topic": [
      "Speech Recognition"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "PARCO: Phoneme-Augmented Robust Contextual ASR via Contrastive Entity Disambiguation",
    "paper_title_zh": "PARCO：通过对比实体消歧实现音素增强的鲁棒上下文 ASR",
    "paper_id": "2509.04357v1",
    "paper_abstract": "Automatic speech recognition (ASR) systems struggle with domain-specific named entities, especially homophones. Contextual ASR improves recognition but often fails to capture fine-grained phoneme variations due to limited entity diversity. Moreover, prior methods treat entities as independent tokens, leading to incomplete multi-token biasing. To address these issues, we propose Phoneme-Augmented Robust Contextual ASR via COntrastive entity disambiguation (PARCO), which integrates phoneme-aware encoding, contrastive entity disambiguation, entity-level supervision, and hierarchical entity filtering. These components enhance phonetic discrimination, ensure complete entity retrieval, and reduce false positives under uncertainty. Experiments show that PARCO achieves CER of 4.22% on Chinese AISHELL-1 and WER of 11.14% on English DATA2 under 1,000 distractors, significantly outperforming baselines. PARCO also demonstrates robust gains on out-of-domain datasets like THCHS-30 and LibriSpeech.",
    "paper_abstract_zh": "自动语音识别（ASR）系统在处理领域特定命名实体尤其是同音词时表现不佳。上下文 ASR 虽能改善识别，却因实体多样性不足而难以捕捉细粒度音素差异；此外，现有方法将实体视为独立 Token，导致多 Token 偏置不完整。为此，我们提出“音素增强的鲁棒上下文 ASR 通过对比实体消歧（PARCO）”，集成音素感知编码、对比实体消歧、实体级监督与分层实体过滤，增强语音区分度，确保实体完整检索，并在不确定环境下降低误报。实验显示，PARCO 在含 1 000 个干扰项的条件下，中文 AISHELL-1 的 CER 达 4.22%，英文 DATA2 的 WER 达 11.14%，显著优于基线；在 THCHS-30 与 LibriSpeech 等跨域数据集上也表现出鲁棒提升。",
    "primary_category": "cs.CL",
    "update_time": "2025-09-04",
    "paper_authors": "Jiajun He, Naoki Sawada, Koichi Miyazaki, Tomoki Toda",
    "topic": [
      "Speech Recognition"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition from Physiological Signals",
    "paper_title_zh": "MuMTAffect：面向生理信号的人格与情感识别的多模态多任务情感框架",
    "paper_id": "2509.04254v1",
    "paper_abstract": "We present MuMTAffect, a novel Multimodal Multitask Affective Embedding Network designed for joint emotion classification and personality prediction (re-identification) from short physiological signal segments. MuMTAffect integrates multiple physiological modalities pupil dilation, eye gaze, facial action units, and galvanic skin response using dedicated, transformer-based encoders for each modality and a fusion transformer to model cross-modal interactions. Inspired by the Theory of Constructed Emotion, the architecture explicitly separates core affect encoding (valence/arousal) from higher-level conceptualization, thereby grounding predictions in contemporary affective neuroscience. Personality trait prediction is leveraged as an auxiliary task to generate robust, user-specific affective embeddings, significantly enhancing emotion recognition performance. We evaluate MuMTAffect on the AFFEC dataset, demonstrating that stimulus-level emotional cues (Stim Emo) and galvanic skin response substantially improve arousal classification, while pupil and gaze data enhance valence discrimination. The inherent modularity of MuMTAffect allows effortless integration of additional modalities, ensuring scalability and adaptability. Extensive experiments and ablation studies underscore the efficacy of our multimodal multitask approach in creating personalized, context-aware affective computing systems, highlighting pathways for further advancements in cross-subject generalisation.",
    "paper_abstract_zh": "本文提出 MuMTAffect，一种新颖的多模态多任务情感嵌入网络，用于基于短时生理信号段联合进行情感分类与人格（重识别）预测。MuMTAffect 整合瞳孔扩张、眼动、面部动作单元与皮肤电反应等多种生理模态，为每类模态配置专用 Transformer 编码器，并通过融合 Transformer 建模跨模态交互。受“建构情绪理论”启发，网络将核心情感编码（效价/唤醒）与高层概念化显式分离，使预测根植于当代情感神经科学。人格特质预测作为辅助任务，生成鲁棒的个体特异性情感嵌入，显著提升情感识别性能。在 AFFEC 数据集上的评估表明，刺激级情绪线索（Stim Emo）与皮肤电反应显著改善唤醒度分类，而瞳孔与眼动数据增强效价判别。MuMTAffect 的模块化设计便于无缝扩展新模态，确保可扩展性与适应性。大量实验与消融研究验证了多模态多任务方法在构建个性化、情境感知情感计算系统方面的有效性，并指出跨被试泛化的进一步研究方向。",
    "primary_category": "cs.HC",
    "update_time": "2025-09-04",
    "paper_authors": "Meisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Burke Dittberner, Paolo Burelli",
    "topic": [
      "Speech Recognition"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Enhancing Self-Supervised Speaker Verification Using Similarity-Connected Graphs and GCN",
    "paper_title_zh": "利用相似连接图与图卷积网络增强自监督说话人验证",
    "paper_id": "2509.04147v1",
    "paper_abstract": "With the continuous development of speech recognition technology, speaker verification (SV) has become an important method for identity authentication. Traditional SV methods rely on handcrafted feature extraction, while deep learning has significantly improved system performance. However, the scarcity of labeled data still limits the widespread application of deep learning in SV. Self-supervised learning, by mining latent information in large unlabeled datasets, enhances model generalization and is a key technology to address this issue.   DINO is an efficient self-supervised learning method that generates pseudo-labels from unlabeled speech data through clustering, supporting subsequent training. However, clustering may produce noisy pseudo-labels, which can reduce overall recognition performance.   To address this issue, this paper proposes an improved clustering framework based on similarity connection graphs and Graph Convolutional Networks. By leveraging GCNs' ability to model structured data and incorporating relational information between nodes in the similarity connection graph, the clustering process is optimized, improving pseudo-label accuracy and enhancing the robustness and performance of the self-supervised speaker verification system. Experimental results show that this method significantly improves system performance and provides a new approach for self-supervised speaker verification.   Index Terms: Speaker Verification, Self-Supervised Learning, DINO, Clustering Algorithm, Graph Convolutional Network, Similarity Connection Graph",
    "paper_abstract_zh": "随着语音识别技术的不断发展，说话人验证（SV）已成为身份认证的重要手段。传统SV方法依赖手工特征提取，而深度学习显著提升了系统性能。然而，标注数据稀缺仍限制深度学习在SV中的广泛应用。自监督学习通过挖掘大规模无标注数据中的潜在信息，增强模型泛化能力，是解决该问题的关键技术。DINO是一种高效的自监督学习方法，通过聚类从无标注语音数据生成伪标签，支持后续训练。但聚类可能产生噪声伪标签，降低整体识别性能。针对该问题，本文提出一种基于相似连接图与图卷积网络（GCN）的改进聚类框架。利用GCN对结构化数据的建模能力，融合相似连接图中节点间关系信息，优化聚类过程，提高伪标签准确性，增强自监督说话人验证系统的鲁棒性与性能。实验结果表明，该方法显著提升系统性能，为自监督说话人验证提供了新思路。",
    "primary_category": "cs.SD",
    "update_time": "2025-09-04",
    "paper_authors": "Zhaorui Sun, Yihao Chen, Jialong Wang, Minqiang Xu, Lei Fang, Sian Fang, Lin Liu",
    "topic": [
      "Speech Recognition"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Learning from Majority Label: A Novel Problem in Multi-class Multiple-Instance Learning",
    "paper_title_zh": "从多数标签学习：多类别多示例学习中的新问题",
    "paper_id": "2509.04023v1",
    "paper_abstract": "The paper proposes a novel multi-class Multiple-Instance Learning (MIL) problem called Learning from Majority Label (LML). In LML, the majority class of instances in a bag is assigned as the bag-level label. The goal of LML is to train a classification model that estimates the class of each instance using the majority label. This problem is valuable in a variety of applications, including pathology image segmentation, political voting prediction, customer sentiment analysis, and environmental monitoring. To solve LML, we propose a Counting Network trained to produce bag-level majority labels, estimated by counting the number of instances in each class. Furthermore, analysis experiments on the characteristics of LML revealed that bags with a high proportion of the majority class facilitate learning. Based on this result, we developed a Majority Proportion Enhancement Module (MPEM) that increases the proportion of the majority class by removing minority class instances within the bags. Experiments demonstrate the superiority of the proposed method on four datasets compared to conventional MIL methods. Moreover, ablation studies confirmed the effectiveness of each module. The code is available at \\href{https://github.com/Shiku-Kaito/Learning-from-Majority-Label-A-Novel-Problem-in-Multi-class-Multiple-Instance-Learning}{here}.",
    "paper_abstract_zh": "本文提出一种全新的多类别多示例学习（MIL）问题——从多数标签学习（LML）。在LML中，将包内实例的多数类别作为包级标签。其目标是利用该多数标签训练分类模型，以估计每个实例的类别。该问题在病理图像分割、政治投票预测、客户情感分析及环境监测等多种场景中具有重要价值。为解决LML，我们提出一种计数网络，通过统计各类实例数量生成包级多数标签。进一步分析发现，多数类占比高的包更利于学习。基于此，我们设计多数比例增强模块（MPEM），通过移除包内少数类实例来提升多数类比例。实验表明，所提方法在四个数据集上均优于传统MIL方法，且消融实验验证了各模块的有效性。代码已开源：https://github.com/Shiku-Kaito/Learning-from-Majority-Label-A-Novel-Problem-in-Multi-class-Multiple-Instance-Learning",
    "primary_category": "cs.CV",
    "update_time": "2025-09-04",
    "paper_authors": "Shiku Kaito, Shinnosuke Matsuo, Daiki Suehiro, Ryoma Bise",
    "topic": [
      "Speech Recognition"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Exploring NLP Benchmarks in an Extremely Low-Resource Setting",
    "paper_title_zh": "极低资源场景下的NLP基准探索",
    "paper_id": "2509.03962v1",
    "paper_abstract": "The effectiveness of Large Language Models (LLMs) diminishes for extremely low-resource languages, such as indigenous languages, primarily due to the lack of labeled data. Despite growing interest, the availability of high-quality natural language processing (NLP) datasets for these languages remains limited, making it difficult to develop robust language technologies. This paper addresses such gap by focusing on Ladin, an endangered Romance language, specifically targeting the Val Badia variant. Leveraging a small set of parallel Ladin-Italian sentence pairs, we create synthetic datasets for sentiment analysis and multiple-choice question answering (MCQA) by translating monolingual Italian data. To ensure linguistic quality and reliability, we apply rigorous filtering and back-translation procedures in our method. We further demonstrate that incorporating these synthetic datasets into machine translation training leads to substantial improvements over existing Italian-Ladin translation baselines. Our contributions include the first publicly available sentiment analysis and MCQA datasets for Ladin, establishing foundational resources that can support broader NLP research and downstream applications for this underrepresented language.",
    "paper_abstract_zh": "对于极低资源语言（如土著语言），大型语言模型（LLM）的效果因缺乏标注数据而显著下降。尽管关注度日益增加，这些语言的高质量自然语言处理（NLP）数据集依然稀缺，阻碍了稳健语言技术的开发。本文以濒危罗曼语族语言拉迪恩语的瓦尔巴迪亚方言为例，利用少量拉迪恩-意大利平行句对，通过翻译意大利单语数据，构建情感分析与多项选择问答（MCQA）的合成数据集。为确保语言质量与可靠性，我们采用严格的过滤与回译流程。实验表明，将这些合成数据集融入机器翻译训练，可较现有意大利-拉迪恩翻译基线带来显著提升。我们首次公开发布拉迪恩语情感分析与MCQA数据集，为该弱势语言的NLP研究与下游应用奠定基础性资源。",
    "primary_category": "cs.CL",
    "update_time": "2025-09-04",
    "paper_authors": "Ulin Nuha, Adam Jatowt",
    "topic": [
      "Speech Recognition"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "WenetSpeech-Yue: A Large-scale Cantonese Speech Corpus with Multi-dimensional Annotation",
    "paper_title_zh": "WenetSpeech-Yue：带多维标注的大规模粤语语音语料库",
    "paper_id": "2509.03959v1",
    "paper_abstract": "The development of speech understanding and generation has been significantly accelerated by the availability of large-scale, high-quality speech datasets. Among these, ASR and TTS are regarded as the most established and fundamental tasks. However, for Cantonese (Yue Chinese), spoken by approximately 84.9 million native speakers worldwide, limited annotated resources have hindered progress and resulted in suboptimal ASR and TTS performance. To address this challenge, we propose WenetSpeech-Pipe, an integrated pipeline for building large-scale speech corpus with multi-dimensional annotation tailored for speech understanding and generation. It comprises six modules: Audio Collection, Speaker Attributes Annotation, Speech Quality Annotation, Automatic Speech Recognition, Text Postprocessing and Recognizer Output Voting, enabling rich and high-quality annotations. Based on this pipeline, we release WenetSpeech-Yue, the first large-scale Cantonese speech corpus with multi-dimensional annotation for ASR and TTS, covering 21,800 hours across 10 domains with annotations including ASR transcription, text confidence, speaker identity, age, gender, speech quality scores, among other annotations. We also release WSYue-eval, a comprehensive Cantonese benchmark with two components: WSYue-ASR-eval, a manually annotated set for evaluating ASR on short and long utterances, code-switching, and diverse acoustic conditions, and WSYue-TTS-eval, with base and coverage subsets for standard and generalization testing. Experimental results show that models trained on WenetSpeech-Yue achieve competitive results against state-of-the-art (SOTA) Cantonese ASR and TTS systems, including commercial and LLM-based models, highlighting the value of our dataset and pipeline.",
    "paper_abstract_zh": "大规模高质量语音数据集的涌现极大推动了语音理解与生成的发展，其中自动语音识别（ASR）与语音合成（TTS）被视为最基础的任务。然而，全球约8490万母语使用者的粤语（粤方言）因标注资源匮乏，导致ASR与TTS性能欠佳。为此，我们提出WenetSpeech-Pipe——面向语音理解与生成的大规模语料构建一体化流水线，包含音频采集、说话人属性标注、语音质量标注、自动语音识别、文本后处理及识别结果投票六大模块，实现丰富且高质量的多维标注。基于该流水线，我们发布首个大规模多维标注粤语语料库WenetSpeech-Yue，涵盖21,800小时、10大领域，提供ASR转写、文本置信度、说话人身份、年龄、性别、语音质量评分等标注。同时发布综合粤语基准WSYue-eval，包括手动标注的WSYue-ASR-eval（评估短句、长句、语码转换及多样声学条件下的ASR）与WSYue-TTS-eval（标准与泛化测试的基线及覆盖子集）。实验表明，在WenetSpeech-Yue上训练的模型在粤语ASR与TTS任务中均可与最先进的商业及LLM基线竞争，凸显本数据集与流水线的价值。",
    "primary_category": "cs.SD",
    "update_time": "2025-09-04",
    "paper_authors": "Longhao Li, Zhao Guo, Hongjie Chen, Yuhang Dai, Ziyu Zhang, Hongfei Xue, Tianlun Zuo, Chengyou Wang, Shuiyuan Wang, Jie Li, Xin Xu, Hui Bu, Binbin Zhang, Ruibin Yuan, Ziya Zhou, Wei Xue, Lei Xie",
    "topic": [
      "Speech Recognition",
      "Speech Synthesis"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "A Multidimensional AI-powered Framework for Analyzing Tourist Perception in Historic Urban Quarters: A Case Study in Shanghai",
    "paper_title_zh": "面向历史街区游客感知的多维AI框架：以上海为例",
    "paper_id": "2509.03830v1",
    "paper_abstract": "Historic urban quarters play a vital role in preserving cultural heritage while serving as vibrant spaces for tourism and everyday life. Understanding how tourists perceive these environments is essential for sustainable, human-centered urban planning. This study proposes a multidimensional AI-powered framework for analyzing tourist perception in historic urban quarters using multimodal data from social media. Applied to twelve historic quarters in central Shanghai, the framework integrates focal point extraction, color theme analysis, and sentiment mining. Visual focus areas are identified from tourist-shared photos using a fine-tuned semantic segmentation model. To assess aesthetic preferences, dominant colors are extracted using a clustering method, and their spatial distribution across quarters is analyzed. Color themes are further compared between social media photos and real-world street views, revealing notable shifts. This divergence highlights potential gaps between visual expectations and the built environment, reflecting both stylistic preferences and perceptual bias. Tourist reviews are evaluated through a hybrid sentiment analysis approach combining a rule-based method and a multi-task BERT model. Satisfaction is assessed across four dimensions: tourist activities, built environment, service facilities, and business formats. The results reveal spatial variations in aesthetic appeal and emotional response. Rather than focusing on a single technical innovation, this framework offers an integrated, data-driven approach to decoding tourist perception and contributes to informed decision-making in tourism, heritage conservation, and the design of aesthetically engaging public spaces.",
    "paper_abstract_zh": "历史街区在保护文化遗产的同时，也是旅游与日常生活的活力空间。理解游客对这些环境的感知对可持续、以人为本的城市规划至关重要。本研究提出一种多维AI框架，利用社交媒体多模态数据分析历史街区游客感知。该框架应用于上海中心城区的十二个历史街区，集成焦点提取、色彩主题分析与情感挖掘。通过微调语义分割模型识别游客分享照片中的视觉焦点区域；采用聚类方法提取主导色彩并分析其在各街区的空间分布，再将社交媒体照片色彩主题与真实街景进行对比，发现显著差异，揭示视觉期望与建成环境之间的潜在差距，反映审美偏好与感知偏差。游客评论则通过基于规则与多任务BERT混合的情感分析方法评估，从旅游活动、建成环境、服务设施与商业业态四个维度衡量满意度。结果呈现审美吸引力与情感反应的空间差异。该框架并非聚焦单一技术创新，而是提供一种整合、数据驱动的游客感知解码方法，为旅游、遗产保护与美学公共空间设计的明智决策提供支持。",
    "primary_category": "cs.AI",
    "update_time": "2025-09-04",
    "paper_authors": "Kaizhen Tan, Yufan Wu, Yuxuan Liu, Haoran Zeng",
    "topic": [
      "Speech Recognition"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Speech Intelligibility Assessment with Uncertainty-Aware Whisper Embeddings and sLSTM",
    "paper_title_zh": "基于不确定性感知Whisper嵌入与sLSTM的可懂度评估",
    "paper_id": "2509.03013v2",
    "paper_abstract": "Non-intrusive speech intelligibility prediction remains challenging due to variability in speakers, noise conditions, and subjective perception. We propose an uncertainty-aware approach that leverages Whisper embeddings in combination with statistical features, specifically the mean, standard deviation, and entropy computed across the embedding dimensions. The entropy, computed via a softmax over the feature dimension, serves as a proxy for uncertainty, complementing global information captured by the mean and standard deviation. To model the sequential structure of speech, we adopt a scalar long short-term memory (sLSTM) network, which efficiently captures long-range dependencies. Building on this foundation, we propose iMTI-Net, an improved multi-target intelligibility prediction network that integrates convolutional neural network (CNN) and sLSTM components within a multitask learning framework. It jointly predicts human intelligibility scores and machine-based word error rates (WER) from Google ASR and Whisper. Experimental results show that iMTI-Net outperforms the original MTI-Net across multiple evaluation metrics, demonstrating the effectiveness of incorporating uncertainty-aware features and the CNN-sLSTM architecture.",
    "paper_abstract_zh": "非侵入式语音可懂度预测因说话人、噪声条件及主观感知的差异而仍具挑战。我们提出一种不确定性感知方法，利用Whisper嵌入与统计特征（均值、标准差及嵌入维度的熵）相结合。通过特征维度上的softmax计算得到的熵作为不确定性的代理，与均值和标准差捕获的全局信息互补。为建模语音的序列结构，采用标量长短期记忆网络（sLSTM）高效捕捉长程依赖。在此基础上，提出iMTI-Net——一种改进的多目标可懂度预测网络，在多任务学习框架内融合CNN与sLSTM组件，联合预测人类可懂度评分及基于Google ASR与Whisper的机器词错误率（WER）。实验表明，iMTI-Net在多项评估指标上优于原始MTI-Net，验证了引入不确定性感知特征与CNN-sLSTM架构的有效性。",
    "primary_category": "eess.AS",
    "update_time": "2025-09-04",
    "paper_authors": "Ryandhimas E. Zezario, Dyah A. M. G. Wisnu, Hsin-Min Wang, Yu Tsao",
    "topic": [
      "Speech Recognition"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "AudioCodecBench: A Comprehensive Benchmark for Audio Codec Evaluation",
    "paper_title_zh": "AudioCodecBench：音频编解码器综合评测基准",
    "paper_id": "2509.02349v2",
    "paper_abstract": "Multimodal Large Language Models (MLLMs) have been widely applied in speech and music. This tendency has led to a focus on audio tokenization for Large Models (LMs). Unlike semantic-only text tokens, audio tokens must both capture global semantic content and preserve fine-grained acoustic details. Moreover, they provide a discrete method for speech and music that can be effectively integrated into MLLMs. However, existing research is unsuitable in the definitions of semantic tokens and acoustic tokens. In addition, the evaluation of different codecs typically concentrates on specific domains or tasks, such as reconstruction or Automatic Speech Recognition (ASR) task, which prevents fair and comprehensive comparisons. To address these problems, this paper provides suitable definitions for semantic and acoustic tokens and introduces a systematic evaluation framework. This framework allows for a comprehensive assessment of codecs' capabilities which evaluate across four dimensions: audio reconstruction metric, codebook index (ID) stability, decoder-only transformer perplexity, and performance on downstream probe tasks. Our results show the correctness of the provided suitable definitions and the correlation among reconstruction metrics, codebook ID stability, downstream probe tasks and perplexity.",
    "paper_abstract_zh": "多模态大语言模型（MLLM）已广泛应用于语音与音乐领域，促使研究聚焦面向大模型的音频标记化。与仅含语义的文本标记不同，音频标记需同时捕获全局语义内容并保留细粒度声学细节，并为语音与音乐提供可高效集成至MLLM的离散表示。然而，现有研究对语义标记与声学标记的定义并不统一，且不同编解码器的评估通常局限于重构或语音识别等单一任务，难以进行公平全面比较。为此，本文给出语义与声学标记的明确定义，并提出系统化评估框架，从音频重构指标、码本索引稳定性、仅解码器Transformer困惑度及下游探测任务性能四个维度综合衡量编解码器能力。实验结果验证了我们定义的正确性，并揭示了重构指标、码本ID稳定性、下游任务与困惑度之间的相关性。",
    "primary_category": "cs.SD",
    "update_time": "2025-09-04",
    "paper_authors": "Lu Wang, Hao Chen, Siyu Wu, Zhiyue Wu, Hao Zhou, Chengfeng Zhang, Ting Wang, Haodi Zhang",
    "topic": [
      "Speech Recognition",
      "Audio Codec"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "AUDETER: A Large-scale Dataset for Deepfake Audio Detection in Open Worlds",
    "paper_title_zh": "AUDETER：面向开放世界深度伪造音频检测的大规模数据集",
    "paper_id": "2509.04345v1",
    "paper_abstract": "Speech generation systems can produce remarkably realistic vocalisations that are often indistinguishable from human speech, posing significant authenticity challenges. Although numerous deepfake detection methods have been developed, their effectiveness in real-world environments remains unrealiable due to the domain shift between training and test samples arising from diverse human speech and fast evolving speech synthesis systems. This is not adequately addressed by current datasets, which lack real-world application challenges with diverse and up-to-date audios in both real and deep-fake categories. To fill this gap, we introduce AUDETER (AUdio DEepfake TEst Range), a large-scale, highly diverse deepfake audio dataset for comprehensive evaluation and robust development of generalised models for deepfake audio detection. It consists of over 4,500 hours of synthetic audio generated by 11 recent TTS models and 10 vocoders with a broad range of TTS/vocoder patterns, totalling 3 million audio clips, making it the largest deepfake audio dataset by scale. Through extensive experiments with AUDETER, we reveal that i) state-of-the-art (SOTA) methods trained on existing datasets struggle to generalise to novel deepfake audio samples and suffer from high false positive rates on unseen human voice, underscoring the need for a comprehensive dataset; and ii) these methods trained on AUDETER achieve highly generalised detection performance and significantly reduce detection error rate by 44.1% to 51.6%, achieving an error rate of only 4.17% on diverse cross-domain samples in the popular In-the-Wild dataset, paving the way for training generalist deepfake audio detectors. AUDETER is available on GitHub.",
    "paper_abstract_zh": "语音生成系统可产生与真人难辨的逼真语音，带来严峻的真实性挑战。尽管已提出多种深度伪造检测方法，但由于训练与测试样本间因人类语音多样及合成系统快速演进导致的域偏移，其在真实环境中的可靠性仍不足。现有数据集缺乏覆盖真实与伪造类别且多样、最新的真实场景音频，无法充分应对这一挑战。为此，我们发布AUDETER（AUdio DEepfake TEst Range），一个大规模、高多样性的深度伪造音频数据集，旨在全面评估并稳健开发通用深度伪造检测模型。该数据集由11种最新TTS模型与10种声码器生成，涵盖广泛TTS/声码器模式，共4500余小时、300万条合成音频，为迄今最大规模深度伪造音频数据集。大量实验表明：i）在现有数据集上训练的SOTA方法难以泛化至新深度伪造样本，且在未见真人语音上误报率高，凸显全面数据集的必要性；ii）在AUDETER上训练后，这些方法获得高度泛化的检测性能，检测错误率降低44.1%–51.6%，在流行的In-the-Wild跨域多样样本上错误率仅4.17%，为训练通用深度伪造音频检测器铺平道路。AUDETER已开源至GitHub。",
    "primary_category": "cs.SD",
    "update_time": "2025-09-04",
    "paper_authors": "Qizhou Wang, Hanxun Huang, Guansong Pang, Sarah Erfani, Christopher Leckie",
    "topic": [
      "Speech Synthesis",
      "Multimodal Generation"
    ],
    "category": [
      "Speech",
      "Image"
    ]
  },
  {
    "paper_title": "Open-Source Full-Duplex Conversational Datasets for Natural and Interactive Speech Synthesis",
    "paper_title_zh": "开源全双工对话数据集：自然交互式语音合成",
    "paper_id": "2509.04093v1",
    "paper_abstract": "Full-duplex, spontaneous conversational data are essential for enhancing the naturalness and interactivity of synthesized speech in conversational TTS systems. We present two open-source dual-track conversational speech datasets, one in Chinese and one in English, designed to enhance the naturalness of synthesized speech by providing more realistic conversational data. The two datasets contain a total of 15 hours of natural, spontaneous conversations recorded in isolated rooms, which produces separate high-quality audio tracks for each speaker. The conversations cover diverse daily topics and domains, capturing realistic interaction patterns including frequent overlaps, backchannel responses, laughter, and other non-verbal vocalizations. We introduce the data collection procedure, transcription and annotation methods. We demonstrate the utility of these corpora by fine-tuning a baseline TTS model with the proposed datasets. The fine-tuned TTS model achieves higher subjective and objective evaluation metrics compared to the baseline, indicating improved naturalness and conversational realism in synthetic speech. All data, annotations, and supporting code for fine-tuning and evaluation are made available to facilitate further research in conversational speech synthesis.",
    "paper_abstract_zh": "全双工、自发的对话数据对提升对话TTS系统合成语音的自然度与交互性至关重要。我们发布两个开源双轨对话语音数据集（中英各一），通过提供更真实的对话数据增强合成语音自然度。两数据集共含15小时自然自发对话，于隔离房间录制，为每位说话人提供独立高质量音轨。对话覆盖多样日常话题与领域，捕捉真实交互模式，包括频繁重叠、反馈回应、笑声及其他非语言发声。我们介绍数据收集流程、转录与标注方法，并通过微调基线TTS模型验证语料效用。微调后的TTS模型在主观与客观指标上均优于基线，表明合成语音的自然度与对话真实感显著提升。所有数据、标注及微调与评估代码均已开源，以促进对话语音合成研究。",
    "primary_category": "cs.SD",
    "update_time": "2025-09-04",
    "paper_authors": "Zhitong Zhou, Qingqing Zhang, Lei Luo, Jiechen Liu, Ruohua Zhou",
    "topic": [
      "Speech Synthesis"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "LibriQuote: A Speech Dataset of Fictional Character Utterances for Expressive Zero-Shot Speech Synthesis",
    "paper_title_zh": "LibriQuote：用于表现力零样本语音合成的虚构角色语句语音数据集",
    "paper_id": "2509.04072v1",
    "paper_abstract": "Text-to-speech (TTS) systems have recently achieved more expressive and natural speech synthesis by scaling to large speech datasets. However, the proportion of expressive speech in such large-scale corpora is often unclear. Besides, existing expressive speech corpora are typically smaller in scale and primarily used for benchmarking TTS systems. In this paper, we introduce the LibriQuote dataset, an English corpus derived from read audiobooks, designed for both fine-tuning and benchmarking expressive zero-shot TTS system. The training dataset includes 12.7K hours of read, non-expressive speech and 5.3K hours of mostly expressive speech drawn from character quotations. Each utterance in the expressive subset is supplemented with the context in which it was written, along with pseudo-labels of speech verbs and adverbs used to describe the quotation (\\textit{e.g. ``he whispered softly''}). Additionally, we provide a challenging 7.5 hour test set intended for benchmarking TTS systems: given a neutral reference speech as input, we evaluate system's ability to synthesize an expressive utterance while preserving reference timbre. We validate qualitatively the test set by showing that it covers a wide range of emotions compared to non-expressive speech, along with various accents. Extensive subjective and objective evaluations show that fine-tuning a baseline TTS system on LibriQuote significantly improves its synthesized speech intelligibility, and that recent systems fail to synthesize speech as expressive and natural as the ground-truth utterances. The dataset and evaluation code are freely available. Audio samples can be found at https://libriquote.github.io/.",
    "paper_abstract_zh": "近期，文本到语音（TTS）系统通过扩展至大规模语音数据集实现了更具表现力与自然的合成。然而，这类语料中表现力语音的比例往往不明，且现有表现力语音数据集规模较小，主要用于系统基准测试。本文提出LibriQuote数据集，一个从有声读物中提取的英语语料，专用于表现力零样本TTS系统的微调与评测。训练集包含12700小时朗读非表现力语音及5300小时主要来自角色引语的表现力语音。每条表现力语句均附带原文语境及描述引语的动词与副词伪标签（如“他轻声低语”）。此外，我们提供一套7.5小时的挑战性测试集，用于基准评测：给定中性参考语音，评估系统在保证音色一致的前提下合成表现力语句的能力。我们通过覆盖较非表现力语音更丰富的情感与口音，定性验证了测试集的多样性。大量主观与客观评估表明，在LibriQuote上微调可显著提升基线TTS合成语音的可懂度，且现有系统仍难以达到与真实语句同等的自然表现力。数据集与评估代码已免费开放，音频样例见https://libriquote.github.io/。",
    "primary_category": "eess.AS",
    "update_time": "2025-09-04",
    "paper_authors": "Gaspard Michel, Elena V. Epure, Christophe Cerisara",
    "topic": [
      "Speech Synthesis"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "SwinSRGAN: Swin Transformer-based Generative Adversarial Network for High-Fidelity Speech Super-Resolution",
    "paper_title_zh": "SwinSRGAN：基于 Swin Transformer 的高保真语音超分辨率生成对抗网络",
    "paper_id": "2509.03913v1",
    "paper_abstract": "Speech super-resolution (SR) reconstructs high-frequency content from low-resolution speech signals. Existing systems often suffer from representation mismatch in two-stage mel-vocoder pipelines and from over-smoothing of hallucinated high-band content by CNN-only generators. Diffusion and flow models are computationally expensive, and their robustness across domains and sampling rates remains limited. We propose SwinSRGAN, an end-to-end framework operating on Modified Discrete Cosine Transform (MDCT) magnitudes. It is a Swin Transformer-based U-Net that captures long-range spectro-temporal dependencies with a hybrid adversarial scheme combines time-domain MPD/MSD discriminators with a multi-band MDCT discriminator specialized for the high-frequency band. We employs a sparse-aware regularizer on arcsinh-compressed MDCT to better preserve transient components. The system upsamples inputs at various sampling rates to 48 kHz in a single pass and operates in real time. On standard benchmarks, SwinSRGAN reduces objective error and improves ABX preference scores. In zero-shot tests on HiFi-TTS without fine-tuning, it outperforms NVSR and mdctGAN, demonstrating strong generalization across datasets",
    "paper_abstract_zh": "语音超分辨率（SR）旨在从低分辨率语音信号重建高频内容。现有系统常面临两阶段 mel-声码器管道的表征失配，以及纯 CNN 生成器对幻听高频内容的过度平滑问题。扩散与流模型计算昂贵，且跨域、跨采样率的鲁棒性有限。我们提出 SwinSRGAN，一种端到端、直接作用于修正离散余弦变换（MDCT）幅度的框架。其核心为基于 Swin Transformer 的 U-Net，可捕获长程谱-时依赖，并采用混合对抗方案：时域 MPD/MSD 判别器与专用于高频带的多频带 MDCT 判别器相结合。我们在 arcsinh 压缩的 MDCT 上施加稀疏感知正则项，以更好保留瞬态成分。系统可将任意采样率输入一次性上采样至 48 kHz，并支持实时处理。在标准基准上，SwinSRGAN 降低客观误差并提升 ABX 偏好得分；在 HiFi-TTS 零样本测试中无需微调即优于 NVSR 与 mdctGAN，展现出强大的跨数据集泛化能力。",
    "primary_category": "cs.SD",
    "update_time": "2025-09-04",
    "paper_authors": "Jiajun Yuan, Xiaochen Wang, Yuhang Xiao, Yulin Wu, Chenhao Hu, Xueyang Lv",
    "topic": [
      "Speech Synthesis",
      "Image Generation"
    ],
    "category": [
      "Speech",
      "Image"
    ]
  },
  {
    "paper_title": "FireRedTTS-2: Towards Long Conversational Speech Generation for Podcast and Chatbot",
    "paper_title_zh": "FireRedTTS-2：面向播客与聊天机器人的长对话语音生成",
    "paper_id": "2509.02020v2",
    "paper_abstract": "Current dialogue generation approaches typically require the complete dialogue text before synthesis and produce a single, inseparable speech containing all voices, making them unsuitable for interactive chat; moreover, they suffer from unstable synthesis, inaccurate speaker transitions, and incoherent prosody. In this work, we present FireRedTTS-2, a long-form streaming TTS system for multi-speaker dialogue generation, delivering stable, natural speech with reliable speaker switching and context-aware prosody. A new 12.5Hz streaming speech tokenizer accelerates training and inference, extends maximum dialogue length, encodes richer semantics to stabilize text-to-token modeling and supports high-fidelity streaming generation for real-time applications. We adopt a text-speech interleaved format, concatenating speaker-labeled text with aligned speech tokens in chronological order, and model it with a dual-transformer: a large decoder-only transformer predicts tokens at the first layer, and a smaller one completes subsequent layers. Experimental results show that FireRedTTS-2 integrates seamlessly with chat frameworks and, with minimal fine-tuning, produces emotionally expressive speech guided by implicit contextual cues. In podcast generation, it surpasses existing systems including MoonCast, Zipvoice-Dialogue, and MOSS-TTSD in objective intelligibility, speaker-turn reliability, and perceived naturalness with context-consistent prosody. Our demos are available at https://fireredteam.github.io/demos/firered_tts_2.",
    "paper_abstract_zh": "现有对话生成方法通常需先获得完整对话文本再合成，输出单一且不可拆分的多人混合语音，难以支持交互式聊天；同时存在合成不稳定、说话人切换不准、韵律不连贯等问题。本文提出 FireRedTTS-2，一种面向多说话人对话的长篇流式 TTS 系统，可稳定、自然地生成语音，具备可靠的说话人切换与上下文感知韵律。新的 12.5 Hz 流式语音分词器加快训练与推理，延长最大对话长度，编码更丰富的语义以稳定文本-令牌建模，并支持高保真流式生成以满足实时应用。我们采用文本-语音交错格式，按时间顺序拼接带说话人标签的文本与对齐语音令牌，并用双 Transformer 建模：大型仅解码器 Transformer 在第一层预测令牌，小型 Transformer 完成后续层。实验表明，FireRedTTS-2 可与聊天框架无缝集成，仅需极少微调即可在隐式上下文提示下生成富有情感的语音。在播客生成任务中，其在客观可懂度、说话人切换可靠性与感知自然度方面均优于 MoonCast、Zipvoice-Dialogue 和 MOSS-TTSD 等现有系统，且韵律与上下文保持一致。演示见 https://fireredteam.github.io/demos/firered_tts_2。",
    "primary_category": "cs.SD",
    "update_time": "2025-09-04",
    "paper_authors": "Kun Xie, Feiyu Shen, Junjie Li, Fenglong Xie, Xu Tang, Yao Hu",
    "topic": [
      "Speech Synthesis"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "MultiGen: Child-Friendly Multilingual Speech Generator with LLMs",
    "paper_title_zh": "MultiGen：基于大语言模型的儿童友好多语种语音生成器",
    "paper_id": "2508.08715v3",
    "paper_abstract": "Generative speech models have demonstrated significant potential in improving human-machine interactions, offering valuable real-world applications such as language learning for children. However, achieving high-quality, child-friendly speech generation remains challenging, particularly for low-resource languages across diverse languages and cultural contexts. In this paper, we propose MultiGen, a multilingual speech generation model with child-friendly interaction, leveraging LLM architecture for speech generation tailored for low-resource languages. We propose to integrate age-appropriate multilingual speech generation using LLM architectures, which can be used to facilitate young children's communication with AI systems through culturally relevant context in three low-resource languages: Singaporean accent Mandarin, Malay, and Tamil. Experimental results from both objective metrics and subjective evaluations demonstrate the superior performance of the proposed MultiGen compared to baseline methods.",
    "paper_abstract_zh": "生成式语音模型在改善人机交互方面展现出巨大潜力，并在儿童语言学习等实际应用中价值显著。然而，生成高质量、适合儿童的语音仍具挑战，尤其在低资源语言及多元文化背景下。本文提出 MultiGen，一种面向儿童交互的多语种语音生成模型，利用大语言模型（LLM）架构为低资源语言定制语音生成。我们整合适合年龄的多语种语音生成，使幼儿能通过文化相关语境与 AI 系统交流，覆盖新加坡口音华语、马来语和泰米尔语三种低资源语言。客观指标与主观评测均表明，所提 MultiGen 优于基线方法。",
    "primary_category": "eess.AS",
    "update_time": "2025-09-04",
    "paper_authors": "Xiaoxue Gao, Huayun Zhang, Nancy F. Chen",
    "topic": [
      "Speech Synthesis"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Test-Time Adaptation for Speech Enhancement via Domain Invariant Embedding Transformation",
    "paper_title_zh": "通过域不变嵌入变换实现语音增强的测试时自适应",
    "paper_id": "2509.04280v1",
    "paper_abstract": "Deep learning-based speech enhancement models achieve remarkable performance when test distributions match training conditions, but often degrade when deployed in unpredictable real-world environments with domain shifts. To address this challenge, we present LaDen (latent denoising), the first test-time adaptation method specifically designed for speech enhancement. Our approach leverages powerful pre-trained speech representations to perform latent denoising, approximating clean speech representations through a linear transformation of noisy embeddings. We show that this transformation generalizes well across domains, enabling effective pseudo-labeling for target domains without labeled target data. The resulting pseudo-labels enable effective test-time adaptation of speech enhancement models across diverse acoustic environments. We propose a comprehensive benchmark spanning multiple datasets with various domain shifts, including changes in noise types, speaker characteristics, and languages. Our extensive experiments demonstrate that LaDen consistently outperforms baseline methods across perceptual metrics, particularly for speaker and language domain shifts.",
    "paper_abstract_zh": "基于深度学习的语音增强模型在测试分布与训练条件一致时表现卓越，但在部署于不可预测、存在域偏移的真实环境时性能常显著下降。为此，我们提出 LaDen（潜在去噪），首个专为语音增强设计的测试时自适应方法。该方法利用强大的预训练语音表征，在潜在空间执行去噪，通过线性变换将含噪嵌入近似为干净语音表征。我们证明该变换可跨域泛化，无需目标域标注即可实现有效的伪标签生成。生成的伪标签进而支持语音增强模型在多样声学环境下的测试时自适应。我们构建了涵盖多种域偏移（噪声类型、说话人特征、语言变化）的多数据集综合基准。大量实验表明，LaDen 在感知指标上持续优于基线方法，尤其在说话人与语言域偏移场景下优势明显。",
    "primary_category": "eess.AS",
    "update_time": "2025-09-04",
    "paper_authors": "Tobias Raichle, Niels Edinger, Bin Yang",
    "topic": [
      "Speech Enhancement"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "TaleDiffusion: Multi-Character Story Generation with Dialogue Rendering",
    "paper_title_zh": "TaleDiffusion：带对话渲染的多角色故事生成",
    "paper_id": "2509.04123v1",
    "paper_abstract": "Text-to-story visualization is challenging due to the need for consistent interaction among multiple characters across frames. Existing methods struggle with character consistency, leading to artifact generation and inaccurate dialogue rendering, which results in disjointed storytelling. In response, we introduce TaleDiffusion, a novel framework for generating multi-character stories with an iterative process, maintaining character consistency, and accurate dialogue assignment via postprocessing. Given a story, we use a pre-trained LLM to generate per-frame descriptions, character details, and dialogues via in-context learning, followed by a bounded attention-based per-box mask technique to control character interactions and minimize artifacts. We then apply an identity-consistent self-attention mechanism to ensure character consistency across frames and region-aware cross-attention for precise object placement. Dialogues are also rendered as bubbles and assigned to characters via CLIPSeg. Experimental results demonstrate that TaleDiffusion outperforms existing methods in consistency, noise reduction, and dialogue rendering.",
    "paper_abstract_zh": "文本到故事的可视化因需保持多角色跨帧一致交互而极具挑战。现有方法难以维持角色一致性，易产生伪影并错误渲染对话，导致叙事不连贯。为此，我们提出 TaleDiffusion，一种迭代式多角色故事生成新框架，通过后期处理保持角色一致并准确分配对话。给定故事，先用预训练大模型通过上下文学习生成每帧描述、角色细节与对话；随后采用基于边界注意力的每框掩码技术控制角色交互、减少伪影。接着引入身份一致自注意力机制确保跨帧角色一致，并使用区域感知交叉注意力实现精准物体放置。对话通过 CLIPSeg 以气泡形式渲染并分配给对应角色。实验表明，TaleDiffusion 在一致性、降噪与对话渲染方面均优于现有方法。",
    "primary_category": "cs.CV",
    "update_time": "2025-09-04",
    "paper_authors": "Ayan Banerjee, Josep Lladós, Umapada Pal, Anjan Dutta",
    "topic": [
      "Speech Enhancement"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "From Evaluation to Optimization: Neural Speech Assessment for Downstream Applications",
    "paper_title_zh": "从评估到优化：面向下游应用的神经语音质量评估",
    "paper_id": "2509.01889v2",
    "paper_abstract": "The evaluation of synthetic and processed speech has long been a cornerstone of audio engineering and speech science. Although subjective listening tests remain the gold standard for assessing perceptual quality and intelligibility, their high cost, time requirements, and limited scalability present significant challenges in the rapid development cycles of modern speech technologies. Traditional objective metrics, while computationally efficient, often rely on a clean reference signal, making them intrusive approaches. This presents a major limitation, as clean signals are often unavailable in real-world applications. In recent years, numerous neural network-based speech assessment models have been developed to predict quality and intelligibility, achieving promising results. Beyond their role in evaluation, these models are increasingly integrated into downstream speech processing tasks. This review focuses on their role in two main areas: (1) serving as differentiable perceptual proxies that not only assess but also guide the optimization of speech enhancement and synthesis models; and (2) enabling the detection of salient speech characteristics to support more precise and efficient downstream processing. Finally, we discuss current limitations and outline future research directions to further advance the integration of speech assessment into speech processing pipelines.",
    "paper_abstract_zh": "合成与处理语音的评估一直是音频工程与语音科学的核心。尽管主观听音测试仍是衡量感知质量与可懂度的金标准，但其高成本、耗时及可扩展性不足，难以适应现代语音技术的快速迭代。传统客观指标虽计算高效，却多依赖干净参考信号，属于侵入式方法，而真实场景中干净信号往往不可得。近年来，众多基于神经网络的语音质量与可懂度预测模型取得可喜进展。除评估外，这些模型正被深度整合到下游语音处理任务中。本文综述其两大作用：(1) 作为可微感知代理，不仅评估，更直接引导语音增强与合成模型的优化；(2) 检测显著语音特征，支撑更精准高效的下游处理。最后，我们讨论现存局限并展望未来的研究方向，以进一步推动语音评估与语音处理流程的深度融合。",
    "primary_category": "eess.AS",
    "update_time": "2025-09-04",
    "paper_authors": "Yu Tsao",
    "topic": [
      "Speech Enhancement"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "EZhouNet:A framework based on graph neural network and anchor interval for the respiratory sound event detection",
    "paper_title_zh": "EZhouNet：基于图神经网络与锚定区间的呼吸音事件检测框架",
    "paper_id": "2509.01153v2",
    "paper_abstract": "Auscultation is a key method for early diagnosis of respiratory and pulmonary diseases, relying on skilled healthcare professionals. However, the process is often subjective, with variability between experts. As a result, numerous deep learning-based automatic classification methods have emerged, most of which focus on respiratory sound classification. In contrast, research on respiratory sound event detection remains limited. Existing sound event detection methods typically rely on frame-level predictions followed by post-processing to generate event-level outputs, making interval boundaries challenging to learn directly. Furthermore, many approaches can only handle fixed-length audio, limiting their applicability to variable-length respiratory sounds. Additionally, the impact of respiratory sound location information on detection performance has not been extensively explored. To address these issues, we propose a graph neural network-based framework with anchor intervals, capable of handling variable-length audio and providing more precise temporal localization for abnormal respiratory sound events. Our method improves both the flexibility and applicability of respiratory sound detection. Experiments on the SPRSound 2024 and HF Lung V1 datasets demonstrate the effectiveness of the proposed approach, and incorporating respiratory position information enhances the discrimination between abnormal sounds. The reference implementation is available at https://github.com/chumingqian/EzhouNet.",
    "paper_abstract_zh": "听诊是早期诊断呼吸与肺部疾病的关键手段，但高度依赖医师经验且主观性强，不同专家间差异显著。为此，大量基于深度学习的自动分类方法涌现，却多聚焦于呼吸音分类，对呼吸音事件检测研究较少。现有事件检测方法通常先做帧级预测再后处理得到事件，难以直接学习区间边界，且多只能处理固定长度音频，难以应对可变长呼吸音。此外，呼吸音位置信息对检测性能的影响亦未被充分挖掘。针对上述问题，我们提出基于图神经网络并引入锚定区间的框架，可处理可变长音频，并对异常呼吸音事件实现更精准的时间定位，提升检测的灵活性与适用性。在 SPRSound 2024 与 HF Lung V1 数据集上的实验验证了方法的有效性，且引入呼吸位置信息可进一步增强异常音辨识能力。参考实现已开源：https://github.com/chumingqian/EzhouNet。",
    "primary_category": "cs.SD",
    "update_time": "2025-09-04",
    "paper_authors": "Yun Chu, Qiuhao Wang, Enze Zhou, Qian Liu, Gang Zheng",
    "topic": [
      "Audio Classification"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "PianoBind: A Multimodal Joint Embedding Model for Pop-piano Music",
    "paper_title_zh": "PianoBind：面向流行钢琴音乐的多模态联合嵌入模型",
    "paper_id": "2509.04215v1",
    "paper_abstract": "Solo piano music, despite being a single-instrument medium, possesses significant expressive capabilities, conveying rich semantic information across genres, moods, and styles. However, current general-purpose music representation models, predominantly trained on large-scale datasets, often struggle to captures subtle semantic distinctions within homogeneous solo piano music. Furthermore, existing piano-specific representation models are typically unimodal, failing to capture the inherently multimodal nature of piano music, expressed through audio, symbolic, and textual modalities. To address these limitations, we propose PianoBind, a piano-specific multimodal joint embedding model. We systematically investigate strategies for multi-source training and modality utilization within a joint embedding framework optimized for capturing fine-grained semantic distinctions in (1) small-scale and (2) homogeneous piano datasets. Our experimental results demonstrate that PianoBind learns multimodal representations that effectively capture subtle nuances of piano music, achieving superior text-to-music retrieval performance on in-domain and out-of-domain piano datasets compared to general-purpose music joint embedding models. Moreover, our design choices offer reusable insights for multimodal representation learning with homogeneous datasets beyond piano music.",
    "paper_abstract_zh": "独奏钢琴虽为单乐器媒介，却具备强大表现力，可跨流派、情绪与风格传递丰富语义。然而，当前通用音乐表示模型多基于大规模数据训练，难以捕捉同质独奏钢琴音乐中的微妙语义差异；现有钢琴专用表示模型又多为单模态，忽略了钢琴音乐在音频、符号与文本等模态上的天然多模态特性。为此，我们提出钢琴专用的多模态联合嵌入模型 PianoBind，系统研究在小规模且同质钢琴数据集下，多源训练与模态利用策略在联合嵌入框架中的优化方法。实验表明，PianoBind 学到的多模态表示能精准刻画钢琴音乐的细腻差异，在域内与域外钢琴数据集上的文本-音乐检索任务中均优于通用音乐联合嵌入模型。此外，我们的设计选择为同质数据集的多模态表示学习提供了可复用的启示，适用范围超越钢琴音乐。",
    "primary_category": "cs.SD",
    "update_time": "2025-09-04",
    "paper_authors": "Hayeon Bang, Eunjin Choi, Seungheon Doh, Juhan Nam",
    "topic": [
      "Music Information Retrieval"
    ],
    "category": [
      "Music"
    ]
  },
  {
    "paper_title": "Spatial disaggregation of time series",
    "paper_title_zh": "时间序列的空间分解",
    "paper_id": "2509.04065v1",
    "paper_abstract": "Spatiotemporal modeling of economic aggregates is increasingly relevant in regional science due to the presence of both spatial spillovers and temporal dynamics. Traditional temporal disaggregation methods, such as Chow-Lin, often ignore spatial dependence, potentially losing important regional information. We propose a novel methodology for spatiotemporal disaggregation, integrating spatial autoregressive models, benchmarking restrictions, and auxiliary covariates. The approach accommodates partially observed regional data through an anchoring mechanism, ensuring consistency with known aggregates while reducing prediction variance. We establish identifiability and asymptotic normality of the estimator under general conditions, including non-Gaussian and heteroskedastic residuals. Extensive simulations confirm the method's robustness across a wide range of spatial autocorrelations and covariate informativeness. The methodology is illustrated by disaggregating Spanish GDP into 17 autonomous communities from 2002 to 2023, using auxiliary indicators and principal component analysis for dimensionality reduction. This framework extends classical temporal disaggregation to the spatial domain, providing accurate regional estimates while accounting for spatial spillovers and irregular data availability.",
    "paper_abstract_zh": "经济总量的时空建模在区域科学中日益重要，因同时存在空间溢出与时序动态。传统时间分解方法（如 Chow-Lin）常忽略空间依赖性，可能丢失关键区域信息。本文提出一种新的时空分解方法，融合空间自回归模型、基准约束与辅助协变量，并通过锚定机制处理部分观测的区域数据，既保证与已知总量一致，又降低预测方差。我们在包含非高斯与异方差残差的一般条件下，建立了估计量的可识别性与渐近正态性。大量模拟验证了方法在不同空间自相关与协变量信息度下的稳健性。我们以 2002–2023 年西班牙 GDP 分解至 17 个自治区为例，利用辅助指标与主成分分析降维，展示框架如何将经典时间分解拓展至空间维度，在考虑空间溢出与数据不规则可得性的同时，提供准确的区域估计。",
    "primary_category": "stat.ME",
    "update_time": "2025-09-04",
    "paper_authors": "A. Tobar, A. Mir, R. Alberich, I. Garcia, M. Miró, NA. Cruz",
    "topic": [
      "Music Information Retrieval"
    ],
    "category": [
      "Music"
    ]
  },
  {
    "paper_title": "Synthesizing Sheet Music Problems for Evaluation and Reinforcement Learning",
    "paper_title_zh": "面向评估与强化学习的乐谱题目合成",
    "paper_id": "2509.04059v1",
    "paper_abstract": "Enhancing the ability of Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) to interpret sheet music is a crucial step toward building AI musicians. However, current research lacks both evaluation benchmarks and training data for sheet music reasoning. To address this, we propose the idea of synthesizing sheet music problems grounded in music theory, which can serve both as evaluation benchmarks and as training data for reinforcement learning with verifiable rewards (RLVR). We introduce a data synthesis framework that generates verifiable sheet music questions in both textual and visual modalities, leading to the Synthetic Sheet Music Reasoning Benchmark (SSMR-Bench) and a complementary training set. Evaluation results on SSMR-Bench show the importance of models' reasoning abilities in interpreting sheet music. At the same time, the poor performance of Gemini 2.5-Pro highlights the challenges that MLLMs still face in interpreting sheet music in a visual format. By leveraging synthetic data for RLVR, Qwen3-8B-Base and Qwen2.5-VL-Instruct achieve improvements on the SSMR-Bench. Besides, the trained Qwen3-8B-Base surpasses GPT-4 in overall performance on MusicTheoryBench and achieves reasoning performance comparable to GPT-4 with the strategies of Role play and Chain-of-Thought. Notably, its performance on math problems also improves relative to the original Qwen3-8B-Base. Furthermore, our results show that the enhanced reasoning ability can also facilitate music composition. In conclusion, we are the first to propose the idea of synthesizing sheet music problems based on music theory rules, and demonstrate its effectiveness not only in advancing model reasoning for sheet music understanding but also in unlocking new possibilities for AI-assisted music creation.",
    "paper_abstract_zh": "提升大语言模型（LLM）与多模态大语言模型（MLLM）解读乐谱的能力，是构建 AI 音乐家的关键一步。然而，当前研究既缺乏乐谱推理的评估基准，也缺少训练数据。为此，我们提出基于音乐理论规则合成乐谱题目的思路，既可作为评估基准，也可作为可验证奖励强化学习（RLVR）的训练数据。我们设计了一套数据合成框架，生成文本与视觉双模态的可验证乐谱题目，形成“合成乐谱推理基准”（SSMR-Bench）及配套训练集。评估结果显示，模型推理能力对乐谱理解至关重要；Gemini 2.5-Pro 的糟糕表现则凸显 MLLM 在视觉乐谱解读上的挑战。利用合成数据进行 RLVR，Qwen3-8B-Base 与 Qwen2.5-VL-Instruct 在 SSMR-Bench 上取得提升；训练后的 Qwen3-8B-Base 在 MusicTheoryBench 上整体超越 GPT-4，并通过角色扮演与思维链策略达到与 GPT-4 相当的推理水平，同时在数学题目上亦优于原版模型。此外，增强的推理能力还能促进音乐创作。综上，我们首次提出基于音乐理论规则合成乐谱题目的思路，并验证其不仅能推进模型对乐谱的理解，也为 AI 辅助音乐创作开启新可能。",
    "primary_category": "cs.CL",
    "update_time": "2025-09-04",
    "paper_authors": "Zhilin Wang, Zhe Yang, Yun Luo, Yafu Li, Haoran Zhang, Runzhe Zhan, Derek F. Wong, Jizhe Zhou, Yu Cheng",
    "topic": [
      "Music Generation"
    ],
    "category": [
      "Music"
    ]
  },
  {
    "paper_title": "AImoclips: A Benchmark for Evaluating Emotion Conveyance in Text-to-Music Generation",
    "paper_title_zh": "AImoclips：评估文本到音乐生成中情感传达的基准",
    "paper_id": "2509.00813v2",
    "paper_abstract": "Recent advances in text-to-music (TTM) generation have enabled controllable and expressive music creation using natural language prompts. However, the emotional fidelity of TTM systems remains largely underexplored compared to human preference or text alignment. In this study, we introduce AImoclips, a benchmark for evaluating how well TTM systems convey intended emotions to human listeners, covering both open-source and commercial models. We selected 12 emotion intents spanning four quadrants of the valence-arousal space, and used six state-of-the-art TTM systems to generate over 1,000 music clips. A total of 111 participants rated the perceived valence and arousal of each clip on a 9-point Likert scale. Our results show that commercial systems tend to produce music perceived as more pleasant than intended, while open-source systems tend to perform the opposite. Emotions are more accurately conveyed under high-arousal conditions across all models. Additionally, all systems exhibit a bias toward emotional neutrality, highlighting a key limitation in affective controllability. This benchmark offers valuable insights into model-specific emotion rendering characteristics and supports future development of emotionally aligned TTM systems.",
    "paper_abstract_zh": "近期文本到音乐（TTM）生成技术已可通过自然语言提示实现可控且富有表现力的音乐创作，但其情感保真度相较于人类偏好或文本对齐仍研究不足。本文提出 AImoclips 基准，系统评估 TTM 系统向听众传达目标情感的能力，涵盖开源与商业模型。我们选取了 12 种情感意图，覆盖效价-唤醒度的四个象限，并用 6 个最先进的 TTM 系统生成逾 1000 段音乐片段。111 名参与者以 9 点李克特量表对每段音乐的效价与唤醒度进行评分。结果表明：商业系统生成的音乐常被感知得比预期更愉悦，而开源系统则相反；所有模型在高唤醒条件下情感传达更准确；此外，各系统均存在偏向情感中性的偏差，暴露出情感可控性的关键局限。该基准揭示了不同模型的情感渲染特性，为构建情感对齐的 TTM 系统提供宝贵参考。",
    "primary_category": "cs.SD",
    "update_time": "2025-09-04",
    "paper_authors": "Gyehun Go, Satbyul Han, Ahyeon Choi, Eunjin Choi, Juhan Nam, Jeong Mi Park",
    "topic": [
      "Music Generation",
      "Multimodal Generation"
    ],
    "category": [
      "Music",
      "Image"
    ]
  },
  {
    "paper_title": "Auto-Regressive vs Flow-Matching: a Comparative Study of Modeling Paradigms for Text-to-Music Generation",
    "paper_title_zh": "自回归 vs 流匹配：文本到音乐生成建模范式比较研究",
    "paper_id": "2506.08570v3",
    "paper_abstract": "Recent progress in text-to-music generation has enabled models to synthesize high-quality musical segments, full compositions, and even respond to fine-grained control signals, e.g. chord progressions. State-of-the-art (SOTA) systems differ significantly in many dimensions, such as training datasets, modeling paradigms, and architectural choices. This diversity complicates efforts to evaluate models fairly and identify which design choices influence performance the most. While factors like data and architecture are important, in this study we focus exclusively on the modeling paradigm. We conduct a systematic empirical analysis to isolate its effects, offering insights into associated trade-offs and emergent behaviors that can guide future text-to-music generation systems. Specifically, we compare the two arguably most common modeling paradigms: auto-regressive decoding and conditional flow-matching. We conduct a controlled comparison by training all models from scratch using identical datasets, training configurations, and similar backbone architectures. Performance is evaluated across multiple axes, including generation quality, robustness to inference configurations, scalability, adherence to both textual and temporally aligned conditioning, and editing capabilities in the form of audio inpainting. This comparative study sheds light on distinct strengths and limitations of each paradigm, providing actionable insights that can inform future architectural and training decisions in the evolving landscape of text-to-music generation. Audio sampled examples are available at: https://huggingface.co/spaces/ortal1602/ARvsFM",
    "paper_abstract_zh": "近期文本到音乐生成进展使模型能够合成高质量音乐片段、完整作品，甚至响应和弦进行等细粒度控制信号。最先进（SOTA）系统在训练数据、建模范式与架构选择等方面差异显著，给公平评估与关键设计因素识别带来困难。本文聚焦建模范式，排除数据与架构干扰，系统比较两种主流范式——自回归解码与条件流匹配。我们在相同数据集、训练配置与相似骨干架构下从零开始训练模型，并从生成质量、推理配置鲁棒性、可扩展性、文本及时间对齐条件遵循度、音频补全编辑能力等多维度评估。该对照研究揭示两种范式各自的优劣，为未来文本到音乐系统的架构与训练决策提供可落地的见解。音频示例见：https://huggingface.co/spaces/ortal1602/ARvsFM。",
    "primary_category": "cs.SD",
    "update_time": "2025-09-04",
    "paper_authors": "Or Tal, Felix Kreuk, Yossi Adi",
    "topic": [
      "Music Generation"
    ],
    "category": [
      "Music"
    ]
  },
  {
    "paper_title": "SongBloom: Coherent Song Generation via Interleaved Autoregressive Sketching and Diffusion Refinement",
    "paper_title_zh": "SongBloom：通过交错式自回归草图与扩散精化生成连贯歌曲",
    "paper_id": "2506.07634v3",
    "paper_abstract": "Generating music with coherent structure, harmonious instrumental and vocal elements remains a significant challenge in song generation. Existing language models and diffusion-based methods often struggle to balance global coherence with local fidelity, resulting in outputs that lack musicality or suffer from incoherent progression and mismatched lyrics. This paper introduces $\\textbf{SongBloom}$, a novel framework for full-length song generation that leverages an interleaved paradigm of autoregressive sketching and diffusion-based refinement. SongBloom employs an autoregressive diffusion model that combines the high fidelity of diffusion models with the scalability of language models. Specifically, it gradually extends a musical sketch from short to long and refines the details from coarse to fine-grained. The interleaved generation paradigm effectively integrates prior semantic and acoustic context to guide the generation process. Experimental results demonstrate that SongBloom outperforms existing methods across both subjective and objective metrics and achieves performance comparable to the state-of-the-art commercial music generation platforms. Audio samples are available on our demo page: https://cypress-yang.github.io/SongBloom_demo. The code and model weights have been released on https://github.com/Cypress-Yang/SongBloom .",
    "paper_abstract_zh": "生成结构连贯、乐器与人声和谐的全曲仍是歌曲生成的一大挑战。现有语言模型与扩散方法常难以兼顾全局一致性与局部保真度，导致输出缺乏音乐性或出现结构不连贯、歌词不匹配等问题。本文提出 SongBloom，一种新颖的全曲生成框架，采用“自回归草图 + 扩散精化”的交错范式。SongBloom 将扩散模型的高保真与语言模型的可扩展性结合，通过自回归扩散模型逐步将音乐草图从短到长、从粗到细地扩展与精化。该交错生成范式有效整合语义与声学上下文，引导生成过程。实验表明，SongBloom 在主观与客观指标上均优于现有方法，并与最先进的商业音乐生成平台性能相当。音频示例见：https://cypress-yang.github.io/SongBloom_demo；代码与模型权重已开源：https://github.com/Cypress-Yang/SongBloom。",
    "primary_category": "eess.AS",
    "update_time": "2025-09-04",
    "paper_authors": "Chenyu Yang, Shuai Wang, Hangting Chen, Wei Tan, Jianwei Yu, Haizhou Li",
    "topic": [
      "Music Generation"
    ],
    "category": [
      "Music"
    ]
  },
  {
    "paper_title": "Plot'n Polish: Zero-shot Story Visualization and Disentangled Editing with Text-to-Image Diffusion Models",
    "paper_title_zh": "Plot'n Polish：利用文本到图像扩散模型实现零样本故事可视化与解耦编辑",
    "paper_id": "2509.04446v1",
    "paper_abstract": "Text-to-image diffusion models have demonstrated significant capabilities to generate diverse and detailed visuals in various domains, and story visualization is emerging as a particularly promising application. However, as their use in real-world creative domains increases, the need for providing enhanced control, refinement, and the ability to modify images post-generation in a consistent manner becomes an important challenge. Existing methods often lack the flexibility to apply fine or coarse edits while maintaining visual and narrative consistency across multiple frames, preventing creators from seamlessly crafting and refining their visual stories. To address these challenges, we introduce Plot'n Polish, a zero-shot framework that enables consistent story generation and provides fine-grained control over story visualizations at various levels of detail.",
    "paper_abstract_zh": "文本到图像扩散模型已展现出在多领域生成丰富细节视觉内容的能力，故事可视化正成为极具前景的应用场景。然而，随着其在真实创作领域的普及，如何在生成后保持一致且可精细或粗略修改的图像控制与精炼能力，成为重要挑战。现有方法常缺乏在多帧之间保持视觉与叙事一致性的灵活编辑手段，使创作者难以无缝打磨视觉故事。为此，我们提出 Plot'n Polish，一个零样本框架，可在不同细节层级上实现一致的故事生成，并对故事可视化进行细粒度控制。",
    "primary_category": "cs.CV",
    "update_time": "2025-09-04",
    "paper_authors": "Kiymet Akdemir, Jing Shi, Kushal Kafle, Brian Price, Pinar Yanardag",
    "topic": [
      "Image Generation"
    ],
    "category": [
      "Image"
    ]
  },
  {
    "paper_title": "Durian: Dual Reference-guided Portrait Animation with Attribute Transfer",
    "paper_title_zh": "Durian：双参考引导的肖像动画与属性迁移",
    "paper_id": "2509.04434v1",
    "paper_abstract": "We present Durian, the first method for generating portrait animation videos with facial attribute transfer from a given reference image to a target portrait in a zero-shot manner. To enable high-fidelity and spatially consistent attribute transfer across frames, we introduce dual reference networks that inject spatial features from both the portrait and attribute images into the denoising process of a diffusion model. We train the model using a self-reconstruction formulation, where two frames are sampled from the same portrait video: one is treated as the attribute reference and the other as the target portrait, and the remaining frames are reconstructed conditioned on these inputs and their corresponding masks. To support the transfer of attributes with varying spatial extent, we propose a mask expansion strategy using keypoint-conditioned image generation for training. In addition, we further augment the attribute and portrait images with spatial and appearance-level transformations to improve robustness to positional misalignment between them. These strategies allow the model to effectively generalize across diverse attributes and in-the-wild reference combinations, despite being trained without explicit triplet supervision. Durian achieves state-of-the-art performance on portrait animation with attribute transfer, and notably, its dual reference design enables multi-attribute composition in a single generation pass without additional training.",
    "paper_abstract_zh": "我们提出 Durian，首个零样本即可将给定参考图像的面部属性迁移到目标肖像并生成动画视频的方法。为实现高保真且跨帧空间一致的属性迁移，我们引入双参考网络，在扩散模型去噪过程中同时注入肖像与属性图像的空间特征。模型采用自重建范式训练：从同一肖像视频中采样两帧，一帧作为属性参考，另一帧作为目标肖像，其余帧在对应掩码条件下重建。为支持不同空间范围的属性迁移，我们提出基于关键点条件图像生成的掩码扩展训练策略。此外，我们对属性与肖像图像施加空间与外观级变换，提升对位置偏移的鲁棒性。尽管训练时无显式三元组监督，上述策略使模型能泛化至多样属性与野生参考组合。Durian 在肖像动画属性迁移任务上达到 SOTA，其双参考设计更可在单次生成中完成多属性组合，无需额外训练。",
    "primary_category": "cs.CV",
    "update_time": "2025-09-04",
    "paper_authors": "Hyunsoo Cha, Byungjun Kim, Hanbyul Joo",
    "topic": [
      "Image Generation"
    ],
    "category": [
      "Image"
    ]
  },
  {
    "paper_title": "Few-step Flow for 3D Generation via Marginal-Data Transport Distillation",
    "paper_title_zh": "基于边缘数据运输蒸馏的三维少步流模型",
    "paper_id": "2509.04406v1",
    "paper_abstract": "Flow-based 3D generation models typically require dozens of sampling steps during inference. Though few-step distillation methods, particularly Consistency Models (CMs), have achieved substantial advancements in accelerating 2D diffusion models, they remain under-explored for more complex 3D generation tasks. In this study, we propose a novel framework, MDT-dist, for few-step 3D flow distillation. Our approach is built upon a primary objective: distilling the pretrained model to learn the Marginal-Data Transport. Directly learning this objective needs to integrate the velocity fields, while this integral is intractable to be implemented. Therefore, we propose two optimizable objectives, Velocity Matching (VM) and Velocity Distillation (VD), to equivalently convert the optimization target from the transport level to the velocity and the distribution level respectively. Velocity Matching (VM) learns to stably match the velocity fields between the student and the teacher, but inevitably provides biased gradient estimates. Velocity Distillation (VD) further enhances the optimization process by leveraging the learned velocity fields to perform probability density distillation. When evaluated on the pioneer 3D generation framework TRELLIS, our method reduces sampling steps of each flow transformer from 25 to 1 or 2, achieving 0.68s (1 step x 2) and 0.94s (2 steps x 2) latency with 9.0x and 6.5x speedup on A800, while preserving high visual and geometric fidelity. Extensive experiments demonstrate that our method significantly outperforms existing CM distillation methods, and enables TRELLIS to achieve superior performance in few-step 3D generation.",
    "paper_abstract_zh": "基于流的 3D 生成模型推理通常需要数十步采样。尽管少步蒸馏方法——尤其是一致性模型（CM）——在加速 2D 扩散模型方面取得显著进展，但在更复杂的 3D 生成任务中仍鲜有探索。本文提出全新框架 MDT-dist，实现 3D 流的少步蒸馏。核心目标是将预训练模型蒸馏为学习“边缘数据运输”。直接优化该目标需积分速度场，而此积分难以实现。为此，我们提出两个可优化目标：速度匹配（VM）与速度蒸馏（VD），分别将优化目标从运输层面等价地转换至速度层面与分布层面。VM 稳定匹配师生速度场，但梯度估计存在偏差；VD 进一步利用已学速度场执行概率密度蒸馏，优化过程更优。在先驱 3D 生成框架 TRELLIS 上的实验表明，本方法将每个流变换器的采样步数从 25 降至 1 或 2，在 A800 上延迟仅 0.68 秒（1 步×2）与 0.94 秒（2 步×2），分别实现 9.0× 与 6.5× 加速，同时保持高视觉与几何保真度。大量实验表明，我们的方法显著优于现有 CM 蒸馏方案，使 TRELLIS 在少步 3D 生成中达到更优性能。",
    "primary_category": "cs.CV",
    "update_time": "2025-09-04",
    "paper_authors": "Zanwei Zhou, Taoran Yi, Jiemin Fang, Chen Yang, Lingxi Xie, Xinggang Wang, Wei Shen, Qi Tian",
    "topic": [
      "Image Generation",
      "3D Generation"
    ],
    "category": [
      "Image"
    ]
  },
  {
    "paper_title": "Transition Models: Rethinking the Generative Learning Objective",
    "paper_title_zh": "过渡模型：重思生成式学习目标",
    "paper_id": "2509.04394v1",
    "paper_abstract": "A fundamental dilemma in generative modeling persists: iterative diffusion models achieve outstanding fidelity, but at a significant computational cost, while efficient few-step alternatives are constrained by a hard quality ceiling. This conflict between generation steps and output quality arises from restrictive training objectives that focus exclusively on either infinitesimal dynamics (PF-ODEs) or direct endpoint prediction. We address this challenge by introducing an exact, continuous-time dynamics equation that analytically defines state transitions across any finite time interval. This leads to a novel generative paradigm, Transition Models (TiM), which adapt to arbitrary-step transitions, seamlessly traversing the generative trajectory from single leaps to fine-grained refinement with more steps. Despite having only 865M parameters, TiM achieves state-of-the-art performance, surpassing leading models such as SD3.5 (8B parameters) and FLUX.1 (12B parameters) across all evaluated step counts. Importantly, unlike previous few-step generators, TiM demonstrates monotonic quality improvement as the sampling budget increases. Additionally, when employing our native-resolution strategy, TiM delivers exceptional fidelity at resolutions up to 4096x4096.",
    "paper_abstract_zh": "生成建模的根本困境仍在：迭代扩散模型保真度极高，却计算代价巨大；高效的少步替代方案又受限于质量天花板。生成步数与输出质量之间的冲突，源于现有训练目标过于局限——要么仅关注无穷小动力学（PF-ODE），要么直接预测端点。本文提出一条精确的连续时间动力学方程，解析地定义任意有限时间间隔内的状态转移，由此诞生新的生成范式——过渡模型（TiM）。TiM 可自适应任意步数的过渡，从单步跳跃到多步精修，无缝遍历生成轨迹。仅 8.65 亿参数的 TiM 在所有采样步数下均超越 80 亿参数的 SD3.5 与 120 亿参数的 FLUX，取得 SOTA。更重要的是，与此前少步生成器不同，TiM 的生成质量随采样预算增加而单调提升。此外，结合原生分辨率策略，TiM 可在高达 4096×4096 的分辨率下呈现卓越保真度。",
    "primary_category": "cs.LG",
    "update_time": "2025-09-04",
    "paper_authors": "Zidong Wang, Yiyuan Zhang, Xiaoyu Yue, Xiangyu Yue, Yangguang Li, Wanli Ouyang, Lei Bai",
    "topic": [
      "Image Generation"
    ],
    "category": [
      "Image"
    ]
  },
  {
    "paper_title": "SSGaussian: Semantic-Aware and Structure-Preserving 3D Style Transfer",
    "paper_title_zh": "SSGaussian：语义感知且结构保持的三维风格迁移",
    "paper_id": "2509.04379v1",
    "paper_abstract": "Recent advancements in neural representations, such as Neural Radiance Fields and 3D Gaussian Splatting, have increased interest in applying style transfer to 3D scenes. While existing methods can transfer style patterns onto 3D-consistent neural representations, they struggle to effectively extract and transfer high-level style semantics from the reference style image. Additionally, the stylized results often lack structural clarity and separation, making it difficult to distinguish between different instances or objects within the 3D scene. To address these limitations, we propose a novel 3D style transfer pipeline that effectively integrates prior knowledge from pretrained 2D diffusion models. Our pipeline consists of two key stages: First, we leverage diffusion priors to generate stylized renderings of key viewpoints. Then, we transfer the stylized key views onto the 3D representation. This process incorporates two innovative designs. The first is cross-view style alignment, which inserts cross-view attention into the last upsampling block of the UNet, allowing feature interactions across multiple key views. This ensures that the diffusion model generates stylized key views that maintain both style fidelity and instance-level consistency. The second is instance-level style transfer, which effectively leverages instance-level consistency across stylized key views and transfers it onto the 3D representation. This results in a more structured, visually coherent, and artistically enriched stylization. Extensive qualitative and quantitative experiments demonstrate that our 3D style transfer pipeline significantly outperforms state-of-the-art methods across a wide range of scenes, from forward-facing to challenging 360-degree environments. Visit our project page https://jm-xu.github.io/SSGaussian for immersive visualization.",
    "paper_abstract_zh": "随着神经辐射场与 3D 高斯抛雪球等神经表示的快速发展，3D 场景风格迁移受到越来越多的关注。现有方法虽能将风格图案迁移到 3D 一致神经表示，却难以有效提取并传递参考风格图像的高层语义，且结果常缺乏结构清晰度与分离度，难以区分场景中的不同实例或物体。为此，我们提出一种新颖的 3D 风格迁移流水线，充分整合预训练 2D 扩散模型的先验知识。流水线分两阶段：首先利用扩散先验生成关键视角的风格化渲染图；随后将这些风格化关键视图迁移至 3D 表示。该过程包含两项创新设计：一是跨视角风格对齐，在 UNet 最后上采样块插入跨视角注意力，实现多关键视图间的特征交互，确保扩散模型生成的风格化关键视图既保持风格保真又具备实例级一致性；二是实例级风格迁移，有效利用风格化关键视图间的实例一致性，并将其迁移至 3D 表示，得到结构更清晰、视觉更连贯且艺术表现力更强的风格化结果。大量定性与定量实验表明，我们的 3D 风格迁移流水线在从正视到极具挑战的 360° 环境等广泛场景中均显著优于 SOTA 方法。项目页面：https://jm-xu.github.io/SSGaussian，可沉浸式查看效果。",
    "primary_category": "cs.CV",
    "update_time": "2025-09-04",
    "paper_authors": "Jimin Xu, Bosheng Qin, Tao Jin, Zhou Zhao, Zhenhui Ye, Jun Yu, Fei Wu",
    "topic": [
      "Image Generation",
      "3D Generation"
    ],
    "category": [
      "Image"
    ]
  },
  {
    "paper_title": "Connections between reinforcement learning with feedback,test-time scaling, and diffusion guidance: An anthology",
    "paper_title_zh": "反馈强化学习、测试时扩展与扩散引导的内在联系：一篇文集",
    "paper_id": "2509.04372v1",
    "paper_abstract": "In this note, we reflect on several fundamental connections among widely used post-training techniques. We clarify some intimate connections and equivalences between reinforcement learning with human feedback, reinforcement learning with internal feedback, and test-time scaling (particularly soft best-of-$N$ sampling), while also illuminating intrinsic links between diffusion guidance and test-time scaling. Additionally, we introduce a resampling approach for alignment and reward-directed diffusion models, sidestepping the need for explicit reinforcement learning techniques.",
    "paper_abstract_zh": "本文反思了若干广泛使用的后训练技术之间的基本联系。我们阐明了人类反馈强化学习、内部反馈强化学习与测试时扩展（特别是软 best-of-N 采样）之间的紧密关联与等价性，同时揭示了扩散引导与测试时扩展的内在联系。此外，我们提出一种用于对齐与奖励导向扩散模型的重采样方法，无需显式强化学习技术即可实现目标。",
    "primary_category": "stat.ML",
    "update_time": "2025-09-04",
    "paper_authors": "Yuchen Jiao, Yuxin Chen, Gen Li",
    "topic": [
      "Image Generation"
    ],
    "category": [
      "Image"
    ]
  },
  {
    "paper_title": "Noisy Label Refinement with Semantically Reliable Synthetic Images",
    "paper_title_zh": "利用语义可靠的合成图像进行噪声标签修正",
    "paper_id": "2509.04298v1",
    "paper_abstract": "Semantic noise in image classification datasets, where visually similar categories are frequently mislabeled, poses a significant challenge to conventional supervised learning approaches. In this paper, we explore the potential of using synthetic images generated by advanced text-to-image models to address this issue. Although these high-quality synthetic images come with reliable labels, their direct application in training is limited by domain gaps and diversity constraints. Unlike conventional approaches, we propose a novel method that leverages synthetic images as reliable reference points to identify and correct mislabeled samples in noisy datasets. Extensive experiments across multiple benchmark datasets show that our approach significantly improves classification accuracy under various noise conditions, especially in challenging scenarios with semantic label noise. Additionally, since our method is orthogonal to existing noise-robust learning techniques, when combined with state-of-the-art noise-robust training methods, it achieves superior performance, improving accuracy by 30% on CIFAR-10 and by 11% on CIFAR-100 under 70% semantic noise, and by 24% on ImageNet-100 under real-world noise conditions.",
    "paper_abstract_zh": "图像分类数据集中存在语义噪声，即视觉上相似的类别常被错误标注，这对传统监督学习构成重大挑战。本文探索利用先进文本到图像模型生成的合成图像解决该问题的潜力。尽管这些高质量合成图像自带可靠标签，但由于域差异和多样性不足，直接用于训练效果有限。与常规方法不同，我们提出一种新颖方法，将合成图像作为可靠参照点，识别并修正噪声数据集中的误标样本。在多个基准数据集上的大量实验表明，该方法在各种噪声条件下显著提升分类精度，尤其在具有挑战性语义标签噪声场景中表现突出。此外，由于该方法与现有抗噪学习技术正交，当与最新抗噪训练方法结合时，在CIFAR-10的70%语义噪声下准确率提升30%，在CIFAR-100提升11%，在ImageNet-100的真实噪声条件下提升24%。",
    "primary_category": "cs.CV",
    "update_time": "2025-09-04",
    "paper_authors": "Yingxuan Li, Jiafeng Mao, Yusuke Matsui",
    "topic": [
      "Image Generation"
    ],
    "category": [
      "Image"
    ]
  },
  {
    "paper_title": "TauGenNet: Plasma-Driven Tau PET Image Synthesis via Text-Guided 3D Diffusion Models",
    "paper_title_zh": "TauGenNet：基于血浆驱动的文本引导3D扩散模型生成Tau PET图像",
    "paper_id": "2509.04269v1",
    "paper_abstract": "Accurate quantification of tau pathology via tau positron emission tomography (PET) scan is crucial for diagnosing and monitoring Alzheimer's disease (AD). However, the high cost and limited availability of tau PET restrict its widespread use. In contrast, structural magnetic resonance imaging (MRI) and plasma-based biomarkers provide non-invasive and widely available complementary information related to brain anatomy and disease progression. In this work, we propose a text-guided 3D diffusion model for 3D tau PET image synthesis, leveraging multimodal conditions from both structural MRI and plasma measurement. Specifically, the textual prompt is from the plasma p-tau217 measurement, which is a key indicator of AD progression, while MRI provides anatomical structure constraints. The proposed framework is trained and evaluated using clinical AV1451 tau PET data from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database. Experimental results demonstrate that our approach can generate realistic, clinically meaningful 3D tau PET across a range of disease stages. The proposed framework can help perform tau PET data augmentation under different settings, provide a non-invasive, cost-effective alternative for visualizing tau pathology, and support the simulation of disease progression under varying plasma biomarker levels and cognitive conditions.",
    "paper_abstract_zh": "通过tau正电子发射断层扫描（PET）准确定量tau病理对阿尔茨海默病（AD）的诊断和监测至关重要。然而，tau PET的高成本和稀缺性限制了其广泛应用。相比之下，结构磁共振成像（MRI）和血浆生物标志物提供非侵入、易获取的脑解剖与疾病进展互补信息。本文提出一种文本引导的3D扩散模型，用于在结构MRI和血浆测量多模态条件下合成3D tau PET图像。具体而言，文本提示来自血浆p-tau217测量值——AD进展的关键指标，而MRI提供解剖结构约束。该框架使用ADNI数据库的临床AV1451 tau PET数据进行训练与评估。实验结果表明，本方法可在不同疾病阶段生成逼真且临床有意义的3D tau PET。该框架可在多种场景下实现tau PET数据增强，提供非侵入、低成本的可视化tau病理替代方案，并支持在不同血浆生物标志物水平和认知状态下模拟疾病进展。",
    "primary_category": "cs.CV",
    "update_time": "2025-09-04",
    "paper_authors": "Yuxin Gong, Se-in Jang, Wei Shao, Yi Su, Kuang Gong",
    "topic": [
      "Image Generation"
    ],
    "category": [
      "Image"
    ]
  },
  {
    "paper_title": "DUDE: Diffusion-Based Unsupervised Cross-Domain Image Retrieval",
    "paper_title_zh": "DUDE：基于扩散的无监督跨域图像检索",
    "paper_id": "2509.04193v1",
    "paper_abstract": "Unsupervised cross-domain image retrieval (UCIR) aims to retrieve images of the same category across diverse domains without relying on annotations. Existing UCIR methods, which align cross-domain features for the entire image, often struggle with the domain gap, as the object features critical for retrieval are frequently entangled with domain-specific styles. To address this challenge, we propose DUDE, a novel UCIR method building upon feature disentanglement. In brief, DUDE leverages a text-to-image generative model to disentangle object features from domain-specific styles, thus facilitating semantical image retrieval. To further achieve reliable alignment of the disentangled object features, DUDE aligns mutual neighbors from within domains to across domains in a progressive manner. Extensive experiments demonstrate that DUDE achieves state-of-the-art performance across three benchmark datasets over 13 domains. The code will be released.",
    "paper_abstract_zh": "无监督跨域图像检索（UCIR）旨在无需标注即可在不同域中检索同一类别图像。现有UCIR方法对整个图像进行跨域特征对齐，常因域差距而受阻，因为用于检索的关键目标特征常与域特定风格纠缠。为解决这一挑战，我们提出DUDE，一种基于特征解耦的新型UCIR方法。简言之，DUDE利用文本到图像生成模型将目标特征与域特定风格解耦，从而促进语义图像检索。为进一步实现解耦目标特征的可靠对齐，DUDE以渐进方式将域内互近邻对齐至跨域。大量实验表明，DUDE在3个基准数据集的13个域上均取得最先进性能。代码将开源。",
    "primary_category": "cs.CV",
    "update_time": "2025-09-04",
    "paper_authors": "Ruohong Yang, Peng Hu, Yunfan Li, Xi Peng",
    "topic": [
      "Image Generation"
    ],
    "category": [
      "Image"
    ]
  },
  {
    "paper_title": "On Riordan groups involving formal semi-Laurent series and their Lie group structure",
    "paper_title_zh": "涉及形式半洛朗级数的Riordan群及其李群结构",
    "paper_id": "2509.04160v1",
    "paper_abstract": "The main goal of this paper is to introduce and to investigate properties of generalized Riordan arrays and generalized Riordan groups that involve formal semi-Laurent series. In particular, we focus on the problem of isomorphy of generalized Riordan groups with the classical Riordan groups giving the negative answer to this problem. We also examine infinite dimensional Lie group structure of the group of generalized Riordan arrays.",
    "paper_abstract_zh": "本文主要目标是引入并研究涉及形式半洛朗级数的广义Riordan阵列及广义Riordan群的性质。特别地，我们关注广义Riordan群与经典Riordan群是否同构的问题，并给出否定答案。我们还考察了广义Riordan群阵列的无穷维李群结构。",
    "primary_category": "math.AC",
    "update_time": "2025-09-04",
    "paper_authors": "Dariusz Bugajewski, Dawid Bugajewski, Xiao-Xiong Gan, Piotr Maćkowiak",
    "topic": [
      "Image Generation"
    ],
    "category": [
      "Image"
    ]
  },
  {
    "paper_title": "Hyper Diffusion Avatars: Dynamic Human Avatar Generation using Network Weight Space Diffusion",
    "paper_title_zh": "超扩散化身：利用网络权重空间扩散生成动态人体化身",
    "paper_id": "2509.04145v1",
    "paper_abstract": "Creating human avatars is a highly desirable yet challenging task. Recent advancements in radiance field rendering have achieved unprecedented photorealism and real-time performance for personalized dynamic human avatars. However, these approaches are typically limited to person-specific rendering models trained on multi-view video data for a single individual, limiting their ability to generalize across different identities. On the other hand, generative approaches leveraging prior knowledge from pre-trained 2D diffusion models can produce cartoonish, static human avatars, which are animated through simple skeleton-based articulation. Therefore, the avatars generated by these methods suffer from lower rendering quality compared to person-specific rendering methods and fail to capture pose-dependent deformations such as cloth wrinkles. In this paper, we propose a novel approach that unites the strengths of person-specific rendering and diffusion-based generative modeling to enable dynamic human avatar generation with both high photorealism and realistic pose-dependent deformations. Our method follows a two-stage pipeline: first, we optimize a set of person-specific UNets, with each network representing a dynamic human avatar that captures intricate pose-dependent deformations. In the second stage, we train a hyper diffusion model over the optimized network weights. During inference, our method generates network weights for real-time, controllable rendering of dynamic human avatars. Using a large-scale, cross-identity, multi-view video dataset, we demonstrate that our approach outperforms state-of-the-art human avatar generation methods.",
    "paper_abstract_zh": "创建逼真的人体化身是一项极具吸引力却充满挑战的任务。近期辐射场渲染的进展在个性化动态人体化身上实现了前所未有的真实感和实时性能。然而，这些方法通常局限于针对单一个体、基于多视角视频训练的人物特定渲染模型，难以泛化到不同身份。另一方面，利用预训练2D扩散模型先验知识的生成式方法可产生卡通风格、静态的人体化身，并通过简单骨架驱动实现动画，但其渲染质量低于人物特定方法，且无法捕捉姿势相关的形变（如衣物褶皱）。本文提出一种新颖方法，融合人物特定渲染与基于扩散的生成建模优势，实现兼具高真实感与逼真姿势相关形变的动态人体化身生成。我们的方法分两阶段：首先优化一组人物特定UNet，每个网络表示一个捕捉精细姿势相关形变的动态人体化身；第二阶段在已优化网络权重上训练超扩散模型。推理时，该方法生成用于实时、可控渲染动态人体化身的网络权重。基于大规模跨身份多视角视频数据集的实验表明，本方法优于最新的人体化身生成技术。",
    "primary_category": "cs.GR",
    "update_time": "2025-09-04",
    "paper_authors": "Dongliang Cao, Guoxing Sun, Marc Habermann, Florian Bernard",
    "topic": [
      "Image Generation"
    ],
    "category": [
      "Image"
    ]
  },
  {
    "paper_title": "MEPG:Multi-Expert Planning and Generation for Compositionally-Rich Image Generation",
    "paper_title_zh": "MEPG：面向组合丰富图像生成的多专家规划与生成框架",
    "paper_id": "2509.04126v1",
    "paper_abstract": "Text-to-image diffusion models have achieved remarkable image quality, but they still struggle with complex, multiele ment prompts, and limited stylistic diversity. To address these limitations, we propose a Multi-Expert Planning and Gen eration Framework (MEPG) that synergistically integrates position- and style-aware large language models (LLMs) with spatial-semantic expert modules. The framework comprises two core components: (1) a Position-Style-Aware (PSA) module that utilizes a supervised fine-tuned LLM to decom pose input prompts into precise spatial coordinates and style encoded semantic instructions; and (2) a Multi-Expert Dif fusion (MED) module that implements cross-region genera tion through dynamic expert routing across both local regions and global areas. During the generation process for each lo cal region, specialized models (e.g., realism experts, styliza tion specialists) are selectively activated for each spatial par tition via attention-based gating mechanisms. The architec ture supports lightweight integration and replacement of ex pert models, providing strong extensibility. Additionally, an interactive interface enables real-time spatial layout editing and per-region style selection from a portfolio of experts. Ex periments show that MEPG significantly outperforms base line models with the same backbone in both image quality   and style diversity.",
    "paper_abstract_zh": "文本到图像扩散模型在图像质量上已取得显著突破，但在处理复杂多元素提示及风格多样性方面仍显不足。为此，我们提出多专家规划与生成框架（MEPG），将具备位置与风格感知能力的大语言模型（LLM）与空间-语义专家模块协同融合。该框架包含两大核心组件：（1）位置-风格感知（PSA）模块，通过监督微调后的LLM将输入提示分解为精确的空间坐标与风格编码的语义指令；（2）多专家扩散（MED）模块，通过动态专家路由在局部区域与全局范围实现跨区域生成。在每一局部区域的生成过程中，基于注意力的门控机制选择性激活专用模型（如写实专家、风格化专家等）。该架构支持专家模型的轻量级集成与替换，具备良好可扩展性。此外，交互式界面允许实时编辑空间布局并为每个区域从专家库中选择风格。实验表明，MEPG在图像质量与风格多样性上均显著优于同骨干基线模型。",
    "primary_category": "cs.CV",
    "update_time": "2025-09-04",
    "paper_authors": "Yuan Zhao, Liu Lin",
    "topic": [
      "Image Generation"
    ],
    "category": [
      "Image"
    ]
  },
  {
    "paper_title": "SMooGPT: Stylized Motion Generation using Large Language Models",
    "paper_title_zh": "SMooGPT：基于大语言模型的风格化动作生成",
    "paper_id": "2509.04058v1",
    "paper_abstract": "Stylized motion generation is actively studied in computer graphics, especially benefiting from the rapid advances in diffusion models. The goal of this task is to produce a novel motion respecting both the motion content and the desired motion style, e.g., ``walking in a loop like a Monkey''. Existing research attempts to address this problem via motion style transfer or conditional motion generation. They typically embed the motion style into a latent space and guide the motion implicitly in a latent space as well. Despite the progress, their methods suffer from low interpretability and control, limited generalization to new styles, and fail to produce motions other than ``walking'' due to the strong bias in the public stylization dataset. In this paper, we propose to solve the stylized motion generation problem from a new perspective of reasoning-composition-generation, based on our observations: i) human motion can often be effectively described using natural language in a body-part centric manner, ii) LLMs exhibit a strong ability to understand and reason about human motion, and iii) human motion has an inherently compositional nature, facilitating the new motion content or style generation via effective recomposing. We thus propose utilizing body-part text space as an intermediate representation, and present SMooGPT, a fine-tuned LLM, acting as a reasoner, composer, and generator when generating the desired stylized motion. Our method executes in the body-part text space with much higher interpretability, enabling fine-grained motion control, effectively resolving potential conflicts between motion content and style, and generalizes well to new styles thanks to the open-vocabulary ability of LLMs. Comprehensive experiments and evaluations, and a user perceptual study, demonstrate the effectiveness of our approach, especially under the pure text-driven stylized motion generation.",
    "paper_abstract_zh": "风格化动作生成是计算机图形学中的活跃研究方向，尤其受益于扩散模型的快速发展。该任务旨在生成既符合动作内容又体现指定风格的新动作，例如“像猴子一样循环行走”。现有研究主要通过动作风格迁移或条件动作生成来解决，通常将风格嵌入潜在空间并在该空间隐式引导动作。尽管取得进展，这些方法可解释性与可控性低，对新风格泛化能力有限，且因公开风格化数据集的强偏差，只能生成“行走”类动作。本文提出从“推理-组合-生成”新视角解决风格化动作生成问题，基于以下观察：i) 人体动作通常可用以身体部位为中心的自然语言有效描述；ii) 大语言模型具备理解与推理人体动作的强能力；iii) 人体动作具有天然组合性，可通过有效重组生成新内容或风格。为此，我们提出以身体部位文本空间为中间表示，并呈现SMooGPT——一种经微调的LLM，在生成目标风格化动作时充当推理器、组合器与生成器。该方法在身体部位文本空间执行，具备更高可解释性，支持细粒度动作控制，有效解决内容与风格冲突，并借助LLM的开放词汇能力对新风格良好泛化。综合实验、评估与用户感知研究验证了方法的有效性，尤其在纯文本驱动的风格化动作生成场景中表现突出。",
    "primary_category": "cs.GR",
    "update_time": "2025-09-04",
    "paper_authors": "Lei Zhong, Yi Yang, Changjian Li",
    "topic": [
      "Image Generation",
      "Video Generation"
    ],
    "category": [
      "Image"
    ]
  },
  {
    "paper_title": "CoT-Space: A Theoretical Framework for Internal Slow-Thinking via Reinforcement Learning",
    "paper_title_zh": "CoT-Space：通过强化学习实现内部慢思考的理论框架",
    "paper_id": "2509.04027v1",
    "paper_abstract": "Reinforcement Learning (RL) has become a pivotal approach for enhancing the reasoning capabilities of Large Language Models (LLMs). However, a significant theoretical gap persists, as traditional token-level RL frameworks fail to align with the reasoning-level nature of complex, multi-step thought processes like Chain-of-Thought (CoT). To address this challenge, we introduce CoT-Space, a novel theoretical framework that recasts LLM reasoning from a discrete token-prediction task to an optimization process within a continuous, reasoning-level semantic space. By analyzing this process from both a noise perspective and a risk perspective, we demonstrate that the convergence to an optimal CoT length is a natural consequence of the fundamental trade-off between underfitting and overfitting. Furthermore, extensive experiments provide strong empirical validation for our theoretical findings. Our framework not only provides a coherent explanation for empirical phenomena such as overthinking but also offers a solid theoretical foundation to guide the future development of more effective and generalizable reasoning agents.",
    "paper_abstract_zh": "强化学习（RL）已成为提升大语言模型（LLM）推理能力的关键方法。然而，传统 token 级 RL 框架与链式思维（CoT）等复杂多步推理过程在“推理级”本质上的错位，造成了显著的理论空白。为此，我们提出 CoT-Space，一种全新理论框架，将 LLM 推理从离散 token 预测任务重构为在连续、推理级语义空间中的优化过程。从噪声视角与风险视角对该过程进行分析，我们证明最优 CoT 长度的收敛是欠拟合与过拟合之间基本权衡的自然结果。大量实验为理论发现提供了强有力的实证支持。该框架不仅一致地解释了“过度思考”等经验现象，也为未来开发更有效、更可泛化的推理智能体提供了坚实的理论基础。",
    "primary_category": "cs.AI",
    "update_time": "2025-09-04",
    "paper_authors": "Zeyu Gan, Hao Yi, Yong Liu",
    "topic": [
      "Image Generation"
    ],
    "category": [
      "Image"
    ]
  },
  {
    "paper_title": "Divergence-Kernel method for linear responses and diffusion models",
    "paper_title_zh": "用于线性响应与扩散模型的散度核方法",
    "paper_id": "2509.03992v1",
    "paper_abstract": "We derive the divergence-kernel formula for the linear response (parameter-derivative of marginal or stationary distributions) of random dynamical systems, and formally pass to the continuous-time limit. Our formula works for multiplicative and parameterized noise over any period of time; it does not require hyperbolicity. Then we derive a pathwise Monte-Carlo algorithm for linear responses. With this, we propose a forward-only diffusion generative model and test on simple problems.",
    "paper_abstract_zh": "我们推导出随机动力系统线性响应（即边际或平稳分布对参数的导数）的散度核公式，并形式化地取连续时间极限。该公式适用于乘性与参数化噪声，且对任意时长有效，无需双曲性假设。随后，我们提出一种路径式蒙特卡洛算法计算线性响应，并据此设计仅需前向传播的扩散生成模型，在简单问题上进行了验证。",
    "primary_category": "math.DS",
    "update_time": "2025-09-04",
    "paper_authors": "Angxiu Ni",
    "topic": [
      "Image Generation"
    ],
    "category": [
      "Image"
    ]
  },
  {
    "paper_title": "NeuroBreak: Unveil Internal Jailbreak Mechanisms in Large Language Models",
    "paper_title_zh": "NeuroBreak：揭示大语言模型内部越狱机制",
    "paper_id": "2509.03985v1",
    "paper_abstract": "In deployment and application, large language models (LLMs) typically undergo safety alignment to prevent illegal and unethical outputs. However, the continuous advancement of jailbreak attack techniques, designed to bypass safety mechanisms with adversarial prompts, has placed increasing pressure on the security defenses of LLMs. Strengthening resistance to jailbreak attacks requires an in-depth understanding of the security mechanisms and vulnerabilities of LLMs. However, the vast number of parameters and complex structure of LLMs make analyzing security weaknesses from an internal perspective a challenging task. This paper presents NeuroBreak, a top-down jailbreak analysis system designed to analyze neuron-level safety mechanisms and mitigate vulnerabilities. We carefully design system requirements through collaboration with three experts in the field of AI security. The system provides a comprehensive analysis of various jailbreak attack methods. By incorporating layer-wise representation probing analysis, NeuroBreak offers a novel perspective on the model's decision-making process throughout its generation steps. Furthermore, the system supports the analysis of critical neurons from both semantic and functional perspectives, facilitating a deeper exploration of security mechanisms. We conduct quantitative evaluations and case studies to verify the effectiveness of our system, offering mechanistic insights for developing next-generation defense strategies against evolving jailbreak attacks.",
    "paper_abstract_zh": "在实际部署中，大语言模型（LLM）通常需经过安全对齐以防止非法与不合规输出。然而，旨在用对抗提示绕过安全机制的越狱攻击技术持续进化，给 LLM 的安全防御带来越来越大压力。增强对越狱攻击的抵抗能力，需要深入理解 LLM 的安全机制与漏洞。但 LLM 参数量巨大、结构复杂，从内部分析安全弱点极具挑战。本文提出 NeuroBreak，一种自顶向下的越狱分析系统，用于在神经元级别剖析安全机制并缓解漏洞。我们与三位 AI 安全领域专家协作，精心设计系统需求。该系统对多种越狱攻击方法进行全方位分析，通过层级表示探测分析，在模型生成步骤的决策过程中提供全新视角。此外，系统支持从语义与功能双视角分析关键神经元，助力深入探索安全机制。我们通过定量评估与案例研究验证系统有效性，为应对不断演化的越狱攻击、开发下一代防御策略提供机制性洞察。",
    "primary_category": "cs.CR",
    "update_time": "2025-09-04",
    "paper_authors": "Chuhan Zhang, Ye Zhang, Bowen Shi, Yuyou Gan, Tianyu Du, Shouling Ji, Dazhan Deng, Yingcai Wu",
    "topic": [
      "Image Generation"
    ],
    "category": [
      "Image"
    ]
  },
  {
    "paper_title": "ANTS: Shaping the Adaptive Negative Textual Space by MLLM for OOD Detection",
    "paper_title_zh": "ANTS：利用多模态大语言模型塑造自适应负文本空间以实现OOD检测",
    "paper_id": "2509.03951v1",
    "paper_abstract": "The introduction of negative labels (NLs) has proven effective in enhancing Out-of-Distribution (OOD) detection. However, existing methods often lack an understanding of OOD images, making it difficult to construct an accurate negative space. In addition, the presence of false negative labels significantly degrades their near-OOD performance. To address these issues, we propose shaping an Adaptive Negative Textual Space (ANTS) by leveraging the understanding and reasoning capabilities of multimodal large language models (MLLMs). Specifically, we identify images likely to be OOD samples as negative images and prompt the MLLM to describe these images, generating expressive negative sentences that precisely characterize the OOD distribution and enhance far-OOD detection. For the near-OOD setting, where OOD samples resemble the in-distribution (ID) subset, we first identify the subset of ID classes that are visually similar to negative images and then leverage the reasoning capability of MLLMs to generate visually similar negative labels tailored to this subset, effectively reducing false negatives and improving near-OOD detection. To balance these two types of negative textual spaces, we design an adaptive weighted score that enables the method to handle different OOD task settings (near-OOD and far-OOD) without relying on task-specific prior knowledge, making it highly adaptable in open environments. On the ImageNet benchmark, our ANTS significantly reduces the FPR95 by 4.2\\%, establishing a new state-of-the-art. Furthermore, our method is training-free and zero-shot, enabling high scalability.",
    "paper_abstract_zh": "引入负标签（NLs）已被证明能有效提升分布外（OOD）检测性能。然而，现有方法往往缺乏对OOD图像的理解，难以构建准确的负空间；此外，假负标签的存在显著降低了其在近OOD场景下的表现。为此，我们提出利用多模态大语言模型（MLLM）的理解与推理能力，塑造一个“自适应负文本空间”（ANTS）。具体而言，我们将疑似OOD图像视为负样本，引导MLLM生成具有表现力的负句，精准刻画OOD分布，从而提升远OOD检测。针对近OOD场景（OOD样本与分布内ID子集高度相似），我们先找出与负图像视觉相似的ID类别子集，再借助MLLM的推理能力为该子集生成视觉相似的负标签，有效减少假负例，提升近OOD检测。为平衡两类负文本空间，我们设计了一种自适应加权分数，使方法无需任务特定先验即可同时适应近OOD与远OOD任务，在开放环境中具备强鲁棒性。在ImageNet基准上，ANTS将FPR95降低4.2%，刷新SOTA；且方法无需训练、零样本，具备高可扩展性。",
    "primary_category": "cs.CV",
    "update_time": "2025-09-04",
    "paper_authors": "Zhu Wenjie, Zhang Yabin, Xin Jin, Wenjun Zeng, Lei Zhang",
    "topic": [
      "Image Generation"
    ],
    "category": [
      "Image"
    ]
  },
  {
    "paper_title": "Diffusion Generative Models Meet Compressed Sensing, with Applications to Image Data and Financial Time Series",
    "paper_title_zh": "扩散生成模型遇上压缩感知：在图像数据与金融时间序列中的应用",
    "paper_id": "2509.03898v1",
    "paper_abstract": "This paper develops dimension reduction techniques for accelerating diffusion model inference in the context of synthetic data generation. The idea is to integrate compressed sensing into diffusion models: (i) compress the data into a latent space, (ii) train a diffusion model in the latent space, and (iii) apply a compressed sensing algorithm to the samples generated in the latent space, facilitating the efficiency of both model training and inference. Under suitable sparsity assumptions on data, the proposed algorithm is proved to enjoy faster convergence by combining diffusion model inference with sparse recovery. As a byproduct, we obtain an optimal value for the latent space dimension. We also conduct numerical experiments on a range of datasets, including image data (handwritten digits, medical images, and climate data) and financial time series for stress testing.",
    "paper_abstract_zh": "本文提出一种降维技术，用于在合成数据生成场景下加速扩散模型推理。核心思想是将压缩感知融入扩散模型：（i）将数据压缩至潜空间；（ii）在潜空间内训练扩散模型；（iii）对潜空间生成的样本应用压缩感知重建算法，从而同时提升训练与推理效率。在数据满足适当稀疏性假设的条件下，理论证明该算法通过结合扩散推理与稀疏恢复可获得更快的收敛速度，并同时给出潜空间维度的最优取值。我们在手写数字、医学图像、气候数据等图像数据集以及金融时间序列压力测试数据集上进行了大量数值实验，验证了方法的有效性。",
    "primary_category": "stat.ML",
    "update_time": "2025-09-04",
    "paper_authors": "Zhengyi Guo, Jiatu Li, Wenpin Tang, David D. Yao",
    "topic": [
      "Image Generation"
    ],
    "category": [
      "Image"
    ]
  },
  {
    "paper_title": "Causality-guided Prompt Learning for Vision-language Models via Visual Granulation",
    "paper_title_zh": "基于视觉颗粒化的因果引导提示学习用于视觉-语言模型",
    "paper_id": "2509.03803v1",
    "paper_abstract": "Prompt learning has recently attracted much attention for adapting pre-trained vision-language models (e.g., CLIP) to downstream recognition tasks. However, most of the existing CLIP-based prompt learning methods only show a limited ability for handling fine-grained datasets. To address this issue, we propose a causality-guided text prompt learning method via visual granulation for CLIP, called CaPL, where the explored visual granulation technique could construct sets of visual granules for the text prompt to capture subtle discrepancies among different fine-grained classes through casual inference. The CaPL method contains the following two modules: (1) An attribute disentanglement module is proposed to decompose visual features into non-individualized attributes (shared by some classes) and individualized attributes (specific to single classes) using a Brownian Bridge Diffusion Model; (2) A granule learning module is proposed to construct visual granules by integrating the aforementioned attributes for recognition under two causal inference strategies. Thanks to the learned visual granules, more discriminative text prompt is expected to be learned. Extensive experimental results on 15 datasets demonstrate that our CaPL method significantly outperforms the state-of-the-art prompt learning methods, especially on fine-grained datasets.",
    "paper_abstract_zh": "提示学习近期被广泛用于将预训练视觉-语言模型（如CLIP）适配到下游识别任务。然而，现有CLIP提示学习方法在细粒度数据集上表现有限。为此，我们提出一种基于视觉颗粒化的因果引导文本提示学习方法CaPL：通过视觉颗粒化技术为文本提示构建视觉颗粒集合，并利用因果推断捕捉细粒度类别间的微妙差异。CaPL包含两大模块：（1）属性解耦模块，利用布朗桥扩散模型将视觉特征分解为非个体属性（多类共享）与个体属性（单类特有）；（2）颗粒学习模块，整合上述属性并在两种因果推断策略下构建视觉颗粒，以提升识别能力。借助所学视觉颗粒，可期望学到更具判别力的文本提示。在15个数据集上的大量实验表明，CaPL显著优于现有提示学习方法，尤其在细粒度数据集上优势更明显。",
    "primary_category": "cs.CV",
    "update_time": "2025-09-04",
    "paper_authors": "Mengyu Gao, Qiulei Dong",
    "topic": [
      "Image Generation",
      "Multimodal Generation"
    ],
    "category": [
      "Image"
    ]
  },
  {
    "paper_title": "A high-lying isomer in ^{92}Zr with lifetime modulated by the atomic charge states: a proposed approach for a nuclear gamma-ray laser",
    "paper_title_zh": "^{92}Zr 中高激发同核异构体：寿命受原子电荷态调控——迈向核γ激光的新方案",
    "paper_id": "2509.03797v1",
    "paper_abstract": "The nuclides ^{92}Zr are produced and transported by using a radioactive beam line to a lowbackground detection station. After a flight time of about 1.14 {\\mu}s, the ions are implanted into a carbon foil, and four {\\gamma} rays deexciting the 8+ state in ^{92}Zr are observed in coincidence with the implantation signals within a few nanoseconds. We conjecture that there exists an isomer located slightly above the 8^{+} state in ^{92}Zr. The isomeric lifetime in highly charged states is extended significantly due to the blocking of internal conversion decay channels, enabling its survival over the transportation. During the slowing-down process in the carbon foil, the ^{92}Zr ions capture electron and evolve toward neutral atoms, and consequently the lifetime is restored to a normal short value. Such a high-lying isomer depopulated by a low-energy transition may provide unique opportunity to develop nuclear {\\gamma} laser.",
    "paper_abstract_zh": "利用放射性束流线产生并输运^{92}Zr核素至低本底探测站。飞行约1.14 μs后，离子被注入碳膜，在几纳秒内观测到与注入信号符合的4条γ射线，它们退激^{92}Zr的8+态。我们推测在8+态之上存在一同核异构体。在高电荷态下，由于内转换衰变通道被阻塞，其寿命显著延长，得以在输运过程中存活；而在碳膜减速捕获电子并趋于中性原子的过程中，寿命恢复至正常短值。这种通过低能跃迁退激的高激发同核异构体为开发核γ激光提供了独特机遇。",
    "primary_category": "nucl-ex",
    "update_time": "2025-09-04",
    "paper_authors": "C. X. Jia, S. Guo, B. Ding, X. H. Zhou, C. X. Yuan, W. Hua J. G. Wang, S. W. Xu, C. M. Petrache, E. A. Lawrie, Y. B. Wu, Y. D. Fang, Y. H. Qiang, Y. Y. Yang, J. B. Ma, J. L. Chen, H. X. Chen, F. Fang, Y. H. Yu, B. F. Lv, F. F. Zeng, Q. B. Zeng, H. Huang, Z. H. Jia, W. Liang, W. Q. Zhang, J. H. Li, J. H. Xu, M. Y. Liu, Y. Zheng, Z. Bai, S. L. Jin, K. Wang, F. F. Duan, G. Yang, G. S. Li, M. L. Liu, Z. Liu, Z. G. Gan, M. Wang, Y. H. Zhang, Y. Q. Liang, Wei Rui, S. Q. Li, H. J. Ong, Y. Li, N. W. Huang, L. J. Liu, A. Rohilla",
    "topic": [
      "Image Generation"
    ],
    "category": [
      "Image"
    ]
  },
  {
    "paper_title": "Fitting Image Diffusion Models on Video Datasets",
    "paper_title_zh": "在视频数据集上训练图像扩散模型",
    "paper_id": "2509.03794v1",
    "paper_abstract": "Image diffusion models are trained on independently sampled static images. While this is the bedrock task protocol in generative modeling, capturing the temporal world through the lens of static snapshots is information-deficient by design. This limitation leads to slower convergence, limited distributional coverage, and reduced generalization. In this work, we propose a simple and effective training strategy that leverages the temporal inductive bias present in continuous video frames to improve diffusion training. Notably, the proposed method requires no architectural modification and can be seamlessly integrated into standard diffusion training pipelines. We evaluate our method on the HandCo dataset, where hand-object interactions exhibit dense temporal coherence and subtle variations in finger articulation often result in semantically distinct motions. Empirically, our method accelerates convergence by over 2$\\text{x}$ faster and achieves lower FID on both training and validation distributions. It also improves generative diversity by encouraging the model to capture meaningful temporal variations. We further provide an optimization analysis showing that our regularization reduces the gradient variance, which contributes to faster convergence.",
    "paper_abstract_zh": "图像扩散模型通常基于独立采样的静态图像进行训练。虽然这是生成建模的基础任务范式，但通过静态快照来捕捉时间世界在信息上天生不足。这种缺陷导致收敛速度变慢、分布覆盖受限且泛化能力下降。本文提出一种简单高效的训练策略，利用连续视频帧中固有的时间归纳偏置来提升扩散训练。值得注意的是，该方法无需修改网络结构，可无缝嵌入标准扩散训练流程。我们在 HandCo 数据集上评估，该数据集的手-物交互具有密集的时间连贯性，手指关节的细微变化常对应语义不同的动作。实验表明，本方法收敛速度提升 2 倍以上，在训练集与验证集上均获得更低 FID，同时通过鼓励模型捕捉有意义的时间变化来提升生成多样性。优化分析进一步表明，所提出的正则化降低了梯度方差，从而加速收敛。",
    "primary_category": "cs.CV",
    "update_time": "2025-09-04",
    "paper_authors": "Juhun Lee, Simon S. Woo",
    "topic": [
      "Image Generation"
    ],
    "category": [
      "Image"
    ]
  },
  {
    "paper_title": "RadioDiff-Loc: Diffusion Model Enhanced Scattering Congnition for NLoS Localization with Sparse Radio Map Estimation",
    "paper_title_zh": "RadioDiff-Loc：基于扩散模型的非视距定位散射认知与稀疏无线地图估计",
    "paper_id": "2509.01875v2",
    "paper_abstract": "Accurate localization of non-cooperative signal sources in non-line-of-sight (NLoS) environments remains a critical challenge with a wide range of applications, including autonomous navigation, industrial automation, and emergency response. In such settings, traditional positioning techniques relying on line-of-sight (LoS) or cooperative signaling fail due to severe multipath propagation and unknown transmit power. This paper proposes a novel generative inference framework for NLoS localization based on conditional diffusion models. By leveraging the physical insight that diffracted electromagnetic energy concentrates near building edges, we develop a sampling strategy that collects sparse received signal strength (RSS) measurements at the geometric vertices of obstacles--locations that maximize Fisher information and mutual information with respect to the unknown source. To overcome the lack of known transmission power, we normalize all sampled RSS values relative to the maximum observed intensity, enabling the construction of a power-invariant radio map (RM). A conditional diffusion model is trained to reconstruct the full RM based on environmental layout and sparse RSS observations. Localization is then achieved by identifying the brightest point on the generated RM. Moreover, the proposed framework is compatible with existing RSS-based localization algorithms, enabling a dual-driven paradigm that fuses physical knowledge and data-driven inference for improved accuracy. Extensive theoretical analysis and empirical validation demonstrate that our approach achieves high localization accuracy with significantly reduced sampling cost, offering a scalable and physically grounded solution for non-cooperative NLoS emitter localization.",
    "paper_abstract_zh": "在非视距（NLoS）环境中对非合作信号源进行精确定位仍是自动驾驶、工业自动化与应急响应等应用的核心难题。传统依赖视距（LoS）或合作信号的定位技术因严重多径与未知发射功率而失效。本文提出一种基于条件扩散模型的 NLoS 定位新框架。利用衍射电磁能量集中于建筑边缘的物理洞察，设计了一种在障碍物几何顶点处采集稀疏接收信号强度（RSS）的采样策略，这些位置可最大化关于未知源位置的费雪信息与互信息。为克服发射功率未知问题，将所有采样 RSS 相对于最大观测强度归一化，构建功率不变的无线地图（RM）。随后训练条件扩散模型，依据环境布局与稀疏 RSS 观测重建完整 RM，并通过生成图上最亮点实现定位。该框架兼容现有 RSS 定位算法，形成物理知识与数据驱动融合的双驱动范式，进一步提升精度。大量理论分析与实验验证表明，本方法在显著降低采样成本的同时实现高精度定位，为非合作 NLoS 辐射源定位提供可扩展且具物理可解释性的解决方案。",
    "primary_category": "eess.SY",
    "update_time": "2025-09-04",
    "paper_authors": "Xiucheng Wang, Qiming Zhang, Nan Cheng",
    "topic": [
      "Image Generation"
    ],
    "category": [
      "Image"
    ]
  },
  {
    "paper_title": "Virtual Fitting Room: Generating Arbitrarily Long Videos of Virtual Try-On from a Single Image -- Technical Preview",
    "paper_title_zh": "虚拟试衣间：从单张图像生成任意长度虚拟试穿视频——技术预览",
    "paper_id": "2509.04450v1",
    "paper_abstract": "We introduce the Virtual Fitting Room (VFR), a novel video generative model that produces arbitrarily long virtual try-on videos. Our VFR models long video generation tasks as an auto-regressive, segment-by-segment generation process, eliminating the need for resource-intensive generation and lengthy video data, while providing the flexibility to generate videos of arbitrary length. The key challenges of this task are twofold: ensuring local smoothness between adjacent segments and maintaining global temporal consistency across different segments. To address these challenges, we propose our VFR framework, which ensures smoothness through a prefix video condition and enforces consistency with the anchor video -- a 360-degree video that comprehensively captures the human's wholebody appearance. Our VFR generates minute-scale virtual try-on videos with both local smoothness and global temporal consistency under various motions, making it a pioneering work in long virtual try-on video generation.",
    "paper_abstract_zh": "我们提出虚拟试衣间（VFR），一种新颖的视频生成模型，可生成任意长度的虚拟试穿视频。VFR 将长视频生成任务建模为自回归的逐段生成过程，无需资源密集的整体生成，也无需冗长视频数据，同时支持任意时长输出。该任务面临两大挑战：保证相邻片段间的局部平滑，以及维持跨片段的全局时间一致性。为此，我们提出 VFR 框架，通过前缀视频条件确保局部平滑，并利用锚点视频——一段 360° 全身外观视频——来强制全局一致性。VFR 可在多种动作下生成分钟级虚拟试穿视频，兼具局部平滑与全局时间一致性，是长虚拟试穿视频生成的开创性工作。",
    "primary_category": "cs.CV",
    "update_time": "2025-09-04",
    "paper_authors": "Jun-Kun Chen, Aayush Bansal, Minh Phuoc Vo, Yu-Xiong Wang",
    "topic": [
      "Video Generation"
    ],
    "category": [
      "Image"
    ]
  },
  {
    "paper_title": "TEn-CATS: Text-Enriched Audio-Visual Video Parsing with Multi-Scale Category-Aware Temporal Graph",
    "paper_title_zh": "TEn-CATS：基于多尺度类别感知时序图的文字增强视听视频解析",
    "paper_id": "2509.04086v1",
    "paper_abstract": "Audio-Visual Video Parsing (AVVP) task aims to identify event categories and their occurrence times in a given video with weakly supervised labels. Existing methods typically fall into two categories: (i) designing enhanced architectures based on attention mechanism for better temporal modeling, and (ii) generating richer pseudo-labels to compensate for the absence of frame-level annotations. However, the first type methods treat noisy segment-level pseudo labels as reliable supervision and the second type methods let indiscriminate attention spread them across all frames, the initial errors are repeatedly amplified during training. To address this issue, we propose a method that combines the Bi-Directional Text Fusion (BiT) module and Category-Aware Temporal Graph (CATS) module. Specifically, we integrate the strengths and complementarity of the two previous research directions. We first perform semantic injection and dynamic calibration on audio and visual modality features through the BiT module, to locate and purify cleaner and richer semantic cues. Then, we leverage the CATS module for semantic propagation and connection to enable precise semantic information dissemination across time. Experimental results demonstrate that our proposed method achieves state-of-the-art (SOTA) performance in multiple key indicators on two benchmark datasets, LLP and UnAV-100.",
    "paper_abstract_zh": "视听视频解析（AVVP）任务旨在利用弱监督标签，识别视频中事件类别及其发生时间。现有方法大致分两类：（i）基于注意力机制设计更强架构以改进时序建模；（ii）生成更丰富的伪标签以弥补帧级标注缺失。然而，前者将含噪的片段级伪标签视为可靠监督，后者让无差别注意力将错误扩散至所有帧，导致初始误差在训练中不断放大。为此，我们提出结合双向文本融合（BiT）模块与类别感知时序图（CATS）模块的方法，融合并互补两类研究优势。首先，通过 BiT 模块对音视频特征进行语义注入与动态校准，定位并净化更干净、更丰富的语义线索；随后利用 CATS 模块进行语义传播与连接，实现精确的跨时语义信息扩散。实验表明，该方法在 LLP 与 UnAV-100 两大基准数据集的多个关键指标上达到 SOTA 性能。",
    "primary_category": "cs.CV",
    "update_time": "2025-09-04",
    "paper_authors": "Yaru Chen, Faegheh Sardari, Peiliang Zhang, Ruohao Guo, Yang Xiang, Zhenbo Li, Wenwu Wang",
    "topic": [
      "Video Generation"
    ],
    "category": [
      "Image"
    ]
  },
  {
    "paper_title": "Human Motion Video Generation: A Survey",
    "paper_title_zh": "人体动作视频生成综述",
    "paper_id": "2509.03883v1",
    "paper_abstract": "Human motion video generation has garnered significant research interest due to its broad applications, enabling innovations such as photorealistic singing heads or dynamic avatars that seamlessly dance to music. However, existing surveys in this field focus on individual methods, lacking a comprehensive overview of the entire generative process. This paper addresses this gap by providing an in-depth survey of human motion video generation, encompassing over ten sub-tasks, and detailing the five key phases of the generation process: input, motion planning, motion video generation, refinement, and output. Notably, this is the first survey that discusses the potential of large language models in enhancing human motion video generation. Our survey reviews the latest developments and technological trends in human motion video generation across three primary modalities: vision, text, and audio. By covering over two hundred papers, we offer a thorough overview of the field and highlight milestone works that have driven significant technological breakthroughs. Our goal for this survey is to unveil the prospects of human motion video generation and serve as a valuable resource for advancing the comprehensive applications of digital humans. A complete list of the models examined in this survey is available in Our Repository https://github.com/Winn1y/Awesome-Human-Motion-Video-Generation.",
    "paper_abstract_zh": "人体动作视频生成因其广泛应用而备受关注，可实现逼真歌唱头像或随音乐起舞的动态化身等创新。然而，现有综述仅聚焦单一方法，缺乏对整个生成流程的全面概述。本文填补这一空白，深入综述人体动作视频生成，涵盖十余项子任务，并详述生成流程的五个关键阶段：输入、动作规划、动作视频生成、精修与输出。值得注意的是，本综述首次探讨大语言模型在提升人体动作视频生成中的潜力。我们回顾视觉、文本与音频三大模态下的最新进展与技术趋势，涵盖 200 余篇论文，全面梳理该领域并指出推动技术突破的里程碑工作。本综述旨在揭示人体动作视频生成的前景，并为数字人综合应用提供宝贵资源。所调研模型完整列表见我们的仓库 https://github.com/Winn1y/Awesome-Human-Motion-Video-Generation。",
    "primary_category": "cs.CV",
    "update_time": "2025-09-04",
    "paper_authors": "Haiwei Xue, Xiangyang Luo, Zhanghao Hu, Xin Zhang, Xunzhi Xiang, Yuqin Dai, Jianzhuang Liu, Zhensong Zhang, Minglei Li, Jian Yang, Fei Ma, Zhiyong Wu, Changpeng Yang, Zonghong Dai, Fei Richard Yu",
    "topic": [
      "Video Generation"
    ],
    "category": [
      "Image"
    ]
  },
  {
    "paper_title": "Temporal Interest-Driven Multimodal Personalized Content Generation",
    "paper_title_zh": "时序兴趣驱动的多模态个性化内容生成",
    "paper_id": "2509.04330v1",
    "paper_abstract": "With the dynamic evolution of user interests and the increasing multimodal demands in internet applications, personalized content generation strategies based on static interest preferences struggle to meet practical application requirements. The proposed TIMGen (Temporal Interest-driven Multimodal Generation) model addresses this challenge by modeling the long-term temporal evolution of users' interests and capturing dynamic interest representations with strong temporal dependencies. This model also supports the fusion of multimodal features, such as text, images, video, and audio, and delivers customized content based on multimodal preferences. TIMGen jointly learns temporal dependencies and modal preferences to obtain a unified interest representation, which it then generates to meet users' personalized content needs. TIMGen overcomes the shortcomings of personalized content recommendation methods based on static preferences, enabling flexible and dynamic modeling of users' multimodal interests, better understanding and capturing their interests and preferences. It can be extended to a variety of practical application scenarios, including e-commerce, advertising, online education, and precision medicine, providing insights for future research.",
    "paper_abstract_zh": "随着用户兴趣的动态演变以及互联网应用对多模态需求的不断增长，基于静态兴趣偏好的个性化内容生成策略已难以满足实际应用需求。本文提出的 TIMGen（时序兴趣驱动多模态生成）模型通过建模用户兴趣的长期时序演化，捕捉具有强时序依赖的动态兴趣表征。该模型支持文本、图像、视频、音频等多模态特征融合，并依据多模态偏好交付定制化内容。TIMGen 联合学习时序依赖与模态偏好，获得统一的兴趣表征，进而生成满足用户个性化内容需求的结果。TIMGen 克服了基于静态偏好的个性化内容推荐方法的不足，能够灵活、动态地建模用户的多模态兴趣，更好地理解与捕捉其兴趣偏好，可扩展至电商、广告、在线教育、精准医疗等多种实际应用场景，为后续研究提供启示。",
    "primary_category": "cs.IR",
    "update_time": "2025-09-04",
    "paper_authors": "Tian Miao",
    "topic": [
      "Multimodal Generation"
    ],
    "category": [
      "Image"
    ]
  },
  {
    "paper_title": "VisioFirm: Cross-Platform AI-assisted Annotation Tool for Computer Vision",
    "paper_title_zh": "VisioFirm：跨平台 AI 辅助计算机视觉标注工具",
    "paper_id": "2509.04180v1",
    "paper_abstract": "AI models rely on annotated data to learn pattern and perform prediction. Annotation is usually a labor-intensive step that require associating labels ranging from a simple classification label to more complex tasks such as object detection, oriented bounding box estimation, and instance segmentation. Traditional tools often require extensive manual input, limiting scalability for large datasets. To address this, we introduce VisioFirm, an open-source web application designed to streamline image labeling through AI-assisted automation. VisioFirm integrates state-of-the-art foundation models into an interface with a filtering pipeline to reduce human-in-the-loop efforts. This hybrid approach employs CLIP combined with pre-trained detectors like Ultralytics models for common classes and zero-shot models such as Grounding DINO for custom labels, generating initial annotations with low-confidence thresholding to maximize recall. Through this framework, when tested on COCO-type of classes, initial prediction have been proven to be mostly correct though the users can refine these via interactive tools supporting bounding boxes, oriented bounding boxes, and polygons. Additionally, VisioFirm has on-the-fly segmentation powered by Segment Anything accelerated through WebGPU for browser-side efficiency. The tool supports multiple export formats (YOLO, COCO, Pascal VOC, CSV) and operates offline after model caching, enhancing accessibility. VisioFirm demonstrates up to 90\\% reduction in manual effort through benchmarks on diverse datasets, while maintaining high annotation accuracy via clustering of connected CLIP-based disambiguate components and IoU-graph for redundant detection suppression. VisioFirm can be accessed from \\href{https://github.com/OschAI/VisioFirm}{https://github.com/OschAI/VisioFirm}.",
    "paper_abstract_zh": "AI 模型依赖标注数据来学习模式并进行预测。标注通常是一项劳动密集型工作，需将标签与图像关联，任务范围从简单分类到复杂的目标检测、定向边界框估计及实例分割。传统工具往往依赖大量人工输入，限制了大规模数据集的扩展性。为此，我们推出 VisioFirm——一款开源 Web 应用，通过 AI 辅助自动化简化图像标注流程。VisioFirm 将前沿基础模型集成到带过滤流程的界面中，减少人机交互工作量。该混合方案采用 CLIP 结合 Ultralytics 预训练检测器处理常见类别，并使用零样本模型 Grounding DINO 应对自定义标签，通过低置信度阈值生成初始标注以最大化召回率。在 COCO 类别上的测试表明，初始预测大多正确，用户可通过交互式工具进一步精炼边界框、定向框与多边形。此外，VisioFirm 提供基于 Segment Anything 的即时分割功能，并通过 WebGPU 在浏览器端加速。工具支持 YOLO、COCO、Pascal VOC、CSV 等多种导出格式，模型缓存后可离线运行，提升可访问性。基准测试显示，VisioFirm 在多样化数据集上可减少高达 90% 的人工工作量，同时通过基于 CLIP 的连通分量聚类与 IoU 图去重保持高标注精度。项目地址：https://github.com/OschAI/VisioFirm",
    "primary_category": "cs.CV",
    "update_time": "2025-09-04",
    "paper_authors": "Safouane El Ghazouali, Umberto Michelucci",
    "topic": [
      "Multimodal Generation"
    ],
    "category": [
      "Image"
    ]
  },
  {
    "paper_title": "SPECS: Specificity-Enhanced CLIP-Score for Long Image Caption Evaluation",
    "paper_title_zh": "SPECS：面向长图像字幕的特异性增强 CLIP 评分",
    "paper_id": "2509.03897v1",
    "paper_abstract": "As interest grows in generating long, detailed image captions, standard evaluation metrics become increasingly unreliable. N-gram-based metrics though efficient, fail to capture semantic correctness. Representational Similarity (RS) metrics, designed to address this, initially saw limited use due to high computational costs, while today, despite advances in hardware, they remain unpopular due to low correlation to human judgments. Meanwhile, metrics based on large language models (LLMs) show strong correlation with human judgments, but remain too expensive for iterative use during model development.   We introduce SPECS (Specificity-Enhanced CLIPScore), a reference-free RS metric tailored to long image captioning. SPECS modifies CLIP with a new objective that emphasizes specificity: rewarding correct details and penalizing incorrect ones. We show that SPECS matches the performance of open-source LLM-based metrics in correlation to human judgments, while being far more efficient. This makes it a practical alternative for iterative checkpoint evaluation during image captioning model development.Our code can be found at https://github.com/mbzuai-nlp/SPECS.",
    "paper_abstract_zh": "随着生成冗长、详细图像字幕的研究兴起，传统评估指标愈发不可靠。基于 n-gram 的指标虽高效，却无法捕捉语义正确性；表征相似度（RS）指标虽被提出以解决该问题，早期因计算成本高昂而受限，如今即便硬件进步，仍因与人类判断相关性低而少人问津。与此同时，基于大语言模型（LLM）的指标与人类判断高度相关，但在模型开发阶段的迭代使用中成本过高。本文提出 SPECS（特异性增强 CLIPScore），一种专为长图像字幕设计的无参考 RS 指标。SPECS 通过新目标函数改造 CLIP，强调特异性：奖励正确细节，惩罚错误细节。实验表明，SPECS 在与人类判断的相关性上媲美开源 LLM 指标，却更加高效，可作为图像字幕模型开发过程中迭代检查点评估的实用替代方案。代码地址：https://github.com/mbzuai-nlp/SPECS",
    "primary_category": "cs.CV",
    "update_time": "2025-09-04",
    "paper_authors": "Xiaofu Chen, Israfel Salazar, Yova Kementchedjhieva",
    "topic": [
      "Multimodal Generation"
    ],
    "category": [
      "Image"
    ]
  },
  {
    "paper_title": "Attn-Adapter: Attention Is All You Need for Online Few-shot Learner of Vision-Language Model",
    "paper_title_zh": "Attn-Adapter：面向视觉-语言模型在线小样本学习的注意力适配器",
    "paper_id": "2509.03895v1",
    "paper_abstract": "Contrastive vision-language models excel in zero-shot image recognition but face challenges in few-shot scenarios due to computationally intensive offline fine-tuning using prompt learning, which risks overfitting. To overcome these limitations, we propose Attn-Adapter, a novel online few-shot learning framework that enhances CLIP's adaptability via a dual attention mechanism. Our design incorporates dataset-specific information through two components: the Memory Attn-Adapter, which refines category embeddings using support examples, and the Local-Global Attn-Adapter, which enriches image embeddings by integrating local and global features. This architecture enables dynamic adaptation from a few labeled samples without retraining the base model. Attn-Adapter outperforms state-of-the-art methods in cross-category and cross-dataset generalization, maintaining efficient inference and scaling across CLIP backbones.",
    "paper_abstract_zh": "对比式视觉-语言模型在零样本图像识别中表现优异，但在小样本场景下因基于提示学习的离线微调计算量大且易过拟合而面临挑战。为此，我们提出 Attn-Adapter，一种新颖的在线小样本学习框架，通过双重注意力机制增强 CLIP 的适应性。设计包含两个组件：记忆注意力适配器利用支持样本精炼类别嵌入，局部-全局注意力适配器通过整合局部与全局特征丰富图像嵌入。该架构可在不重新训练基础模型的情况下，通过少量标注样本实现动态适应。Attn-Adapter 在跨类别与跨数据集泛化上均优于现有最佳方法，保持高效推理并可扩展至不同 CLIP 骨干网络。",
    "primary_category": "cs.CV",
    "update_time": "2025-09-04",
    "paper_authors": "Phuoc-Nguyen Bui, Khanh-Binh Nguyen, Hyunseung Choo",
    "topic": [
      "Multimodal Generation"
    ],
    "category": [
      "Image"
    ]
  },
  {
    "paper_title": "Single-shot focal plane wavefront sensing with the spatially-clipped self-coherent camera",
    "paper_title_zh": "基于空间裁剪自相干相机的单次焦平面波前传感",
    "paper_id": "2509.03870v1",
    "paper_abstract": "The Habitable Worlds Observatory requires active speckle suppression to directly image Earth-like exoplanets. Focal plane wavefront sensing and control allows us to detect, and subsequently remove, time-varying speckles through measurements of the electric field. Two measurement-based wavefront sensing approaches are pairwise probing (PWP) and the self-coherent camera (SCC). However, the PWP technique is time-consuming, requiring at least 4 images and reducing the speed at which aberrations can be eliminated. In the SCC, a coronagraph mask diffracts light outside of the Lyot stop, where it is filtered with a pinhole. The filtered light creates a reference beam, interfering with speckles that leak through the coronagraph. The classic implementation of the SCC only works over small spectral bandwidths and needs significantly oversized optics which limits its implementation. We propose a new variant, the Spatially-Clipped Self-Coherent Camera (SCSCC). The SCSCC utilizes a pinhole placed closer to the Lyot Stop, reducing the overall beam footprint and boosting the sensor resolution. A knife-edge beam splitter downstream of the Lyot Stop splits the light into two channels: fringed and unfringed. This allows us to sense the wavefront with a single exposure. Time-varying aberrations are effectively frozen in place, making them easy to remove. We present monochromatic simulation results of the SCSCC in a sensing and control loop, demonstrating a normalized intensity of ~ 4 * 10^-10 in a 5-20 lambda/D dark hole. We find that wavefront control paired with the SCSCC achieves ~ 50x deeper contrast than that achieved with PWP in a temporally evolving speckle field. Our results make the SCSCC a valuable wavefront sensor concept for the upcoming Habitable World Observatory mission.",
    "paper_abstract_zh": "宜居世界天文台需主动抑制散斑以直接成像类地系外行星。焦平面波前传感与控制技术可通过测量电场来检测并随后消除时变散斑。两种基于测量的波前传感方法为成对探测（PWP）与自相干相机（SCC）。然而，PWP 技术耗时，至少需 4 幅图像，降低了消除像差的速度。在 SCC 中，日冕仪掩模将光衍射至 Lyot 光阑外，并通过针孔滤波；滤波光形成参考光束，与泄漏散斑发生干涉。经典 SCC 仅适用于窄光谱带宽，且需显著超大光学系统，限制其实际部署。本文提出新型变体——空间裁剪自相干相机（SCSCC）。SCSCC 将针孔置于更靠近 Lyot 光阑的位置，减小整体光束足迹并提升传感器分辨率。Lyot 光阑下游的刀口分束器将光分为干涉与非干涉双通道，使我们能够单次曝光完成波前传感。时变像差被瞬间“冻结”，易于消除。我们展示了 SCSCC 在传感-控制回路中的单色仿真结果，在 5–20 λ/D 暗区实现约 4×10⁻¹⁰ 的归一化强度。结果表明，结合波前控制的 SCSCC 在时变散斑场中实现的对比度比 PWP 深约 50 倍。该结果使 SCSCC 成为即将开展的宜居世界天文台任务中极具价值的波前传感器概念。",
    "primary_category": "astro-ph.IM",
    "update_time": "2025-09-04",
    "paper_authors": "Joshua Liberman, Sebastiaan Y. Haffert, Jared R. Males, Kevin Derby, Ewan S. Douglas",
    "topic": [
      "Multimodal Generation"
    ],
    "category": [
      "Image"
    ]
  },
  {
    "paper_title": "Accelerated Interactive Auralization of Highly Reverberant Spaces using Graphics Hardware",
    "paper_title_zh": "利用图形硬件加速高混响空间的交互式可听化",
    "paper_id": "2509.04390v1",
    "paper_abstract": "Interactive acoustic auralization allows users to explore virtual acoustic environments in real-time, enabling the acoustic recreation of concert hall or Historical Worship Spaces (HWS) that are either no longer accessible, acoustically altered, or impractical to visit. Interactive acoustic synthesis requires real-time convolution of input signals with a set of synthesis filters that model the space-time acoustic response of the space. The acoustics in concert halls and HWS are both characterized by a long reverberation time, resulting in synthesis filters containing many filter taps. As a result, the convolution process can be computationally demanding, introducing significant latency that limits the real-time interactivity of the auralization system. In this paper, the implementation of a real-time multichannel loudspeaker-based auralization system is presented. This system is capable of synthesizing the acoustics of highly reverberant spaces in real-time using GPU-acceleration. A comparison between traditional CPU-based convolution and GPU-accelerated convolution is presented, showing that the latter can achieve real-time performance with significantly lower latency. Additionally, the system integrates acoustic synthesis with acoustic feedback cancellation on the GPU, creating a unified loudspeaker-based auralization framework that minimizes processing latency.",
    "paper_abstract_zh": "交互式声学可听化允许用户实时探索虚拟声环境，从而对无法进入、声学特性已改变或不便实地参观的音乐厅或历史礼拜空间（HWS）进行声学重现。交互式声学合成需要实时将输入信号与一组建模空间-时间声学响应的合成滤波器进行卷积。音乐厅与HWS 的共同特征是混响时间长，导致合成滤波器具有大量抽头，卷积过程计算量巨大，引入显著延迟，限制了可听化系统的实时交互性。本文提出一种实时多通道扬声器可听化系统实现，利用 GPU 加速可在高混响空间中实时合成声学效果。对比传统 CPU 卷积与 GPU 加速卷积，后者显著降低延迟，实现实时性能。此外，系统将声学合成与 GPU 上的声学反馈消除集成，构建统一的扬声器可听化框架，最大限度减少处理延迟。",
    "primary_category": "eess.AS",
    "update_time": "2025-09-04",
    "paper_authors": "Hannes Rosseel, Toon van Waterschoot",
    "topic": [],
    "category": [
      "Other"
    ]
  },
  {
    "paper_title": "Hierarchical Sparse Sound Field Reconstruction with Spherical and Linear Microphone Arrays",
    "paper_title_zh": "基于球形与线性麦克风阵列的层次化稀疏声场重建",
    "paper_id": "2509.03902v1",
    "paper_abstract": "Spherical microphone arrays (SMAs) are widely used for sound field analysis, and sparse recovery (SR) techniques can significantly enhance their spatial resolution by modeling the sound field as a sparse superposition of dominant plane waves. However, the spatial resolution of SMAs is fundamentally limited by their spherical harmonic order, and their performance often degrades in reverberant environments. This paper proposes a two-stage SR framework with residue refinement that integrates observations from a central SMA and four surrounding linear microphone arrays (LMAs). The core idea is to exploit complementary spatial characteristics by treating the SMA as a primary estimator and the LMAs as a spatially complementary refiner. Simulation results demonstrate that the proposed SMA-LMA method significantly enhances spatial energy map reconstruction under varying reverberation conditions, compared to both SMA-only and direct one-step joint processing. These results demonstrate the effectiveness of the proposed framework in enhancing spatial fidelity and robustness in complex acoustic environments.",
    "paper_abstract_zh": "球形麦克风阵列（SMA）广泛用于声场分析，稀疏恢复（SR）技术通过将声场建模为少量主导平面波的稀疏叠加，可显著提升其空间分辨率。然而，SMA 的空间分辨率受球谐阶数根本限制，且在混响环境中性能常下降。本文提出一种带残差修正的两阶段 SR 框架，融合中央 SMA 与四个周边线性麦克风阵列（LMA）的观测。核心思想是利用互补空间特性：以 SMA 作为主估计器，LMA 作为空间互补精修器。仿真结果表明，所提 SMA-LMA 方法在不同混响条件下，空间能量图重建效果显著优于仅使用 SMA 以及直接一步联合处理的方式，验证了该框架在复杂声环境中提升空间保真度与鲁棒性的有效性。",
    "primary_category": "eess.AS",
    "update_time": "2025-09-04",
    "paper_authors": "Shunxi Xu, Craig T. Jin",
    "topic": [],
    "category": [
      "Other"
    ]
  },
  {
    "paper_title": "Crossing the Species Divide: Transfer Learning from Speech to Animal Sounds",
    "paper_title_zh": "跨越物种鸿沟：从语音到动物声音的迁移学习",
    "paper_id": "2509.04166v1",
    "paper_abstract": "Self-supervised speech models have demonstrated impressive performance in speech processing, but their effectiveness on non-speech data remains underexplored. We study the transfer learning capabilities of such models on bioacoustic detection and classification tasks. We show that models such as HuBERT, WavLM, and XEUS can generate rich latent representations of animal sounds across taxa. We analyze the models properties with linear probing on time-averaged representations. We then extend the approach to account for the effect of time-wise information with other downstream architectures. Finally, we study the implication of frequency range and noise on performance. Notably, our results are competitive with fine-tuned bioacoustic pre-trained models and show the impact of noise-robust pre-training setups. These findings highlight the potential of speech-based self-supervised learning as an efficient framework for advancing bioacoustic research.",
    "paper_abstract_zh": "自监督语音模型在语音处理领域表现卓越，但其在非语音数据上的有效性仍待探索。我们研究了此类模型在生物声学检测与分类任务中的迁移能力。实验表明，HuBERT、WavLM 与 XEUS 等模型可为跨分类群的动物声音生成丰富的潜在表征。我们通过线性探测对时间平均表征进行模型属性分析，随后扩展方法以利用时序信息，采用其他下游架构。最后，我们探讨频率范围与噪声对性能的影响。值得注意的是，其结果可与经过微调的生物声学预训练模型竞争，并显示抗噪预训练设置的影响。这些发现凸显了基于语音的自监督学习作为推进生物声学研究的高效框架的潜力。",
    "primary_category": "cs.LG",
    "update_time": "2025-09-04",
    "paper_authors": "Jules Cauzinille, Marius Miron, Olivier Pietquin, Masato Hagiwara, Ricard Marxer, Arnaud Rey, Benoit Favre",
    "topic": [],
    "category": [
      "Other"
    ]
  },
  {
    "paper_title": "Wav2DF-TSL: Two-stage Learning with Efficient Pre-training and Hierarchical Experts Fusion for Robust Audio Deepfake Detection",
    "paper_title_zh": "Wav2DF-TSL：高效预训练与层次专家融合的两阶段学习用于鲁棒音频深度伪造检测",
    "paper_id": "2509.04161v1",
    "paper_abstract": "In recent years, self-supervised learning (SSL) models have made significant progress in audio deepfake detection (ADD) tasks. However, existing SSL models mainly rely on large-scale real speech for pre-training and lack the learning of spoofed samples, which leads to susceptibility to domain bias during the fine-tuning process of the ADD task. To this end, we propose a two-stage learning strategy (Wav2DF-TSL) based on pre-training and hierarchical expert fusion for robust audio deepfake detection. In the pre-training stage, we use adapters to efficiently learn artifacts from 3000 hours of unlabelled spoofed speech, improving the adaptability of front-end features while mitigating catastrophic forgetting. In the fine-tuning stage, we propose the hierarchical adaptive mixture of experts (HA-MoE) method to dynamically fuse multi-level spoofing cues through multi-expert collaboration with gated routing. Experimental results show that the proposed method significantly outperforms the baseline system on all four benchmark datasets, especially on the cross-domain In-the-wild dataset, achieving a 27.5% relative improvement in equal error rate (EER), outperforming the existing state-of-the-art systems. Index Terms: audio deepfake detection, self-supervised learning, parameter-efficient fine-tuning, mixture of experts",
    "paper_abstract_zh": "近年来，自监督学习（SSL）模型在音频深度伪造检测（ADD）任务上取得显著进展。然而，现有 SSL 模型主要依赖大规模真实语音进行预训练，缺乏对伪造样本的学习，导致在 ADD 微调阶段易受域偏差影响。为此，我们提出一种基于预训练与层次专家融合的两阶段学习策略（Wav2DF-TSL），用于鲁棒音频深度伪造检测。预训练阶段，利用适配器高效学习 3000 小时无标注伪造语音中的伪影，提升前端特征适应性并缓解灾难性遗忘。微调阶段，提出层次自适应混合专家（HA-MoE）方法，通过多专家协作与门控路由动态融合多层次伪造线索。实验结果表明，所提方法在四个基准数据集上均显著优于基线系统，尤其在跨域 In-the-wild 数据集上，等错误率（EER）相对降低 27.5%，超越现有最先进系统。关键词：音频深度伪造检测，自监督学习，参数高效微调，混合专家。",
    "primary_category": "cs.SD",
    "update_time": "2025-09-04",
    "paper_authors": "Yunqi Hao, Yihao Chen, Minqiang Xu, Jianbo Zhan, Liang He, Lei Fang, Sian Fang, Lin Liu",
    "topic": [],
    "category": [
      "Other"
    ]
  },
  {
    "paper_title": "VoxRole: A Comprehensive Benchmark for Evaluating Speech-Based Role-Playing Agents",
    "paper_title_zh": "VoxRole：评估基于语音的角色扮演智能体的综合基准",
    "paper_id": "2509.03940v1",
    "paper_abstract": "Recent significant advancements in Large Language Models (LLMs) have greatly propelled the development of Role-Playing Conversational Agents (RPCAs). These systems aim to create immersive user experiences through consistent persona adoption. However, current RPCA research faces dual limitations. First, existing work predominantly focuses on the textual modality, entirely overlooking critical paralinguistic features including intonation, prosody, and rhythm in speech, which are essential for conveying character emotions and shaping vivid identities. Second, the speech-based role-playing domain suffers from a long-standing lack of standardized evaluation benchmarks. Most current spoken dialogue datasets target only fundamental capability assessments, featuring thinly sketched or ill-defined character profiles. Consequently, they fail to effectively quantify model performance on core competencies like long-term persona consistency. To address this critical gap, we introduce VoxRole, the first comprehensive benchmark specifically designed for the evaluation of speech-based RPCAs. The benchmark comprises 13335 multi-turn dialogues, totaling 65.6 hours of speech from 1228 unique characters across 261 movies. To construct this resource, we propose a novel two-stage automated pipeline that first aligns movie audio with scripts and subsequently employs an LLM to systematically build multi-dimensional profiles for each character. Leveraging VoxRole, we conduct a multi-dimensional evaluation of contemporary spoken dialogue models, revealing crucial insights into their respective strengths and limitations in maintaining persona consistency.",
    "paper_abstract_zh": "近期大语言模型（LLM）的显著进展极大推动了角色扮演对话智能体（RPCA）的发展，这些系统旨在通过一致的人物设定创造沉浸式用户体验。然而，当前 RPCA 研究面临双重局限：其一，现有工作主要关注文本模态，完全忽视语音中关键的副语言特征（如语调、韵律、节奏），而这些特征对传达角色情感与塑造鲜活身份至关重要；其二，基于语音的角色扮演领域长期缺乏标准化评估基准。现有口语对话数据集大多仅针对基础能力评估，角色轮廓单薄或定义不清，无法有效量化模型在长期人格一致性等核心能力上的表现。为填补这一关键空白，我们推出 VoxRole——首个专为评估基于语音的 RPCA 设计的综合基准。该基准包含 13335 段多轮对话，总计 65.6 小时语音，涵盖 261 部电影中的 1228 个独特角色。为构建该资源，我们提出新型两阶段自动化流程：首先将电影音频与剧本对齐，随后利用 LLM 系统地为每个角色构建多维画像。借助 VoxRole，我们对当代口语对话模型进行多维评估，揭示了其在保持人格一致性方面的关键优势与局限。",
    "primary_category": "cs.CL",
    "update_time": "2025-09-04",
    "paper_authors": "Weihao Wu, Liang Cao, Xinyu Wu, Zhiwei Lin, Rui Niu, Jingbei Li, Zhiyong Wu",
    "topic": [],
    "category": [
      "Other"
    ]
  }
]