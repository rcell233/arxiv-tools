[
  {
    "paper_title": "Auditory Filter Behavior and Updated Estimated Constants",
    "paper_title_zh": "听觉滤波器行为与更新的估计常数",
    "paper_id": "2601.06094",
    "paper_abstract": "Filters from the Gammatone family are often used to model auditory signal processing, but the filter constant values used to mimic human hearing are largely set to values based on historical psychoacoustic data collected several decades ago. Here, we move away from this long-standing convention, and estimate filter constants using a range of more recent reported filter characteristics (such as quality factors and ratios between quality factors and peak group delay) within a characteristics-based framework that clarifies how filter behavior is related to the underlying constants. Using a sharp-filter approximation that captures shared peak-region behavior across certain classes of filters, we analyze the range of behaviors accessible when the full degrees of freedom of the filter are utilized rather than fixing the filter order or exponent to historically prescribed values. Filter behavior is characterized using magnitude-based and phase-based characteristics and their ratios, which reveal which characteristics are informative for constraining filter constants and which are only weakly constraining. We show that these insights and estimation methods extend to multiple realizable filter classes from the Gammatone family and apply them, together with recent physiological and psychoacoustic observations, to derive constraints on and estimates for filter constants for human auditory filters. More broadly, this framework supports the design of auditory filters with arbitrary characteristic-level specifications and enables systematic assessment of how variations in filter characteristics influence auditory models, perceptual findings, and technologies that rely on auditory filterbanks.",
    "paper_abstract_zh": "来自Gammatone族的滤波器常被用于模拟听觉信号处理，但用于模拟人类听觉的滤波器常数值主要基于几十年前收集的历史心理声学数据。本文摒弃了这一长期惯例，在一个基于特征的框架内估计滤波器常数，该框架阐明了滤波器行为如何与底层常数相关联，并使用了更近期的报告滤波器特性（如品质因数和品质因数与峰值群延迟之间的比率）范围。通过捕捉某些类别滤波器共享的峰值区域行为的锐化滤波器近似，我们分析了当利用滤波器的全部自由度而非将滤波器阶数或指数固定为历史规定值时，可实现的多种行为范围。使用基于幅度和相位的特性及其比率来表征滤波器行为，揭示了哪些特性对约束滤波器常数具有信息价值，而哪些则约束性较弱。我们展示了这些见解和估计方法可扩展到Gammatone族中多个可实现的滤波器类别，并结合近期的生理和心理声学观察，推导出人类听觉滤波器常数的约束条件和估计值。更广泛地说，该框架支持具有任意特性级别规范的听觉滤波器设计，并能够系统评估滤波器特性的变化如何影响听觉模型、感知发现和依赖听觉滤波器组的技术。",
    "subjects": [
      "Systems and Control (eess.SY)",
      "Audio and Speech Processing (eess.AS)",
      "Signal Processing (eess.SP)",
      "Sound (cs.SD)",
      "Tissues and Organs (q-bio.TO)"
    ],
    "update_time": "2026-01-13",
    "paper_authors": "Samiya A Alkhairy",
    "topic": [
      "Audio Representation Learning",
      "Other"
    ],
    "category": [
      "Other"
    ]
  },
  {
    "paper_title": "FastSLM: Hierarchical Frame Q-Former for Effective Speech Modality Adaptation",
    "paper_title_zh": "FastSLM: 用于有效语音模态适应的分层帧查询变换器",
    "paper_id": "2601.06199",
    "paper_abstract": "Recent advances in large language models (LLMs) have demonstrated human-expert-level capabilities, driving significant interest in their potential for achieving artificial general intelligence (AGI). In particular, there is growing momentum in adapting LLMs to various modalities, including vision, video, and speech, through the development of multimodal LLMs (MLLMs). However, existing speech-language model (SLM) research has largely overlooked cost-effective adaptation strategies for leveraging LLMs in the speech domain. In this paper, we propose FastSLM, a lightweight yet efficient SLM designed for effective understanding and reasoning over long-form speech. To address the challenge of aligning high-frame-rate speech features with LLMs, we introduce the Hierarchical Frame Querying Transformer (HFQ-Former), which compresses frame-level speech features while capturing both local and global context. Furthermore, we present a novel three-stage training strategy that enhances generalization across a wide range of speech-related tasks. Experimental results demonstrate that FastSLM achieves competitive performance compared to existing state-of-the-art models, despite operating with significantly lower FLOPs and parameter counts, while representing speech with only 1.67 tokens per second. The source code and model checkpoints are available at this https URL.",
    "paper_abstract_zh": "大型语言模型（LLMs）的最新进展展示了人类专家水平的能力，推动了人们对实现通用人工智能（AGI）潜力的浓厚兴趣。特别是，通过开发多模态大型语言模型（MLLMs），将LLMs适应到包括视觉、视频和语音在内的各种模态中的势头日益增长。然而，现有的语音语言模型（SLM）研究在很大程度上忽视了在语音领域利用LLMs的成本效益适应策略。在本文中，我们提出了FastSLM，这是一种轻量级且高效的SLM，专为对长篇语音进行有效理解和推理而设计。为了解决高帧率语音特征与LLMs对齐的挑战，我们引入了分层帧查询变换器（HFQ-Former），它在捕获局部和全局上下文的同时压缩了帧级语音特征。此外，我们提出了一种新颖的三阶段训练策略，增强了在广泛语音相关任务上的泛化能力。实验结果表明，尽管FastSLM的FLOPs和参数数量显著减少，并且每秒仅用1.67个token表示语音，但其性能与现有最先进的模型具有竞争力。源代码和模型检查点可在提供的https URL获取。",
    "subjects": [
      "Artificial Intelligence (cs.AI)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2026-01-13",
    "paper_authors": "Junseok Lee, Sangyong Lee, Chang-Jae Chun",
    "topic": [
      "Audio Representation Learning",
      "Speech Recognition"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Lightweight Resolution-Aware Audio Deepfake Detection via Cross-Scale Attention and Consistency Learning",
    "paper_title_zh": "基于跨尺度注意力和一致性学习的轻量级分辨率感知音频深度伪造检测",
    "paper_id": "2601.06560",
    "paper_abstract": "Audio deepfake detection has become increasingly challenging due to rapid advances in speech synthesis and voice conversion technologies, particularly under channel distortions, replay attacks, and real-world recording conditions. This paper proposes a resolution-aware audio deepfake detection framework that explicitly models and aligns multi-resolution spectral representations through cross-scale attention and consistency learning. Unlike conventional single-resolution or implicit feature-fusion approaches, the proposed method enforces agreement across complementary time--frequency scales. The proposed framework is evaluated on three representative benchmarks: ASVspoof 2019 (LA and PA), the Fake-or-Real (FoR) dataset, and the In-the-Wild Audio Deepfake dataset under a speaker-disjoint protocol. The method achieves near-perfect performance on ASVspoof LA (EER 0.16%), strong robustness on ASVspoof PA (EER 5.09%), FoR rerecorded audio (EER 4.54%), and in-the-wild deepfakes (AUC 0.98, EER 4.81%), significantly outperforming single-resolution and non-attention baselines under challenging conditions. The proposed model remains lightweight and efficient, requiring only 159k parameters and less than 1~GFLOP per inference, making it suitable for practical deployment. Comprehensive ablation studies confirm the critical contributions of cross-scale attention and consistency learning, while gradient-based interpretability analysis reveals that the model learns resolution-consistent and semantically meaningful spectral cues across diverse spoofing conditions. These results demonstrate that explicit cross-resolution modeling provides a principled, robust, and scalable foundation for next-generation audio deepfake detection systems.",
    "paper_abstract_zh": "由于语音合成和语音转换技术的快速发展，音频深度伪造检测变得越来越具有挑战性，尤其是在信道失真、重放攻击和真实录音条件下。本文提出了一种分辨率感知的音频深度伪造检测框架，通过跨尺度和一致性学习显式地对齐多分辨率频谱表示。与传统的单分辨率或隐式特征融合方法不同，所提出的方法强制互补的时间-频率尺度之间达成一致。该框架在三个代表性基准上进行了评估：ASVspoof 2019（LA和PA）、真假数据集（FoR）和野外音频深度伪造数据集，采用说话人分离协议。该方法在ASVspoof LA上达到近乎完美的性能（EER 0.16%），在ASVspoof PA上表现出强大的鲁棒性（EER 5.09%），FoR重新录制音频（EER 4.54%）和野外深度伪造（AUC 0.98，EER 4.81%）上显著优于单分辨率和非注意力基线方法。所提出的模型保持轻量级和高效，仅需159k参数和每推理小于1 GFLOP，适合实际部署。全面的消融研究证实了跨尺度注意力和一致性学习的关键贡献，而基于梯度的可解释性分析揭示，该模型在多样化的欺骗条件下学习了分辨率一致且语义上有意义的频谱线索。这些结果表明，显式的跨分辨率建模为下一代音频深度伪造检测系统提供了原则性、鲁棒性和可扩展的基础。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "update_time": "2026-01-13",
    "paper_authors": "K.A.Shahriar",
    "topic": [
      "Audio Representation Learning",
      "Audio Classification"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Stereo Audio Rendering for Personal Sound Zones Using a Binaural Spatially Adaptive Neural Network (BSANN)",
    "paper_title_zh": "使用双耳空间自适应神经网络(BSANN)实现个人声区的立体声音频渲染",
    "paper_id": "2601.06621",
    "paper_abstract": "A binaural rendering framework for personal sound zones (PSZs) is proposed to enable multiple head-tracked listeners to receive fully independent stereo audio programs. Current PSZ systems typically rely on monophonic rendering and therefore cannot control the left and right ears separately, which limits the quality and accuracy of spatial imaging. The proposed method employs a Binaural Spatially Adaptive Neural Network (BSANN) to generate ear-optimized loudspeaker filters that reconstruct the desired acoustic field at each ear of multiple listeners. The framework integrates anechoically measured loudspeaker frequency responses, analytically modeled transducer directivity, and rigid-sphere head-related transfer functions (HRTFs) to enhance acoustic accuracy and spatial rendering fidelity. An explicit active crosstalk cancellation (XTC) stage further improves three-dimensional spatial perception. Experiments show significant gains in measured objective performance metrics, including inter-zone isolation (IZI), inter-program isolation (IPI), and crosstalk cancellation (XTC), with log-frequency-weighted values of 10.23/10.03 dB (IZI), 11.11/9.16 dB (IPI), and 10.55/11.13 dB (XTC), respectively, over 100-20,000 Hz. The combined use of ear-wise control, accurate acoustic modeling, and integrated active XTC produces a unified rendering method that delivers greater isolation performance, increased robustness to room asymmetry, and more faithful spatial reproduction in real acoustic environments.",
    "paper_abstract_zh": "本文提出了一种用于个人声区(PSZs)的双耳渲染框架，使多个头部跟踪的听众能够接收完全独立的立体声音频节目。当前的PSZ系统通常依赖单声道渲染，因此无法分别控制左右耳，这限制了空间成像的质量和准确性。所提出的方法采用双耳空间自适应神经网络(BSANN)来生成耳优化的扬声器滤波器，以重建多个听众每只耳中的期望声场。该框架集成了消声室测量的扬声器频率响应、解析建模的换能器指向性以及刚性球头相关传输函数(HRTF)，以提高声学准确性和空间渲染保真度。一个明确的活动串扰消除(XTC)阶段进一步增强了三维空间感知。实验表明，在测量的客观性能指标方面取得了显著提升，包括区间隔离(IZI)、节目间隔离(IPI)和串扰消除(XTC)，在100-20,000 Hz范围内，对数频率加权值分别为10.23/10.03 dB(IZI)、11.11/9.16 dB(IPI)和10.55/11.13 dB(XTC)。耳朵级控制、精确声学建模和集成活动XTC的结合产生了一种统一的渲染方法，在真实声学环境中提供了更好的隔离性能、增强的房间不对称鲁棒性和更真实的空间再现。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "update_time": "2026-01-13",
    "paper_authors": "Hao Jiang, Edgar Choueiri",
    "topic": [
      "Audio Representation Learning",
      "Other"
    ],
    "category": [
      "Other"
    ]
  },
  {
    "paper_title": "Dereverberation Filter by Deconvolution with Frequency Bin Specific Faded Impulse Response",
    "paper_title_zh": "基于频率区间特定衰减脉冲响应的反卷积去混响滤波器",
    "paper_id": "2601.06662",
    "paper_abstract": "This work introduces a robust single-channel inverse filter for dereverberation of non-ideal recordings, validated on real audio. The developed method focuses on the calculation and modification of a discrete impulse response in order to filter the characteristics from a known digital single channel recording setup and room characteristics such as early reflections and reverberations. The aim is a dryer and clearer signal reconstruction, which ideally would be the direct-path signal. The time domain impulse response is calculated from the cepstral domain and faded by means of frequency bin specific exponential decay in the spectrum. The decay rates are obtained by using the blind estimates of reverberation time ratio between recorded output and test signals for each frequency bin. The modified impulse response does filter a recorded audio-signal by deconvolution. The blind estimation is well known and stands out for its robustness to noise and non-idealities. Estimation of a direct path signal is key to many applications.",
    "paper_abstract_zh": "这项工作介绍了一种鲁棒的单通道逆滤波器，用于非理想录音的去混响，并在真实音频上进行了验证。所开发的方法专注于计算和修改离散脉冲响应，以过滤已知数字单通道录音设置和房间特性（如早期反射和混响）的特征。目标是实现更干燥、更清晰的信号重建，理想情况下应为直接路径信号。时间域脉冲响应是从倒谱域计算得出的，并通过频谱中频率区间特定的指数衰减进行衰减。衰减率是通过使用记录输出和测试信号之间混响时间的盲估计值（针对每个频率区间）获得的。修改后的脉冲响应通过反卷积对记录的音频信号进行滤波。盲估计是众所周知的，因其对噪声和非理想条件的鲁棒性而突出。直接路径信号的估计对许多应用至关重要。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "update_time": "2026-01-13",
    "paper_authors": "Stefan Ciba",
    "topic": [
      "Speech Enhancement",
      "Other"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "TagSpeech: End-to-End Multi-Speaker ASR and Diarization with Fine-Grained Temporal Grounding",
    "paper_title_zh": "TagSpeech：基于细粒度时间绑定的端到端多说话人ASR和说话人分离",
    "paper_id": "2601.06896",
    "paper_abstract": "We present TagSpeech, a unified LLM-based framework that utilizes Temporal Anchor Grounding for joint multi-speaker ASR and diarization. The framework is built on two key designs: (1) decoupled semantic and speaker streams fine-tuned via Serialized Output Training (SOT) to learn turn-taking dynamics; and (2) an interleaved time anchor mechanism that not only supports fine-grained timestamp prediction but also acts as a synchronization signal between semantic understanding and speaker tracking. Compared to previous works that primarily focus on speaker-attributed ASR or implicit diarization, TagSpeech addresses the challenge of fine-grained speaker-content alignment and explicitly models \"who spoke what and when\" in an end-to-end manner. Experiments on AMI and AliMeeting benchmarks demonstrate that our method achieves consistent improvements in Diarization Error Rate (DER) over strong end-to-end baselines, including Qwen-Omni and Gemini, particularly in handling complex speech overlaps. Moreover, TagSpeech employs a parameter-efficient training paradigm in which the LLM backbone is frozen and only lightweight projectors are trained, resulting in strong performance with low computational cost.",
    "paper_abstract_zh": "我们提出了TagSpeech，一个基于LLM的统一框架，利用时间锚定(Temporal Anchor Grounding)进行联合多说话人自动语音识别(ASR)和说话人分离。该框架基于两个关键设计：(1)通过序列化输出训练(SOT)微调的解耦语义和说话人流，以学习轮流发言动态；(2)交错时间锚机制，不仅支持细粒度时间戳预测，还作为语义理解和说话人跟踪之间的同步信号。与之前主要关注说话人归因ASR或隐式说话人分离的工作相比，TagSpeech解决了细粒度说话人-内容对齐的挑战，并以端到端方式明确建模'谁在何时说了什么'。在AMI和AliMeeting基准测试上的实验表明，我们的方法在说话人错误率(DER)上相比强大的端到端基线(包括Qwen-Omni和Gemini)取得了持续改进，特别是在处理复杂语音重叠方面。此外，TagSpeech采用参数高效的训练范式，其中LLM主干网络被冻结，仅训练轻量级投影器，从而在低计算成本下实现强大性能。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)",
      "Computation and Language (cs.CL)"
    ],
    "update_time": "2026-01-13",
    "paper_authors": "Mingyue Huo, Yiwen Shao, Yuheng Zhang",
    "topic": [
      "Speech Recognition",
      "Audio Representation Learning"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "DIVINE: Coordinating Multimodal Disentangled Representations for Oro-Facial Neurological Disorder Assessment",
    "paper_title_zh": "DIVINE：协调多模态解缠表示用于口面神经系统疾病评估",
    "paper_id": "2601.07014",
    "paper_abstract": "In this study, we present a multimodal framework for predicting neuro-facial disorders by capturing both vocal and facial cues. We hypothesize that explicitly disentangling shared and modality-specific representations within multimodal foundation model embeddings can enhance clinical interpretability and generalization. To validate this hypothesis, we propose DIVINE a fully disentangled multimodal framework that operates on representations extracted from state-of-the-art (SOTA) audio and video foundation models, incorporating hierarchical variational bottlenecks, sparse gated fusion, and learnable symptom tokens. DIVINE operates in a multitask learning setup to jointly predict diagnostic categories (Healthy Control,ALS, Stroke) and severity levels (Mild, Moderate, Severe). The model is trained using synchronized audio and video inputs and evaluated on the Toronto NeuroFace dataset under full (audio-video) as well as single-modality (audio- only and video-only) test conditions. Our proposed approach, DIVINE achieves SOTA result, with the DeepSeek-VL2 and TRILLsson combination reaching 98.26% accuracy and 97.51% F1-score. Under modality-constrained scenarios, the framework performs well, showing strong generalization when tested with video-only or audio-only inputs. It consistently yields superior performance compared to unimodal models and baseline fusion techniques. To the best of our knowledge, DIVINE is the first framework that combines cross-modal disentanglement, adaptive fusion, and multitask learning to comprehensively assess neurological disorders using synchronized speech and facial video.",
    "paper_abstract_zh": "在这项研究中，我们提出了一个多模态框架，通过捕捉语音和面部线索来预测神经面部疾病。我们假设，在多模态基础模型嵌入中明确解缠共享和模态特定的表示可以提高临床可解释性和泛化能力。为了验证这一假设，我们提出了DIVINE，一个完全解缠的多模态框架，它基于从最先进的（SOTA）音频和视频基础模型中提取的表示，并包含分层变分瓶颈、稀疏门控融合和可学习的症状标记。DIVINE在多任务学习设置中运行，联合预测诊断类别（健康对照组、肌萎缩侧索硬化症、中风）和严重程度（轻度、中度、重度）。该模型使用同步的音频和视频输入进行训练，并在多伦多神经面部数据集上进行了全模态（音频-视频）以及单模态（仅音频和仅视频）测试条件下的评估。我们提出的方法DIVINE达到了SOTA结果，其中DeepSeek-VL2和TRILLsson的组合达到了98.26%的准确率和97.51%的F1分数。在模态受限场景下，该框架表现良好，在使用仅视频或仅音频输入进行测试时显示出强大的泛化能力。与单模态模型和基线融合技术相比，它始终表现出优越的性能。据我们所知，DIVINE是第一个结合跨模态解缠、自适应融合和多任务学习的框架，用于使用同步的语音和面部视频全面评估神经系统疾病。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2026-01-13",
    "paper_authors": "Mohd Mujtaba Akhtar, Girish, Muskaan Singh",
    "topic": [
      "Audio Representation Learning",
      "Video Generation"
    ],
    "category": [
      "Speech",
      "Image&Video"
    ]
  },
  {
    "paper_title": "Bridging Attribution and Open-Set Detection using Graph-Augmented Instance Learning in Synthetic Speech",
    "paper_title_zh": "使用图增强实例学习桥接归因与开放集检测在合成语音中",
    "paper_id": "2601.07064",
    "paper_abstract": "We propose a unified framework for not only attributing synthetic speech to its source but also for detecting speech generated by synthesizers that were not encountered during training. This requires methods that move beyond simple detection to support both detailed forensic analysis and open-set generalization. To address this, we introduce SIGNAL, a hybrid framework that combines speech foundation models (SFMs) with graph-based modeling and open-set-aware inference. Our framework integrates Graph Neural Networks (GNNs) and a k-Nearest Neighbor (KNN) classifier, allowing it to capture meaningful relationships between utterances and recognize speech that doesn`t belong to any known generator. It constructs a query-conditioned graph over generator class prototypes, enabling the GNN to reason over relationships among candidate generators, while the KNN branch supports open-set detection via confidence-based thresholding. We evaluate SIGNAL using the DiffSSD dataset, which offers a diverse mix of real speech and synthetic audio from both open-source and commercial diffusion-based TTS systems. To further assess generalization, we also test on the SingFake benchmark. Our results show that SIGNAL consistently improves performance across both tasks, with Mamba-based embeddings delivering especially strong results. To the best of our knowledge, this is the first study to unify graph-based learning and open-set detection for tracing synthetic speech back to its origin.",
    "paper_abstract_zh": "我们提出了一个统一框架，不仅可以将合成语音归因于其来源，还可以检测在训练过程中未遇到的由合成器生成的语音。这需要超越简单检测的方法，以支持详细的法证分析和开放集泛化。为此，我们引入了SIGNAL，这是一个混合框架，结合了语音基础模型（SFMs）、基于图的建模和开放集感知推理。我们的框架整合了图神经网络（GNN）和k近邻（KNN）分类器，使其能够捕获语音之间的有意义关系，并识别不属于任何已知生成器的语音。它在生成器类原型上构建了一个查询条件图，使GNN能够推理候选生成器之间的关系，同时KNN分支通过基于置信度的阈值支持开放集检测。我们使用DiffSSD数据集评估SIGNAL，该数据集提供了真实语音和开源及商业扩散式TTS系统生成的合成音频的多样化混合。为进一步评估泛化能力，我们还在SingFake基准上进行了测试。我们的结果表明，SIGNAL在两项任务中均持续提高性能，特别是基于Mamba的嵌入效果尤为显著。据我们所知，这是首个将基于图的学习与开放集检测相结合以追踪合成语音来源的研究。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2026-01-13",
    "paper_authors": "Mohd Mujtaba Akhtar, Girish, Farhan Sheth, Muskaan Singh",
    "topic": [
      "Audio Representation Learning",
      "Speech Recognition",
      "Audio Classification"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "The ICASSP 2026 Automatic Song Aesthetics Evaluation Challenge",
    "paper_title_zh": "ICASSP 2026 自动歌曲美学评估挑战",
    "paper_id": "2601.07237",
    "paper_abstract": "This paper summarizes the ICASSP 2026 Automatic Song Aesthetics Evaluation (ASAE) Challenge, which focuses on predicting the subjective aesthetic scores of AI-generated songs. The challenge consists of two tracks: Track 1 targets the prediction of the overall musicality score, while Track 2 focuses on predicting five fine-grained aesthetic scores. The challenge attracted strong interest from the research community and received numerous submissions from both academia and industry. Top-performing systems significantly surpassed the official baseline, demonstrating substantial progress in aligning objective metrics with human aesthetic preferences. The outcomes establish a standardized benchmark and advance human-aligned evaluation methodologies for modern music generation systems.",
    "paper_abstract_zh": "本文总结了ICASSP 2026自动歌曲美学评估(ASAE)挑战，该挑战专注于预测AI生成歌曲的主观美学评分。挑战包含两个赛道：赛道1针对整体音乐性评分的预测，而赛道2则专注于预测五个细粒度的美学评分。该挑战引起了研究界的强烈兴趣，并收到了来自学术界和工业界的众多投稿。表现优异的系统显著超越了官方基线，展示了客观指标与人类美学偏好对齐方面的实质性进展。这些成果为现代音乐生成系统建立了标准化基准，并推动了人类对齐评估方法的发展。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "update_time": "2026-01-13",
    "paper_authors": "Guobin Ma, Yuxuan Xia, Jixun Yao, Huixin Xue, Hexin Liu, Shuai Wang, Hao Liu, Lei Xie",
    "topic": [
      "Music Generation",
      "Music Information Retrieval"
    ],
    "category": [
      "Music"
    ]
  },
  {
    "paper_title": "Directional reflection modeling via wavenumber-domain reflection coefficient for 3D acoustic field simulation",
    "paper_title_zh": "基于波数域反射系数的三维声场模拟中的定向反射建模",
    "paper_id": "2601.07481",
    "paper_abstract": "This study proposes a framework for incorporating wavenumber-domain acoustic reflection coefficients into sound field analysis to characterize direction-dependent material reflection and scattering phenomena. The reflection coefficient is defined as the amplitude ratio between incident and reflected waves for each propagation direction and is estimated from spatial Fourier transforms of the incident and reflected sound fields. The resulting wavenumber-domain reflection coefficients are converted into an acoustic admittance representation that is directly compatible with numerical methods such as the Boundary Element Method (BEM), enabling simulation of reflections beyond simple specular components. Unlike conventional extended reaction models, the proposed approach avoids explicit modeling of the material interior. This significantly reduces computational cost while allowing direct use of measured data, empirical models, or user-defined directional reflection characteristics. The validity of the proposed formulation was previously demonstrated by the authors through two-dimensional sound field simulations, in which accurate reproduction of direction-dependent reflection behavior was confirmed. In the present work, the framework is extended to three-dimensional analysis, demonstrating its applicability to more realistic and complex acoustic environments. The proposed approach provides a practical and flexible tool for simulating direction-dependent acoustic reflections and scattering, with potential applications in architectural acoustics, material characterization, and noise control.",
    "paper_abstract_zh": "本研究提出了一种将波数域声学反射系数整合到声场分析中的框架，以表征依赖于材料的方向性反射和散射现象。反射系数被定义为每个传播方向上入射波和反射波之间的振幅比，并通过入射声场和反射声场的空间傅里叶变换进行估计。所得的波数域反射系数被转换为声导纳表示，该表示直接与边界元法(BEM)等数值方法兼容，使得能够模拟超越简单镜面反射分量的反射。与传统的扩展反应模型不同，所提出的方法避免了材料内部的显式建模。这显著降低了计算成本，同时允许直接使用测量数据、经验模型或用户定义的方向性反射特性。作者先前通过二维声场模拟验证了所提出公式的有效性，其中确认了对方向性反射行为的准确再现。在当前工作中，该框架被扩展到三维分析，展示了其在更真实和复杂声学环境中的适用性。所提出的方法为模拟依赖于方向性的声学反射和散射提供了一种实用且灵活的工具，在建筑声学、材料表征和噪声控制方面具有潜在应用。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2026-01-13",
    "paper_authors": "Satoshi Hoshika, Takahiro Iwami, Akira Omoto",
    "topic": [
      "Other"
    ],
    "category": [
      "Other"
    ]
  },
  {
    "paper_title": "AzeroS: Extending LLM to Speech with Self-Generated Instruction-Free Tuning",
    "paper_title_zh": "AzeroS: 通过自生成无指令调扩展LLM到语音领域",
    "paper_id": "2601.06086",
    "paper_abstract": "Extending large language models (LLMs) to the speech domain has recently gained significant attention. A typical approach connects a pretrained LLM with an audio encoder through a projection module and trains the resulting model on large-scale, task-specific instruction-tuning datasets. However, curating such instruction-tuning data for specific requirements is time-consuming, and models trained in this manner often generalize poorly to unseen tasks. In this work, we first formulate that the strongest generalization of a speech-LLM is achieved when it is trained with Self-Generated Instruction-Free Tuning (SIFT), in which supervision signals are generated by a frozen LLM using textual representations of speech as input. Our proposed SIFT paradigm eliminates the need for collecting task-specific question-answer pairs and yields the theoretically best generalization to unseen tasks. Building upon this paradigm, we introduce AZeroS (Auden Zero-instruction-tuned Speech-LLM), which is trained on speech-text pairs derived from publicly available corpora, including approximately 25,000 hours of speech with ASR transcripts and 3,000 hours of speech with paralinguistic labels. Built upon Qwen2.5-7B-Instruct, the model updates only two lightweight projection modules (23.8 million parameters each), while keeping both the LLM and audio encoders frozen. Despite the minimal training cost and modest data scale, AZeroS achieves state-of-the-art performance on both semantic and paralinguistic benchmarks, including VoiceBench, AIR-Bench Foundation (Speech), and AIR-Bench Chat (Speech).",
    "paper_abstract_zh": "将大型语言模型（LLMs）扩展到语音领域最近引起了广泛关注。典型方法是通过投影模块将预训练的LLM与音频编码器连接，并在大规模、任务特定的指令调优数据集上训练 resulting 模型。然而，为特定需求整理此类指令调优数据耗时较长，且以这种方式训练的模型通常难以泛化到未见任务。在这项工作中，我们首先提出，当通过自生成无指令调优（SIFT）训练时，语音LLM可实现最强的泛化能力，其中监督信号由冻结的LLM使用语音的文本表示作为输入生成。我们提出的SIFT范式消除了收集任务特定问答对的需求，并理论上实现了对未见任务的最佳泛化。基于这一范式，我们引入了AZeroS（Auden Zero-instruction-tuned Speech-LLM），该模型在从公开语料库中衍生的语音-文本对上进行训练，包括约25,000小时的带ASR转录的语音和3,000小时的带副语言标签的语音。基于Qwen2.5-7B-Instruct，该模型仅更新两个轻量级投影模块（各2380万参数），同时保持LLM和音频编码器冻结。尽管训练成本最低且数据规模适中，AZeroS在语义和副语言基准测试中均实现了最先进的性能，包括VoiceBench、AIR-Bench Foundation（Speech）和AIR-Bench Chat（Speech）。",
    "subjects": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2026-01-13",
    "paper_authors": "Yiwen Shao, Wei Liu, Jiahong Li, Tianzi Wang, Kun Wei, Meng Yu, Dong Yu",
    "topic": [
      "Audio Representation Learning",
      "Speech Recognition"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Variational decomposition autoencoding improves disentanglement of latent representations",
    "paper_title_zh": "变分分解自编码提高潜在表示的解耦性",
    "paper_id": "2601.06844",
    "paper_abstract": "Understanding the structure of complex, nonstationary, high-dimensional time-evolving signals is a central challenge in scientific data analysis. In many domains, such as speech and biomedical signal processing, the ability to learn disentangled and interpretable representations is critical for uncovering latent generative mechanisms. Traditional approaches to unsupervised representation learning, including variational autoencoders (VAEs), often struggle to capture the temporal and spectral diversity inherent in such data. Here we introduce variational decomposition autoencoding (VDA), a framework that extends VAEs by incorporating a strong structural bias toward signal decomposition. VDA is instantiated through variational decomposition autoencoders (DecVAEs), i.e., encoder-only neural networks that combine a signal decomposition model, a contrastive self-supervised task, and variational prior approximation to learn multiple latent subspaces aligned with time-frequency characteristics. We demonstrate the effectiveness of DecVAEs on simulated data and three publicly available scientific datasets, spanning speech recognition, dysarthria severity evaluation, and emotional speech classification. Our results demonstrate that DecVAEs surpass state-of-the-art VAE-based methods in terms of disentanglement quality, generalization across tasks, and the interpretability of latent encodings. These findings suggest that decomposition-aware architectures can serve as robust tools for extracting structured representations from dynamic signals, with potential applications in clinical diagnostics, human-computer interaction, and adaptive neurotechnologies.",
    "paper_abstract_zh": "理解复杂、非平稳、高维时变信号的结构是科学数据分析中的核心挑战。在许多领域，如语音和生物医学信号处理中，学习解耦且可解释的表示对于揭示潜在生成机制至关重要。传统的无监督表示学习方法，包括变分自编码器(VAEs)，通常难以捕捉此类数据中固有的时域和频域多样性。在此，我们引入变分分解自编码(VDA)，这是一个通过引入强信号分解结构偏向来扩展VAEs的框架。VDA通过变分分解自编码器(DecVAEs)实现，即仅使用编码器的神经网络，结合信号分解模型、对比自监督任务和变分先验近似，以学习与时频特征对齐的多个潜在子空间。我们在模拟数据和三个公开可用的科学数据集上证明了DecVAEs的有效性，涵盖语音识别、构音障碍严重程度评估和情感语音分类。我们的结果表明，DecVAEs在解耦质量、跨任务泛化能力和潜在编码的可解释性方面超越了最先进的VAE基线方法。这些发现表明，分解感知架构可以作为从动态信号中提取结构化表示的强大工具，在临床诊断、人机交互和自适应神经技术方面具有潜在应用。",
    "subjects": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Audio and Speech Processing (eess.AS)",
      "Signal Processing (eess.SP)",
      "Machine Learning (stat.ML)"
    ],
    "update_time": "2026-01-13",
    "paper_authors": "Ioannis Ziogas, Aamna Al Shehhi, Ahsan H. Khandoker, Leontios J. Hadjileontiadis",
    "topic": [
      "Audio Representation Learning",
      "Speech Recognition",
      "Audio Classification"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Directional Selective Fixed-Filter Active Noise Control Based on a Convolutional Neural Network in Reverberant Environments",
    "paper_title_zh": "基于卷积神经网络的混响环境中方向选择性固定滤波器有源噪声控制",
    "paper_id": "2601.06981",
    "paper_abstract": "Selective fixed-filter active noise control (SFANC) is a novel approach capable of mitigating noise with varying frequency characteristics. It offers faster response and greater computational efficiency compared to traditional adaptive algorithms. However, spatial factors, particularly the influence of the noise source location, are often overlooked. Some existing studies have explored the impact of the direction-of-arrival (DoA) of the noise source on ANC performance, but they are mostly limited to free-field conditions and do not consider the more complex indoor reverberant environments. To address this gap, this paper proposes a learning-based directional SFANC method that incorporates the DoA of the noise source in reverberant environments. In this framework, multiple reference signals are processed by a convolutional neural network (CNN) to estimate the azimuth and elevation angles of the noise source, as well as to identify the most appropriate control filter for effective noise cancellation. Compared to traditional adaptive algorithms, the proposed approach achieves superior noise reduction with shorter response times, even in the presence of reverberations.",
    "paper_abstract_zh": "选择性固定滤波器有源噪声控制(SFANC)是一种能够缓解具有变化频率特性噪声的新方法。与传统自适应算法相比，它具有更快的响应速度和更高的计算效率。然而，空间因素，特别是噪声源位置的影响，经常被忽视。一些现有研究探讨了噪声源的到达方向(DoA)对ANC性能的影响，但它们大多局限于自由场条件，而没有考虑更复杂的室内混响环境。为了解决这一差距，本文提出了一种基于学习的方向性SFANC方法，该方法将混响环境中噪声源的DoA纳入考虑。在该框架中，多个参考信号通过卷积神经网络(CNN)进行处理，以估计噪声源的方位角和仰角，并识别最合适的控制滤波器以实现有效的噪声消除。与传统自适应算法相比，即使在存在混响的情况下，所提出的方法也能实现更好的降噪效果和更短的响应时间。",
    "subjects": [
      "Signal Processing (eess.SP)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2026-01-13",
    "paper_authors": "Boxiang Wang, Zhengding Luo, Haowen Li, Dongyuan Shi, Junwei Ji, Ziyi Yang, Woon-Seng Gan",
    "topic": [
      "Speech Enhancement",
      "Other"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "An Intelligent AI glasses System with Multi-Agent Architecture for Real-Time Voice Processing and Task Execution",
    "paper_title_zh": "具有多智能体架构的智能AI眼镜系统，用于实时语音处理和任务执行",
    "paper_id": "2601.06235",
    "paper_abstract": "This paper presents an AI glasses system that integrates real-time voice processing, artificial intelligence(AI) agents, and cross-network streaming capabilities. The system employs dual-agent architecture where Agent 01 handles Automatic Speech Recognition (ASR) and Agent 02 manages AI processing through local Large Language Models (LLMs), Model Context Protocol (MCP) tools, and Retrieval-Augmented Generation (RAG). The system supports real-time RTSP streaming for voice and video data transmission, eye tracking data collection, and remote task execution through RabbitMQ messaging. Implementation demonstrates successful voice command processing with multilingual support and cross-platform task execution capabilities.",
    "paper_abstract_zh": "本文介绍了一个集成实时语音处理、人工智能(AI)智能体和跨网络流传输能力的AI眼镜系统。该系统采用双智能体架构，其中智能体01负责自动语音识别(ASR)，智能体02通过本地大型语言模型(LLMs)、模型上下文协议(MCP)工具和检索增强生成(RAG)管理AI处理。该系统支持实时RTSP流传输用于语音和视频数据传输，眼动追踪数据收集，以及通过RabbitMQ消息传递的远程任务执行。实现结果表明，该系统能成功处理多语言支持的语音命令，并具有跨平台任务执行能力。",
    "subjects": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "update_time": "2026-01-13",
    "paper_authors": "Sheng-Kai Chen, Jyh-Horng Wu, Ching-Yao Lin, Yen-Ting Lin",
    "topic": [
      "Speech Recognition",
      "Other"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Representing Sounds as Neural Amplitude Fields: A Benchmark of Coordinate-MLPs and A Fourier Kolmogorov-Arnold Framework",
    "paper_title_zh": "将声音表示为神经振幅场：坐标MLP和傅里叶柯尔莫哥洛夫-阿诺德框架的基准测试",
    "paper_id": "2601.06406",
    "paper_abstract": "Although Coordinate-MLP-based implicit neural representations have excelled in representing radiance fields, 3D shapes, and images, their application to audio signals remains underexplored. To fill this gap, we investigate existing implicit neural representations, from which we extract 3 types of positional encoding and 16 commonly used activation functions. Through combinatorial design, we establish the first benchmark for Coordinate-MLPs in audio signal representations. Our benchmark reveals that Coordinate-MLPs require complex hyperparameter tuning and frequency-dependent initialization, limiting their robustness. To address these issues, we propose Fourier-ASR, a novel framework based on the Fourier series theorem and the Kolmogorov-Arnold representation theorem. Fourier-ASR introduces Fourier Kolmogorov-Arnold Networks (Fourier-KAN), which leverage periodicity and strong nonlinearity to represent audio signals, eliminating the need for additional positional encoding. Furthermore, a Frequency-adaptive Learning Strategy (FaLS) is proposed to enhance the convergence of Fourier-KAN by capturing high-frequency components and preventing overfitting of low-frequency signals. Extensive experiments conducted on natural speech and music datasets reveal that: (1) well-designed positional encoding and activation functions in Coordinate-MLPs can effectively improve audio representation quality; and (2) Fourier-ASR can robustly represent complex audio signals without extensive hyperparameter tuning. Looking ahead, the continuity and infinite resolution of implicit audio representations make our research highly promising for tasks such as audio compression, synthesis, and generation. The source code will be released publicly to ensure reproducibility. The code is available at this https URL.",
    "paper_abstract_zh": "尽管基于坐标MLP的隐式神经表示在表示辐射场、3D形状和图像方面表现出色，但它们在音频信号中的应用仍未得到充分探索。为了填补这一空白，我们研究了现有的隐式神经表示，从中提取了3种位置编码和16种常用的激活函数。通过组合设计，我们建立了音频信号表示中坐标MLP的第一个基准测试。我们的基准测试表明，坐标MLP需要复杂的超参数调整和频率相关的初始化，这限制了它们的鲁棒性。为了解决这些问题，我们提出了Fourier-ASR，这是一种基于傅里叶级数定理和柯尔莫哥洛夫-阿诺德表示定理的新框架。Fourier-ASR引入了傅里叶柯尔莫哥洛夫-阿诺德网络（Fourier-KAN），它们利用周期性和强非线性来表示音频信号，无需额外的位置编码。此外，我们还提出了频率自适应学习策略（FaLS），通过捕获高频分量并防止低频信号过拟合来增强Fourier-KAN的收敛性。在自然语音和音乐数据集上进行的大量实验表明：(1) 坐标MLP中精心设计的位置编码和激活函数可以有效提高音频表示质量；(2) Fourier-ASR可以在无需大量超参数调整的情况下鲁棒地表示复杂的音频信号。展望未来，隐式音频表示的连续性和无限分辨率使我们的研究在音频压缩、合成和生成等任务中具有很高的前景。源代码将公开发布以确保可复现性。代码可在提供的https URL获取。",
    "subjects": [
      "Sound (cs.SD)"
    ],
    "update_time": "2026-01-13",
    "paper_authors": "Linfei Li, Lin Zhang, Zhong Wang, Fengyi Zhang, Zelin Li, Ying Shen",
    "topic": [
      "Audio Representation Learning",
      "Speech Recognition",
      "Music Information Retrieval"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "MoEScore: Mixture-of-Experts-Based Text-Audio Relevance Score Prediction for Text-to-Audio System Evaluation",
    "paper_title_zh": "MoEScore: 基于专家混合的文本-音频相关性评分预测用于文本到音频系统评估",
    "paper_id": "2601.06829",
    "paper_abstract": "Recent advances in generative models have enabled modern Text-to-Audio (TTA) systems to synthesize audio with high perceptual quality. However, TTA systems often struggle to maintain semantic consistency with the input text, leading to mismatches in sound events, temporal tructures, or contextual relationships. Evaluating semantic fidelity in TTA remains a significant challenge. Traditional methods primarily rely on subjective human listening tests, which is time-consuming. To solve this, we propose an objective evaluator based on a Mixture of Experts (MoE) architecture with Sequential Cross-Attention (SeqCoAttn). Our model achieves the first rank in the XACLE Challenge, with an SRCC of 0.6402 (an improvement of 30.6% over the challenge baseline) on the test dataset. Code is available at: this https URL.",
    "paper_abstract_zh": "生成模型的最新进展使得现代文本到音频（TTA）系统能够合成具有高感知质量的音频。然而，TTA系统通常难以保持与输入文本的语义一致性，导致声音事件、时间结构或上下文关系的不匹配。评估TTA中的语义保真度仍然是一个重大挑战。传统方法主要依赖主观的人类听音测试，这非常耗时。为此，我们提出了一种基于专家混合（MoE）架构和顺序交叉注意力（SeqCoAttn）的客观评估器。我们的模型在XACLE挑战赛中排名第一，在测试数据集上实现了0.6402的SRCC（比挑战赛基线提高30.6%）。代码可在以下网址获取：this https URL。",
    "subjects": [
      "Sound (cs.SD)"
    ],
    "update_time": "2026-01-13",
    "paper_authors": "Bochao Sun, Yang Xiao, Han Yin",
    "topic": [
      "Audio Representation Learning",
      "Audio Classification"
    ],
    "category": [
      "Other"
    ]
  },
  {
    "paper_title": "ESDD2: Environment-Aware Speech and Sound Deepfake Detection Challenge Evaluation Plan",
    "paper_title_zh": "ESDD2：环境感知语音和声音深度伪造检测挑战评估计划",
    "paper_id": "2601.07303",
    "paper_abstract": "Audio recorded in real-world environments often contains a mixture of foreground speech and background environmental sounds. With rapid advances in text-to-speech, voice conversion, and other generation models, either component can now be modified independently. Such component-level manipulations are harder to detect, as the remaining unaltered component can mislead the systems designed for whole deepfake audio, and they often sound more natural to human listeners. To address this gap, we have proposed CompSpoofV2 dataset and a separation-enhanced joint learning framework. CompSpoofV2 is a large-scale curated dataset designed for component-level audio anti-spoofing, which contains over 250k audio samples, with a total duration of approximately 283 hours. Based on the CompSpoofV2 and the separation-enhanced joint learning framework, we launch the Environment-Aware Speech and Sound Deepfake Detection Challenge (ESDD2), focusing on component-level spoofing, where both speech and environmental sounds may be manipulated or synthesized, creating a more challenging and realistic detection scenario. The challenge will be held in conjunction with the IEEE International Conference on Multimedia and Expo 2026 (ICME 2026).",
    "paper_abstract_zh": "在真实环境中录制的音频通常包含前景语音和背景环境声音的混合。随着文本转语音、语音转换和其他生成模型的快速发展，这两个组件现在都可以被独立修改。这种组件级别的操作更难检测，因为未修改的剩余组件可能会误导为整个深度伪造音频设计的系统，并且它们对人类听众来说通常听起来更自然。为了解决这一差距，我们提出了CompSpoofV2数据集和一种分离增强的联合学习框架。CompSpoofV2是一个专为组件级别音频反欺骗设计的大型策划数据集，包含超过25万个音频样本，总时长约283小时。基于CompSpoofV2和分离增强的联合学习框架，我们启动了环境感知语音和声音深度伪造检测挑战（ESDD2），专注于组件级别的欺骗，其中语音和环境声音都可能被操作或合成，从而创建更具挑战性和现实性的检测场景。该挑战将与IEEE国际多媒体与博览会2026（ICME 2026）同期举行。",
    "subjects": [
      "Sound (cs.SD)"
    ],
    "update_time": "2026-01-13",
    "paper_authors": "Xueping Zhang, Han Yin, Yang Xiao, Lin Zhang, Ting Dang",
    "topic": [
      "Audio Classification",
      "Speech Recognition",
      "Speech Synthesis"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "SEE: Signal Embedding Energy for Quantifying Noise Interference in Large Audio Language Models",
    "paper_title_zh": "SEE：信号嵌入能量用于量化大型音频语言模型中的噪声干扰",
    "paper_id": "2601.07331",
    "paper_abstract": "Large Audio Language Models (LALMs) have been widely applied in real-time scenarios, such as in-car assistants and online meeting comprehension. In practice, audio inputs are often corrupted by device and environmental noise, leading to performance degradation. However, existing LALM studies on noise lack quantitative analysis and rely mainly on intuition and empirical observation, thus failing to understand practical robustness. To address this issue, we introduce Signal Embedding Energy (SEE), a method for quantifying the impact of noise intensity on LALM inputs, enabling the differentiation of LALM robustness in real-world deployments. SEE introduces a perspective based on structured activation subspaces derived from the model's internal representations, which more accurately captures its perception of noise than raw audio features. Across experiments, SEE exhibits a strong correlation with LALM performance, achieving a correlation of 0.98. Surprisingly, traditional audio denoising methods are only marginally effective for LALMs, and, in some cases, even increase SEE and impair performance. This suggests a mismatch between speech-centric denoising objectives and the noise sensitivity of modern LALMs. Therefore, we propose a mitigation strategy derived from SEE to denoise LALM inputs, outperforming existing denoising methods. This paper introduces a novel metric for noise quantification in LALMs, providing guidance for robustness improvements in real-world deployments.",
    "paper_abstract_zh": "大型音频语言模型（LALMs）已广泛应用于实时场景，如车载助手和在线会议理解。在实际应用中，音频输入常受设备和环境噪声的干扰，导致性能下降。然而，现有关于LALM噪声的研究缺乏定量分析，主要依赖直觉和经验观察，因此无法理解其实际鲁棒性。为解决这一问题，我们引入了信号嵌入能量（SEE），一种量化噪声强度对LALM输入影响的方法，能够区分实际部署中LALM的鲁棒性。SEE基于模型内部表示的结构化激活子空间引入了一种视角，比原始音频特征更准确地捕捉模型对噪声的感知。在实验中，SEE与LALM性能表现出强相关性，相关系数达0.98。令人惊讶的是，传统音频降噪方法对LALMs效果有限，在某些情况下甚至增加SEE并损害性能。这表明以语音为中心的降噪目标与现代LALMs的噪声敏感性之间存在不匹配。因此，我们提出了一种基于SEE的降噪策略来处理LALM输入，优于现有降噪方法。本文为LALMs中的噪声量化引入了一种新指标，为实际部署中的鲁棒性改进提供了指导。",
    "subjects": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)"
    ],
    "update_time": "2026-01-13",
    "paper_authors": "Yuanhe Zhang, Jiayu Tian, Yibo Zhang, Shilinlu Yan, Liang Lin, Zhenhong Zhou, Li Sun, Sen Su",
    "topic": [
      "Speech Recognition",
      "Speech Enhancement"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "FOCAL: A Novel Benchmarking Technique for Multi-modal Agents",
    "paper_title_zh": "FOCAL: 一种用于多模态代理的新型基准测试技术",
    "paper_id": "2601.07367",
    "paper_abstract": "With the recent advancements in reasoning capa- bilities, tool calling using MCP servers and Audio Language Models (ALMs), development and integration of multi-modal agents (with voice and text support) has come to the industry forefront. Cascading pipelines for voice agents still play a central role in the industry owing to their superior reasoning capabilities facilitated by LLMs. Although, cascading pipelines often present error propagation through the pipeline. We propose a framework, FOCAL to benchmark end-to-end reasoning, component-wise error propagation and error analysis for automated as well as human-assisted testing of multi-modal agents (voice to voice + text input). We also share two novel metrics viz. Reasoning and Semantic scores to evaluate efficacy of the agent in having meaningful conversations in voice mode.",
    "paper_abstract_zh": "随着推理能力的最新进展，使用MCP服务器和音频语言模型(ALMs)进行工具调用，多模态代理（具有语音和文本支持）的开发和集成已成为行业前沿。由于大语言模型(LLMs)提供的卓越推理能力，级联管道语音代理仍在行业中扮演核心角色。然而，级联管道通常会在管道中传播错误。我们提出一个名为FOCAL的框架，用于对多模态代理（语音到语音+文本输入）进行端到端推理、组件级错误传播和错误分析的基准测试，支持自动化和人工辅助测试。我们还分享了两种新颖的指标，即推理分数和语义分数，用于评估代理在语音模式下进行有意义对话的效能。",
    "subjects": [
      "Sound (cs.SD)"
    ],
    "update_time": "2026-01-13",
    "paper_authors": "Aditya Choudhary, Anupam Purwar",
    "topic": [
      "Speech Recognition",
      "Other"
    ],
    "category": [
      "Speech"
    ]
  }
]