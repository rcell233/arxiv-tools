[
  {
    "paper_title": "Hearing Health in Home Healthcare: Leveraging LLMs for Illness Scoring and ALMs for Vocal Biomarker Extraction",
    "paper_title_zh": "家庭医疗中的听力健康：利用LLM进行疾病评分和利用ALM提取语音生物标志物",
    "paper_id": "2510.18169",
    "paper_abstract": "The growing demand for home healthcare calls for tools that can support care delivery. In this study, we explore automatic health assessment from voice using real-world home care visit data, leveraging the diverse patient information it contains. First, we utilize Large Language Models (LLMs) to integrate Subjective, Objective, Assessment, and Plan (SOAP) notes derived from unstructured audio transcripts and structured vital signs into a holistic illness score that reflects a patient's overall health. This compact representation facilitates cross-visit health status comparisons and downstream analysis. Next, we design a multi-stage preprocessing pipeline to extract short speech segments from target speakers in home care recordings for acoustic analysis. We then employ an Audio Language Model (ALM) to produce plain-language descriptions of vocal biomarkers and examine their association with individuals' health status. Our experimental results benchmark both commercial and open-source LLMs in estimating illness scores, demonstrating their alignment with actual clinical outcomes, and revealing that SOAP notes are substantially more informative than vital signs. Building on the illness scores, we provide the first evidence that ALMs can identify health-related acoustic patterns from home care recordings and present them in a human-readable form. Together, these findings highlight the potential of LLMs and ALMs to harness heterogeneous in-home visit data for better patient monitoring and care.",
    "paper_abstract_zh": "家庭医疗需求的增长需要支持护理交付的工具。在本研究中，我们利用真实世界家庭护理访问数据中的多样化患者信息，探索从语音进行自动健康评估。首先，我们利用大型语言模型（LLM）将来自非结构化音频转录和结构化生命体征的主观、客观、评估和计划（SOAP）笔记整合成一个反映患者整体健康的综合疾病评分。这种紧凑表示促进了跨访问健康状态比较和下游分析。接下来，我们设计了一个多阶段预处理管道，从家庭护理录音中提取目标说话者的短语音段进行声学分析。然后，我们采用音频语言模型（ALM）生成语音生物标志物的通俗描述，并检查它们与个体健康状态的关联。我们的实验结果对商业和开源LLM在估计疾病评分方面进行了基准测试，证明了它们与实际临床结果的一致性，并揭示SOAP笔记比生命体征信息量大得多。基于疾病评分，我们首次提供了ALM可以从家庭护理录音中识别与健康相关的声学模式并以人类可读形式呈现的证据。这些发现共同突显了LLM和ALM利用异构家庭访问数据进行更好患者监测和护理的潜力。",
    "subjects": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2025-10-22",
    "paper_authors": "Yu-Wen Chen, William Ho, Sasha M. Vergez, Grace Flaherty, Pallavi Gupta, Zhihong Zhang, Maryam Zolnoori, Margaret V. McDonald, Maxim Topaz, Zoran Kostic, Julia Hirschberg",
    "topic": [
      "Audio Representation Learning",
      "Speech Recognition"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Joint Estimation of Piano Dynamics and Metrical Structure with a Multi-task Multi-Scale Network",
    "paper_title_zh": "使用多任务多尺度网络联合估计钢琴力度和节拍结构",
    "paper_id": "2510.18190",
    "paper_abstract": "Estimating piano dynamic from audio recordings is a fundamental challenge in computational music analysis. In this paper, we propose an efficient multi-task network that jointly predicts dynamic levels, change points, beats, and downbeats from a shared latent representation. These four targets form the metrical structure of dynamics in the music score. Inspired by recent vocal dynamic research, we use a multi-scale network as the backbone, which takes Bark-scale specific loudness as the input feature. Compared to log-Mel as input, this reduces model size from 14.7 M to 0.5 M, enabling long sequential input. We use a 60-second audio length in audio segmentation, which doubled the length of beat tracking commonly used. Evaluated on the public MazurkaBL dataset, our model achieves state-of-the-art results across all tasks. This work sets a new benchmark for piano dynamic estimation and delivers a powerful and compact tool, paving the way for large-scale, resource-efficient analysis of musical expression.",
    "paper_abstract_zh": "从音频录音中估计钢琴力度是计算音乐分析中的一个基本挑战。在本文中，我们提出了一种高效的多任务网络，可以从共享的潜在表示中联合预测力度级别、变化点、节拍和强拍。这四个目标构成了音乐乐谱中力度的节拍结构。受近期声乐动态研究的启发，我们使用多尺度网络作为骨干网络，以Bark尺度特定响度作为输入特征。与使用log-Mel作为输入相比，这使模型大小从14.7 M减少到0.5 M，从而能够处理长序列输入。我们在音频分段中使用60秒的音频长度，这是常用节拍跟踪长度的两倍。在公开的MazurkaBL数据集上评估，我们的模型在所有任务上都达到了最先进的结果。这项工作为钢琴力度估计设定了新的基准，并提供了一个强大而紧凑的工具，为音乐表达的大规模、资源高效分析铺平了道路。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)"
    ],
    "update_time": "2025-10-22",
    "paper_authors": "Zhanhong He, Hanyu Meng, David Huang, Roberto Togneri",
    "topic": [
      "Audio Representation Learning",
      "Music Information Retrieval"
    ],
    "category": [
      "Music"
    ]
  },
  {
    "paper_title": "Adaptive Per-Channel Energy Normalization Front-end for Robust Audio Signal Processing",
    "paper_title_zh": "自适应按通道能量归一化前端用于鲁棒音频信号处理",
    "paper_id": "2510.18206",
    "paper_abstract": "In audio signal processing, learnable front-ends have shown strong performance across diverse tasks by optimizing task-specific representation. However, their parameters remain fixed once trained, lacking flexibility during inference and limiting robustness under dynamic complex acoustic environments. In this paper, we introduce a novel adaptive paradigm for audio front-ends that replaces static parameterization with a closed-loop neural controller. Specifically, we simplify the learnable front-end LEAF architecture and integrate a neural controller for adaptive representation via dynamically tuning Per-Channel Energy Normalization. The neural controller leverages both the current and the buffered past subband energies to enable input-dependent adaptation during inference. Experimental results on multiple audio classification tasks demonstrate that the proposed adaptive front-end consistently outperforms prior fixed and learnable front-ends under both clean and complex acoustic conditions. These results highlight neural adaptability as a promising direction for the next generation of audio front-ends.",
    "paper_abstract_zh": "在音频信号处理领域，可学习的前端架构通过优化任务特定表示展现了强大的性能。然而，这些架构在训练完成后参数固定，在推理过程中缺乏灵活性，限制了其在动态复杂声学环境下的鲁棒性。本文提出一种新颖的音频前端自适应范式，通过闭环神经控制器替代静态参数化方法。具体而言，我们简化了可学习前端LEAF架构，并通过动态调整按通道能量归一化集成神经控制器实现自适应表示。该神经控制器利用当前和缓冲的过去子带能量，使推理过程具备输入依赖的自适应能力。在多个音频分类任务上的实验结果表明，所提出的自适应前端在清洁和复杂声学条件下均显著优于现有固定参数和可学习前端。这些结果凸显了神经自适应作为下一代音频前端有前景的发展方向。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)",
      "Signal Processing (eess.SP)",
      "Sound (cs.SD)"
    ],
    "update_time": "2025-10-22",
    "paper_authors": "Hanyu Meng, Vidhyasaharan Sethu, Eliathamby Ambikairajah, Qiquan Zhang, Haizhou Li",
    "topic": [
      "Audio Classification",
      "Audio Representation Learning"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "MVDR Beamforming for Cyclostationary Processes",
    "paper_title_zh": "循环平稳过程的MVDR波束形成",
    "paper_id": "2510.18391",
    "paper_abstract": "Conventional acoustic beamformers assume that noise is stationary within short time frames. This assumption prevents them from exploiting correlations between frequencies in almost-periodic noise sources such as musical instruments, fans, and engines. These signals exhibit periodically varying statistics and are better modeled as cyclostationary processes. This paper introduces the cyclic MVDR (cMVDR) beamformer, an extension of the conventional MVDR that leverages both spatial and spectral correlations to improve noise reduction, particularly in low-SNR scenarios. The method builds on frequency-shifted (FRESH) filtering, where shifted versions of the input are combined to attenuate or amplify components that are coherent across frequency. To address inharmonicity, where harmonic partials deviate from exact integer multiples of the fundamental frequency, we propose a data-driven strategy that estimates resonant frequencies via periodogram analysis and computes the frequency shifts from their spacing. Analytical and experimental results demonstrate that performance improves with increasing spectral correlation. On real recordings, the cMVDR achieves up to 5 dB gain in scale-invariant signal-to-distortion ratio (SI-SDR) over the MVDR and remains effective even with a single microphone. Code is available at this https URL.",
    "paper_abstract_zh": "传统的声学波束形成器假设噪声在短时间帧内是平稳的。这一假设使得它们无法利用音乐乐器、风扇和发动机等几乎周期性噪声源中频率之间的相关性。这些信号表现出周期性变化的统计特性，更适合建模为循环平稳过程。本文引入了循环MVDR (cMVDR)波束形成器，这是传统MVDR的扩展，它利用空间和频谱相关性来提高降噪性能，特别是在低信噪比场景下。该方法基于频移(FRESH)滤波，其中输入的移位版本被组合起来以衰减或增强跨频率相干的分量。为了解决非谐波性问题（即谐波分量偏离基频的整数倍），我们提出了一种数据驱动策略，通过周期图分析估计谐振频率，并根据它们的间距计算频移。分析和实验结果表明，性能随着频谱相关性的增加而提高。在真实录音中，cMVDR在尺度不变信噪比(SI-SDR)上比MVDR提高了高达5 dB，并且即使使用单个麦克风也保持有效。代码可在提供的URL获取。",
    "subjects": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2025-10-22",
    "paper_authors": "Giovanni Bologni, Martin Bo Møller, Richard Heusdens, Richard C. Hendriks",
    "topic": [
      "Speech Enhancement",
      "Other"
    ],
    "category": [
      "Other"
    ]
  },
  {
    "paper_title": "ProLAP: Probabilistic Language-Audio Pre-Training",
    "paper_title_zh": "ProLAP: 概率语言-音频预训练",
    "paper_id": "2510.18423",
    "paper_abstract": "Language-audio joint representation learning frameworks typically depend on deterministic embeddings, assuming a one-to-one correspondence between audio and text. In real-world settings, however, the language-audio relationship is inherently many-to-many: one audio segment can be described by multiple captions and vice versa. To address this, we propose Probabilistic Language-Audio Pre-training (ProLAP), which models multiplicity as the spread of probability distributions in a joint language-audio embedding space. To train the intra-modal hierarchical relationship effectively, we also introduce two objectives: (i) hierarchical inclusion loss to promote semantic hierarchical understanding of inputs and (ii) mask repulsive loss to improve the efficiency of learning when optimizing the hierarchical inclusion loss. With this training strategy, our model can learn the hierarchical structure inherent in the data even from small datasets, in contrast to prior probabilistic approaches that rely on large-scale datasets. In our experiments, ProLAP outperforms existing deterministic approaches on audio-text retrieval tasks. Moreover, through experiments on the audio traversal task introduced in this paper, we demonstrate that ProLAP captures the plausible semantic hierarchy.",
    "paper_abstract_zh": "语言-音频联合表征学习框架通常依赖于确定性嵌入，假设音频和文本之间存在一一对应关系。然而，在现实世界中，语言-音频关系本质上是多对多的：一个音频片段可以由多个字幕描述，反之亦然。为解决这一问题，我们提出了概率语言-音频预训练（ProLAP），它将多样性建模为联合语言-嵌入空间中概率分布的扩散。为了有效训练模态内的层次关系，我们还引入了两个目标：（i）层次包含损失，以促进对输入的语义层次理解；（ii）掩码排斥损失，以提高优化层次包含损失时的学习效率。通过这种训练策略，我们的模型即使从小数据集也能学习数据中固有的层次结构，这与依赖大规模数据集的先前概率方法形成对比。在我们的实验中，ProLAP在音频-文本检索任务上优于现有的确定性方法。此外，通过本文引入的音频遍历任务的实验，我们证明了ProLAP能够捕捉合理的语义层次结构。",
    "subjects": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2025-10-22",
    "paper_authors": "Toranosuke Manabe, Yuchi Ishikawa, Hokuto Munakata, Tatsuya Komatsu",
    "topic": [
      "Audio Representation Learning",
      "Other"
    ],
    "category": [
      "Other"
    ]
  },
  {
    "paper_title": "Diffusion Buffer for Online Generative Speech Enhancement",
    "paper_title_zh": "用于在线生成式语音增强的扩散缓冲器",
    "paper_id": "2510.18744",
    "paper_abstract": "Online Speech Enhancement was mainly reserved for predictive models. A key advantage of these models is that for an incoming signal frame from a stream of data, the model is called only once for enhancement. In contrast, generative Speech Enhancement models often require multiple calls, resulting in a computational complexity that is too high for many online speech enhancement applications. This work presents the Diffusion Buffer, a generative diffusion-based Speech Enhancement model which only requires one neural network call per incoming signal frame from a stream of data and performs enhancement in an online fashion on a consumer-grade GPU. The key idea of the Diffusion Buffer is to align physical time with Diffusion time-steps. The approach progressively denoises frames through physical time, where past frames have more noise removed. Consequently, an enhanced frame is output to the listener with a delay defined by the Diffusion Buffer, and the output frame has a corresponding look-ahead. In this work, we extend upon our previous work by carefully designing a 2D convolutional UNet architecture that specifically aligns with the Diffusion Buffer's look-ahead. We observe that the proposed UNet improves performance, particularly when the algorithmic latency is low. Moreover, we show that using a Data Prediction loss instead of Denoising Score Matching loss enables flexible control over the trade-off between algorithmic latency and quality during inference. The extended Diffusion Buffer equipped with a novel NN and loss function drastically reduces the algorithmic latency from 320 - 960 ms to 32 - 176 ms with an even increased performance. While it has been shown before that offline generative diffusion models outperform predictive approaches in unseen noisy speech data, we confirm that the online Diffusion Buffer also outperforms its predictive counterpart on unseen noisy speech data.",
    "paper_abstract_zh": "在线语音增强主要采用预测模型。这些模型的一个关键优势是，对于来自数据流的输入信号帧，模型只需调用一次即可完成增强。相比之下，生成式语音增强模型通常需要多次调用，导致计算复杂度过高，不适合许多在线语音增强应用。本文提出了扩散缓冲器（Diffusion Buffer），这是一种基于生成式扩散的语音增强模型，对于来自数据流的每个输入信号帧只需调用一次神经网络，并在消费级GPU上以在线方式执行增强。扩散缓冲器的核心思想是将物理时间与扩散时间步对齐。该方法通过物理时间逐步去噪帧，其中过去的帧去除了更多噪声。因此，增强后的帧会以由扩散缓冲器定义的延迟输出给听众，且输出帧具有相应的前瞻性。在本文中，我们通过精心设计二维卷积UNet架构来扩展我们之前的工作，该架构与扩散缓冲器的前瞻性相匹配。我们观察到，所提出的UNet提高了性能，特别是在算法延迟较低的情况下。此外，我们证明，使用数据预测损失而非去噪得分匹配损失，可以在推理过程中灵活控制算法延迟与质量之间的权衡。配备新型神经网络和损失函数的扩展扩散缓冲器将算法延迟从320-960毫秒大幅降低至32-176毫秒，同时性能进一步提升。虽然已有研究表明，离线生成式扩散模型在未见过的噪声语音数据中优于预测方法，但我们确认，在线扩散缓冲器在未见过的噪声语音数据中也优于其预测对应模型。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)"
    ],
    "update_time": "2025-10-22",
    "paper_authors": "Bunlong Lay, Rostislav Makarov, Simon Welker, Maris Hillemann, Timo Gerkmann",
    "topic": [
      "Speech Enhancement",
      "Audio Representation Learning"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Transformer Redesign for Late Fusion of Audio-Text Features on Ultra-Low-Power Edge Hardware",
    "paper_title_zh": "面向超低功耗边缘硬件的音频-文本特征后融合Transformer重构",
    "paper_id": "2510.18036",
    "paper_abstract": "Deploying emotion recognition systems in real-world environments where devices must be small, low-power, and private remains a significant challenge. This is especially relevant for applications such as tension monitoring, conflict de-escalation, and responsive wearables, where cloud-based solutions are impractical. Multimodal emotion recognition has advanced through deep learning, but most systems remain unsuitable for deployment on ultra-constrained edge devices. Prior work typically relies on powerful hardware, lacks real-time performance, or uses unimodal input. This paper addresses that gap by presenting a hardware-aware emotion recognition system that combines acoustic and linguistic features using a late-fusion architecture optimised for Edge TPU. The design integrates a quantised transformer-based acoustic model with frozen keyword embeddings from a DSResNet-SE network, enabling real-time inference within a 1.8MB memory budget and 21-23ms latency. The pipeline ensures spectrogram alignment between training and deployment using MicroFrontend and MLTK. Evaluation on re-recorded, segmented IEMOCAP samples captured through the Coral Dev Board Micro microphone shows a 6.3% macro F1 improvement over unimodal baselines. This work demonstrates that accurate, real-time multimodal emotion inference is achievable on microcontroller-class edge platforms through task-specific fusion and hardware-guided model design.",
    "paper_abstract_zh": "在小型化、低功耗且注重隐私的现实环境部署情感识别系统仍面临重大挑战。这一问题在紧张监控、冲突降级和响应式可穿戴设备等场景中尤为突出，因云端解决方案在此类应用中不切实际。尽管深度学习推动了多模态情感识别的发展，但大多数系统仍难以在超约束边缘设备上部署。现有研究通常依赖高性能硬件、缺乏实时性或仅采用单模态输入。本文通过提出一种面向边缘TPU的硬件感知情感识别系统，填补了这一空白。该系统采用后融合架构整合声学与语言特征，结合量化Transformer声学模型和DSResNet-SE网络的冻结关键词嵌入，实现在1.8MB内存预算和21-23ms延迟下的实时推理。通过Coral Dev Board Micro麦克风录制的分段IEMOCAP样本评估表明，该方案较单模态基线提升了6.3%的宏平均F1分数。本研究证明，通过任务特异性融合与硬件引导的模型设计，微控制器级边缘平台可实现准确、实时的多模态情感推理。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)"
    ],
    "update_time": "2025-10-22",
    "paper_authors": "Stavros Mitsis, Ermos Hadjikyriakos, Humaid Ibrahim, Savvas Neofytou, Shashwat Raman, James Myles, Eiman Kanjo",
    "topic": [
      "Audio Classification",
      "Audio Representation Learning"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation",
    "paper_title_zh": "ParaStyleTTS：面向高效鲁棒的副语言风格控制的情感文本生成语音合成",
    "paper_id": "2510.18308",
    "paper_abstract": "Controlling speaking style in text-to-speech (TTS) systems has become a growing focus in both academia and industry. While many existing approaches rely on reference audio to guide style generation, such methods are often impractical due to privacy concerns and limited accessibility. More recently, large language models (LLMs) have been used to control speaking style through natural language prompts; however, their high computational cost, lack of interpretability, and sensitivity to prompt phrasing limit their applicability in real-time and resource-constrained environments. In this work, we propose ParaStyleTTS, a lightweight and interpretable TTS framework that enables expressive style control from text prompts alone. ParaStyleTTS features a novel two-level style adaptation architecture that separates prosodic and paralinguistic speech style modeling. It allows fine-grained and robust control over factors such as emotion, gender, and age. Unlike LLM-based methods, ParaStyleTTS maintains consistent style realization across varied prompt formulations and is well-suited for real-world applications, including on-device and low-resource deployment. Experimental results show that ParaStyleTTS generates high-quality speech with performance comparable to state-of-the-art LLM-based systems while being 30x faster, using 8x fewer parameters, and requiring 2.5x less CUDA memory. Moreover, ParaStyleTTS exhibits superior robustness and controllability over paralinguistic speaking styles, providing a practical and efficient solution for style-controllable text-to-speech generation. Demo can be found at this https URL. Code can be found at this https URL.",
    "paper_abstract_zh": "在文本到语音（TTS）系统中控制说话风格已成为学术界和工业界日益关注的焦点。虽然许多现有方法依赖参考音频来指导风格生成，但由于隐私问题和有限的可用性，这些方法通常不切实际。最近，大型语言模型（LLMs）已被用于通过自然语言提示控制说话风格；然而，它们的高计算成本、缺乏可解释性以及对提示措辞的敏感性限制了它们在实时和资源受限环境中的应用。在这项工作中，我们提出了ParaStyleTTS，一个轻量级且可解释的TTS框架，仅从文本提示中实现情感风格控制。ParaStyleTTS采用了一种新颖的两级风格适应架构，将韵律和副语言语音风格建模分开。它允许对情感、性别和年龄等因素进行细粒度和鲁棒的控制。与基于LLM的方法不同，ParaStyleTTS在不同的提示表述中保持一致的风格实现，非常适合包括设备端和低资源部署在内的实际应用。实验结果表明，ParaStyleTTS生成高质量的语音，性能与最先进的基于LLM的系统相当，同时速度快30倍，参数少8倍，CUDA内存需求少2.5倍。此外，ParaStyleTTS在副语言说话风格方面表现出卓越的鲁棒性和可控性，为风格可控的文本到语音生成提供了实用高效的解决方案。演示可以在https URL找到。代码可以在https URL找到。",
    "subjects": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2025-10-22",
    "paper_authors": "Haowei Lou, Hye-Young Paik, Wen Hu, Lina Yao",
    "topic": [
      "Speech Synthesis",
      "Audio Representation Learning"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "A Stage-Wise Learning Strategy with Fixed Anchors for Robust Speaker Verification",
    "paper_title_zh": "一种基于固定锚点的阶段性学习策略用于鲁棒说话人验证",
    "paper_id": "2510.18530",
    "paper_abstract": "Learning robust speaker representations under noisy conditions presents significant challenges, which requires careful handling of both discriminative and noise-invariant properties. In this work, we proposed an anchor-based stage-wise learning strategy for robust speaker representation learning. Specifically, our approach begins by training a base model to establish discriminative speaker boundaries, and then extract anchor embeddings from this model as stable references. Finally, a copy of the base model is fine-tuned on noisy inputs, regularized by enforcing proximity to their corresponding fixed anchor embeddings to preserve speaker identity under distortion. Experimental results suggest that this strategy offers advantages over conventional joint optimization, particularly in maintaining discrimination while improving noise robustness. The proposed method demonstrates consistent improvements across various noise conditions, potentially due to its ability to handle boundary stabilization and variation suppression separately.",
    "paper_abstract_zh": "在噪声条件下学习鲁棒的说话人表示具有重大挑战，这需要仔细处理判别性和噪声不变性。在这项工作中，我们提出了一种基于锚点的阶段性学习策略，用于鲁棒说话人表示学习。具体而言，我们的方法首先训练一个基础模型来建立判别性说话人边界，然后从中提取锚点嵌入作为稳定参考。最后，将基础模型的副本在噪声输入上进行微调，通过强制其接近对应的固定锚点嵌入进行正则化，以在失真情况下保留说话人身份。实验结果表明，与传统联合优化相比，该策略在保持判别性的同时提高噪声鲁棒性方面具有优势。所提出的方法在各种噪声条件下表现出一致的改进，这可能是由于其能够分别处理边界稳定性和变化抑制的能力。",
    "subjects": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2025-10-22",
    "paper_authors": "Bin Gu, Lipeng Dai, Huipeng Du, Haitao Zhao, Jibo Wei",
    "topic": [
      "Audio Representation Learning",
      "Audio Classification"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Noise-Conditioned Mixture-of-Experts Framework for Robust Speaker Verification",
    "paper_title_zh": "基于噪声条件混合专家框架的鲁棒说话人验证",
    "paper_id": "2510.18533",
    "paper_abstract": "Robust speaker verification under noisy conditions remains an open challenge. Conventional deep learning methods learn a robust unified speaker representation space against diverse background noise and achieve significant improvement. In contrast, this paper presents a noise-conditioned mixture-ofexperts framework that decomposes the feature space into specialized noise-aware subspaces for speaker verification. Specifically, we propose a noise-conditioned expert routing mechanism, a universal model based expert specialization strategy, and an SNR-decaying curriculum learning protocol, collectively improving model robustness and generalization under diverse noise conditions. The proposed method can automatically route inputs to expert networks based on noise information derived from the inputs, where each expert targets distinct noise characteristics while preserving speaker identity information. Comprehensive experiments demonstrate consistent superiority over baselines, confirming that explicit noise-dependent feature modeling significantly enhances robustness without sacrificing verification accuracy.",
    "paper_abstract_zh": "噪声条件下的鲁棒说话人验证仍然是一个开放的挑战。传统的深度学习方法学习一个统一的鲁棒说话人表示空间来应对各种背景噪声，并取得了显著改进。相比之下，本文提出了一种噪声条件混合专家框架，将特征空间分解为专门的噪声感知子空间用于说话人验证。具体而言，我们提出了噪声条件专家路由机制、基于模型的专家专业化策略以及信噪比衰减课程学习协议，共同提高了模型在多样化噪声条件下的鲁棒性和泛化能力。所提出的方法能够根据输入中提取的噪声信息自动将输入路由到专家网络，其中每个专家针对不同的噪声特征，同时保留说话人身份信息。全面的实验证明该方法在基线模型上具有持续优势，证实了显式的噪声依赖特征建模显著增强了鲁棒性，同时不牺牲验证准确性。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)",
      "Multimedia (cs.MM)"
    ],
    "update_time": "2025-10-22",
    "paper_authors": "Bin Gu, Lipeng Dai, Huipeng Du, Haitao Zhao, Jibo Wei",
    "topic": [
      "Audio Representation Learning",
      "Speech Enhancement"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Bayesian Low-Rank Factorization for Robust Model Adaptation",
    "paper_title_zh": "用于鲁棒模型适配的贝叶斯低秩分解",
    "paper_id": "2510.18723",
    "paper_abstract": "Large speech foundation models achieve strong performance across many domains, but they often require adaptation to handle local needs such as code-switching, where speakers mix languages within the same utterance. Direct fine-tuning of these models risks overfitting to the target domain and overwriting the broad capabilities of the base model. To address this challenge, we explore Bayesian factorized adapters for speech foundation models, which place priors near zero to achieve sparser adaptation matrices and thereby retain general performance while adapting to specific domains. We apply our approach to the Whisper model and evaluate on different multilingual code-switching scenarios. Our results show only minimal adaptation loss while significantly reducing catastrophic forgetting of the base model. Compared to LoRA, our method achieves a backward gain of 54% with only a 4% drop on the new domain. These findings highlight the effectiveness of Bayesian adaptation for fine-tuning speech foundation models without sacrificing generalization.",
    "paper_abstract_zh": "大型语音基础模型在许多领域表现出强大的性能，但它们通常需要适配以处理本地需求，如代码转换（code-switching），即说话者在同一话语中混合使用多种语言。直接对这些模型进行微调可能会导致在目标域上过拟合并覆盖基础模型的广泛能力。为了解决这一挑战，我们探索了语音基础模型的贝叶斯分解适配器（Bayesian factorized adapters），其在接近零的位置设置先验，以实现更稀疏的适配矩阵，从而在适应特定领域的同时保留通用性能。我们将该方法应用于Whisper模型，并在不同的多语言代码转换场景下进行了评估。结果表明，我们的方法仅造成最小的适配损失，同时显著减少了基础模型的灾难性遗忘。与LoRA相比，我们的方法在新领域仅下降4%的情况下，实现了54%的向后增益。这些发现凸显了贝叶斯适配在微调语音基础模型而不牺牲泛化能力方面的有效性。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)",
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)"
    ],
    "update_time": "2025-10-22",
    "paper_authors": "Enes Yavuz Ugan, Ngoc-Quan Pham, Alexander Waibel",
    "topic": [
      "Speech Recognition",
      "Audio Representation Learning"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Adapting Language Balance in Code-Switching Speech",
    "paper_title_zh": "调整代码切换语言平衡",
    "paper_id": "2510.18724",
    "paper_abstract": "Despite achieving impressive results on standard benchmarks, large foundational models still struggle against code-switching test cases. When data scarcity cannot be used as the usual justification for poor performance, the reason may lie in the infrequent occurrence of code-switched moments, where the embedding of the second language appears subtly. Instead of expecting the models to learn this infrequency on their own, it might be beneficial to provide the training process with labels. Evaluating model performance on code-switching data requires careful localization of code-switching points where recognition errors are most consequential, so that the analysis emphasizes mistakes occurring at those moments. Building on this observation, we leverage the difference between the embedded and the main language to highlight those code-switching points and thereby emphasize learning at those locations. This simple yet effective differentiable surrogate mitigates context bias during generation -- the central challenge in code-switching -- thereby improving the model's robustness. Our experiments with Arabic and Chinese-English showed that the models are able to predict the switching places more correctly, reflected by the reduced substitution error.",
    "paper_abstract_zh": "尽管在标准基准测试中取得了令人印象深刻的结果，大型基础模型在代码切换测试案例上仍然表现不佳。当数据稀缺不能作为表现不佳的通常理由时，原因可能在于代码切换时刻的罕见发生，此时第二语言的嵌入微妙地出现。与其期望模型自行学习这种罕见性，不如为训练过程提供标签。在代码切换数据上评估模型性能需要仔细定位代码切换点，这些点是识别错误最关键的位置，以便分析强调在这些时刻发生的错误。基于这一观察，我们利用嵌入语言和主要语言之间的差异来突出这些代码切换点，从而强调在这些位置的学习。这种简单而有效的可微分代理减轻了生成过程中的上下文偏差——这是代码切换中的核心挑战——从而提高了模型的鲁棒性。我们对阿拉伯语和中文-英语的实验表明，模型能够更准确地预测切换位置，这体现在替换错误的减少上。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)",
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)"
    ],
    "update_time": "2025-10-22",
    "paper_authors": "Enes Yavuz Ugan, Ngoc-Quan Pham, Alexander Waibel",
    "topic": [
      "Speech Recognition",
      "Audio Representation Learning"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "SegTune: Structured and Fine-Grained Control for Song Generation",
    "paper_title_zh": "SegTune：用于歌曲生成的结构化和细粒度控制",
    "paper_id": "2510.18416",
    "paper_abstract": "Recent advancements in song generation have shown promising results in generating songs from lyrics and/or global text prompts. However, most existing systems lack the ability to model the temporally varying attributes of songs, limiting fine-grained control over musical structure and dynamics. In this paper, we propose SegTune, a non-autoregressive framework for structured and controllable song generation. SegTune enables segment-level control by allowing users or large language models to specify local musical descriptions aligned to song this http URL segmental prompts are injected into the model by temporally broadcasting them to corresponding time windows, while global prompts influence the whole song to ensure stylistic coherence. To obtain accurate segment durations and enable precise lyric-to-music alignment, we introduce an LLM-based duration predictor that autoregressively generates sentence-level timestamped lyrics in LRC format. We further construct a large-scale data pipeline for collecting high-quality songs with aligned lyrics and prompts, and propose new evaluation metrics to assess segment-level alignment and vocal attribute consistency. Experimental results show that SegTune achieves superior controllability and musical coherence compared to existing baselines. See this https URL for demos of our work.",
    "paper_abstract_zh": "最近的歌曲生成研究在根据歌词和/或全局文本提示生成歌曲方面取得了 promising 的结果。然而，大多数现有系统缺乏对歌曲时变属性建模的能力，限制了音乐结构和动态的细粒度控制。在本文中，我们提出了 SegTune，一个用于结构化和可控歌曲生成的非自回归框架。SegTune 通过允许用户或大型语言模型指定与歌曲片段对齐的局部音乐描述，实现了片段级别的控制。片段提示通过时间广播方式注入模型到相应的时间窗口，而全局提示则影响整个歌曲以确保风格一致性。为了获得准确的片段持续时间并实现精确的歌词到音乐对齐，我们引入了一个基于 LLM 的持续时间预测器，该预测器自回归生成 LRC 格式的句子级带时间戳的歌词。我们进一步构建了一个大规模数据管道，用于收集具有对齐歌词和提示的高质量歌曲，并提出了新的评估指标来评估片段级别的对齐和声乐属性一致性。实验结果表明，与现有基线相比，SegTune 实现了更好的可控性和音乐连贯性。有关我们工作的演示，请访问 this http URL。",
    "subjects": [
      "Sound (cs.SD)"
    ],
    "update_time": "2025-10-22",
    "paper_authors": "Pengfei Cai, Joanna Wang, Haorui Zheng, Xu Li, Zihao Ji, Teng Ma, Zhongliang Liu, Chen Zhang, Pengfei Wan",
    "topic": [
      "Music Generation",
      "Music Information Retrieval"
    ],
    "category": [
      "Music"
    ]
  },
  {
    "paper_title": "SAC: Neural Speech Codec with Semantic-Acoustic Dual-Stream Quantization",
    "paper_title_zh": "SAC：基于语义-声学双流量化的神经语音编解码器",
    "paper_id": "2510.16841",
    "paper_abstract": "Speech codecs that convert continuous speech signals into discrete tokens have become essential for speech language models (SLMs). However, existing codecs struggle to balance high-quality reconstruction with semantically rich representations, limiting their effectiveness in both generative and understanding tasks. In this work, we propose SAC, a neural speech codec with semantic-acoustic dual-stream quantization. By disentangling semantic and acoustic modeling into two dedicated streams, SAC enables each to be optimized for its respective role. Comprehensive evaluations show that SAC achieves strong reconstruction performance across diverse bitrates under both clean and noisy conditions, with particularly high scores on UTMOS and WER, demonstrating superior perceptual quality and intelligibility. Moreover, SAC substantially outperforms state-of-the-art codecs in semantic representation, achieving a level comparable to that of self-supervised learning (SSL) continuous embeddings. Finally, our analysis of speech disentanglement highlights the effectiveness of the dual-stream design, offering new potential for controllable speech applications.",
    "paper_abstract_zh": "将连续语音信号转换为离散标记的语音编解码器已成为语音语言模型(SLM)的重要组成部分。然而，现有的编解码器难以在高质量重建与语义丰富表示之间取得平衡，限制了它们在生成和理解任务中的有效性。在这项工作中，我们提出了SAC，一种基于语义-声学双流量化的神经语音编解码器。通过将语义建模和声学建模解耦为两个专用流，SAC使每个流都能针对其各自的角色进行优化。全面的评估表明，SAC在干净和噪声条件下，各种比特率下均实现了强大的重建性能，特别是在UTMOS和WER上获得了高分，展示了卓越的感知质量和可懂度。此外，SAC在语义表示方面显著优于最先进的编解码器，达到了与自监督学习(SSL)连续嵌入相当的水平。最后，我们对语音解耦的分析强调了双流设计的有效性，为可控语音应用提供了新的可能性。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "update_time": "2025-10-22",
    "paper_authors": "Wenxi Chen, Xinsheng Wang, Ruiqi Yan, Yushen Chen, Zhikang Niu, Ziyang Ma, Xiquan Li, Yuzhe Liang, Hanlin Wen, Shunshun Yin, Ming Tao, Xie Chen",
    "topic": [
      "Audio Codec",
      "Speech Recognition",
      "Speech Synthesis"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "MLMA: Towards Multilingual with Mamba Based Architectures",
    "paper_title_zh": "MLMA：基于Mamba架构的多语言语音识别",
    "paper_id": "2510.18684",
    "paper_abstract": "Multilingual automatic speech recognition (ASR) remains a challenging task, especially when balancing performance across high- and low-resource languages. Recent advances in sequence modeling suggest that architectures beyond Transformers may offer better scalability and efficiency. In this work, we introduce MLMA (Multilingual Language Modeling with Mamba for ASR), a new approach that leverages the Mamba architecture--an efficient state-space model optimized for long-context sequence processing--for multilingual ASR. Using Mamba, MLMA implicitly incorporates language-aware conditioning and shared representations to support robust recognition across diverse languages. Experiments on standard multilingual benchmarks show that MLMA achieves competitive performance compared to Transformer-based architectures. These results highlight Mamba's potential as a strong backbone for scalable, efficient, and accurate multilingual speech recognition.",
    "paper_abstract_zh": "多语言自动语音识别（ASR）仍然是一个具有挑战性的任务，特别是在平衡高资源和低资源语言之间的性能时。序列建模的最新进展表明，超越Transformer的架构可能提供更好的可扩展性和效率。在这项工作中，我们引入了MLMA（用于ASR的多语言语言建模与Mamba），这是一种新方法，它利用Mamba架构——一种为长上下文序列处理优化的高效状态空间模型——进行多语言ASR。通过使用Mamba，MLMA隐式地融入了语言感知的条件和共享表示，以支持对多种语言的鲁棒识别。在标准多语言基准测试上的实验表明，MLMA与基于Transformer的架构实现了具有竞争力的性能。这些结果突显了Mamba作为可扩展、高效和准确的多语言语音识别骨干的潜力。",
    "subjects": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)"
    ],
    "update_time": "2025-10-22",
    "paper_authors": "Mohamed Nabih Ali, Daniele Falavigna, Alessio Brutti",
    "topic": [
      "Speech Recognition",
      "Audio Representation Learning"
    ],
    "category": [
      "Speech"
    ]
  }
]