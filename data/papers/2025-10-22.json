[
  {
    "paper_title": "Hearing Health in Home Healthcare: Leveraging LLMs for Illness Scoring and ALMs for Vocal Biomarker Extraction",
    "paper_title_zh": "家庭医疗中的听力健康：利用LLM进行疾病评分和ALM进行语音生物标志物提取",
    "paper_id": "2510.18169",
    "paper_abstract": "The growing demand for home healthcare calls for tools that can support care delivery. In this study, we explore automatic health assessment from voice using real-world home care visit data, leveraging the diverse patient information it contains. First, we utilize Large Language Models (LLMs) to integrate Subjective, Objective, Assessment, and Plan (SOAP) notes derived from unstructured audio transcripts and structured vital signs into a holistic illness score that reflects a patient's overall health. This compact representation facilitates cross-visit health status comparisons and downstream analysis. Next, we design a multi-stage preprocessing pipeline to extract short speech segments from target speakers in home care recordings for acoustic analysis. We then employ an Audio Language Model (ALM) to produce plain-language descriptions of vocal biomarkers and examine their association with individuals' health status. Our experimental results benchmark both commercial and open-source LLMs in estimating illness scores, demonstrating their alignment with actual clinical outcomes, and revealing that SOAP notes are substantially more informative than vital signs. Building on the illness scores, we provide the first evidence that ALMs can identify health-related acoustic patterns from home care recordings and present them in a human-readable form. Together, these findings highlight the potential of LLMs and ALMs to harness heterogeneous in-home visit data for better patient monitoring and care.",
    "paper_abstract_zh": "家庭医疗日益增长的需求需要能够支持护理交付的工具。在本研究中，我们利用家庭护理访问数据中的多样化患者信息，探索从语音中进行自动健康评估。首先，我们利用大型语言模型（LLM）将非结构化音频转录本中的主观、客观、评估和计划（SOAP）笔记与结构化生命体征整合，以反映患者整体健康的综合疾病评分。这种紧凑表示促进了跨访问健康状态比较和下游分析。接下来，我们设计了一个多阶段预处理流程，从家庭护理录音中提取目标说话者的短语音段进行声学分析。然后，我们采用音频语言模型（ALM）生成语音生物标志物的通俗描述，并检查其与个体健康状态的关联。我们的实验结果对商业和开源LLM在估计疾病评分方面进行了基准测试，证明了它们与实际临床结果的一致性，并揭示SOAP笔记比生命体征信息量大得多。基于疾病评分，我们首次提供了ALM能够从家庭护理录音中识别与健康相关的声学模式并以人类可读形式呈现的证据。这些发现共同突显了LLM和ALM利用异构家庭访问数据进行更好患者监测和护理的潜力。",
    "subjects": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2025-10-22",
    "paper_authors": "Yu-Wen Chen, William Ho, Sasha M. Vergez, Grace Flaherty, Pallavi Gupta, Zhihong Zhang, Maryam Zolnoori, Margaret V. McDonald, Maxim Topaz, Zoran Kostic, Julia Hirschberg",
    "topic": [
      "Audio Representation Learning",
      "Speech Recognition"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Joint Estimation of Piano Dynamics and Metrical Structure with a Multi-task Multi-Scale Network",
    "paper_title_zh": "基于多任务多尺度网络的钢琴力度与节拍结构联合估计",
    "paper_id": "2510.18190",
    "paper_abstract": "Estimating piano dynamic from audio recordings is a fundamental challenge in computational music analysis. In this paper, we propose an efficient multi-task network that jointly predicts dynamic levels, change points, beats, and downbeats from a shared latent representation. These four targets form the metrical structure of dynamics in the music score. Inspired by recent vocal dynamic research, we use a multi-scale network as the backbone, which takes Bark-scale specific loudness as the input feature. Compared to log-Mel as input, this reduces model size from 14.7 M to 0.5 M, enabling long sequential input. We use a 60-second audio length in audio segmentation, which doubled the length of beat tracking commonly used. Evaluated on the public MazurkaBL dataset, our model achieves state-of-the-art results across all tasks. This work sets a new benchmark for piano dynamic estimation and delivers a powerful and compact tool, paving the way for large-scale, resource-efficient analysis of musical expression.",
    "paper_abstract_zh": "从音频记录中估计钢琴力度是计算音乐分析领域的基础性挑战。本文提出了一种高效的多任务网络，通过共享潜在表示联合预测力度等级、变化点、节拍和强拍。这四个目标构成了乐谱中力度的节拍结构。受近期声乐力度研究的启发，我们采用多尺度网络作为主干网络，以Bark尺度特定响度作为输入特征。相较于使用log-Mel作为输入，该方法将模型参数量从14.7M减少至0.5M，从而支持更长序列输入。在音频分割中采用60秒长度，使节拍跟踪的输入长度翻倍。在公开的MazurkaBL数据集上的评估表明，该模型在所有任务上均达到最先进水平。本研究为钢琴力度估计设立了新基准，并提供了一个强大且紧凑的工具，为音乐表现力的大规模资源高效分析奠定了基础。",
    "subjects": [
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "update_time": "2025-10-22",
    "paper_authors": "Zhanhong He, Hanyu Meng, David Huang, Roberto Togneri",
    "topic": [
      "Audio Representation Learning",
      "Music Information Retrieval"
    ],
    "category": [
      "Music"
    ]
  },
  {
    "paper_title": "Adaptive Per-Channel Energy Normalization Front-end for Robust Audio Signal Processing",
    "paper_title_zh": "用于鲁棒音频信号处理的自适应每通道能量归一化前端",
    "paper_id": "2510.18206",
    "paper_abstract": "In audio signal processing, learnable front-ends have shown strong performance across diverse tasks by optimizing task-specific representation. However, their parameters remain fixed once trained, lacking flexibility during inference and limiting robustness under dynamic complex acoustic environments. In this paper, we introduce a novel adaptive paradigm for audio front-ends that replaces static parameterization with a closed-loop neural controller. Specifically, we simplify the learnable front-end LEAF architecture and integrate a neural controller for adaptive representation via dynamically tuning Per-Channel Energy Normalization. The neural controller leverages both the current and the buffered past subband energies to enable input-dependent adaptation during inference. Experimental results on multiple audio classification tasks demonstrate that the proposed adaptive front-end consistently outperforms prior fixed and learnable front-ends under both clean and complex acoustic conditions. These results highlight neural adaptability as a promising direction for the next generation of audio front-ends.",
    "paper_abstract_zh": "在音频信号处理中，可学习前端通过优化任务特定表示在各种任务中表现出强大的性能。然而，它们的参数一旦训练完成就保持固定，在推理过程中缺乏灵活性，限制了在动态复杂声学环境下的鲁棒性。在本文中，我们提出了一种新颖的音频前端自适应范式，用闭环神经控制器替代静态参数化。具体来说，我们简化了可学习前端LEAF架构，并集成了一个神经控制器，通过动态调整每通道能量归一化来实现自适应表示。神经控制器利用当前和缓冲的过去子带能量，在推理过程中实现输入相关的自适应。在多个音频分类任务上的实验结果表明，所提出的自适应前端在干净和复杂的声学条件下都一致地优于先前的固定和可学习前端。这些结果突显了神经适应性作为下一代音频前端的有前景方向。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)",
      "Signal Processing (eess.SP)"
    ],
    "update_time": "2025-10-22",
    "paper_authors": "Hanyu Meng, Vidhyasaharan Sethu, Eliathamby Ambikairajah, Qiquan Zhang, Haizhou Li",
    "topic": [
      "Audio Representation Learning",
      "Audio Classification"
    ],
    "category": [
      "Other"
    ]
  },
  {
    "paper_title": "MVDR Beamforming for Cyclostationary Processes",
    "paper_title_zh": "循环平稳过程的MVDR波束形成",
    "paper_id": "2510.18391",
    "paper_abstract": "Conventional acoustic beamformers assume that noise is stationary within short time frames. This assumption prevents them from exploiting correlations between frequencies in almost-periodic noise sources such as musical instruments, fans, and engines. These signals exhibit periodically varying statistics and are better modeled as cyclostationary processes. This paper introduces the cyclic MVDR (cMVDR) beamformer, an extension of the conventional MVDR that leverages both spatial and spectral correlations to improve noise reduction, particularly in low-SNR scenarios. The method builds on frequency-shifted (FRESH) filtering, where shifted versions of the input are combined to attenuate or amplify components that are coherent across frequency. To address inharmonicity, where harmonic partials deviate from exact integer multiples of the fundamental frequency, we propose a data-driven strategy that estimates resonant frequencies via periodogram analysis and computes the frequency shifts from their spacing. Analytical and experimental results demonstrate that performance improves with increasing spectral correlation. On real recordings, the cMVDR achieves up to 5 dB gain in scale-invariant signal-to-distortion ratio (SI-SDR) over the MVDR and remains effective even with a single microphone. Code is available at this https URL.",
    "paper_abstract_zh": "传统的声学波束形成器假设噪声在短时间内是平稳的。这一假设使得它们无法利用音乐乐器、风扇和发动机等几乎周期性噪声源中频率之间的相关性。这些信号表现出周期性变化的统计特性，更适合被建模为循环平稳过程。本文引入了循环MVDR（cMVDR）波束形成器，这是传统MVDR的扩展，它利用空间和频谱相关性来提高降噪性能，特别是在低信噪比场景下。该方法基于频移（FRESH）滤波，其中输入的移位版本被组合起来，以衰减或增强频率间相干的分量。为了解决非谐波性问题（即谐波分量偏离基频的整数倍），我们提出了一种数据驱动策略，通过周期图分析估计共振频率，并根据它们的间距计算频移。分析和实验结果表明，性能随着频谱相关性的增加而提高。在真实录音中，cMVDR在尺度不变信噪比（SI-SDR）上比MVDR提高了最多5 dB，并且即使在单个麦克风的情况下也保持有效。代码可在提供的URL获取。",
    "subjects": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2025-10-22",
    "paper_authors": "Giovanni Bologni, Martin Bo Møller, Richard Heusdens, Richard C. Hendriks",
    "topic": [
      "Speech Enhancement",
      "Other"
    ],
    "category": [
      "Other"
    ]
  },
  {
    "paper_title": "ProLAP: Probabilistic Language-Audio Pre-Training",
    "paper_title_zh": "ProLAP: 概率语言-音频预训练",
    "paper_id": "2510.18423",
    "paper_abstract": "Language-audio joint representation learning frameworks typically depend on deterministic embeddings, assuming a one-to-one correspondence between audio and text. In real-world settings, however, the language-audio relationship is inherently many-to-many: one audio segment can be described by multiple captions and vice versa. To address this, we propose Probabilistic Language-Audio Pre-training (ProLAP), which models multiplicity as the spread of probability distributions in a joint language-audio embedding space. To train the intra-modal hierarchical relationship effectively, we also introduce two objectives: (i) hierarchical inclusion loss to promote semantic hierarchical understanding of inputs and (ii) mask repulsive loss to improve the efficiency of learning when optimizing the hierarchical inclusion loss. With this training strategy, our model can learn the hierarchical structure inherent in the data even from small datasets, in contrast to prior probabilistic approaches that rely on large-scale datasets. In our experiments, ProLAP outperforms existing deterministic approaches on audio-text retrieval tasks. Moreover, through experiments on the audio traversal task introduced in this paper, we demonstrate that ProLAP captures the plausible semantic hierarchy.",
    "paper_abstract_zh": "语言-音频联合表征学习框架通常依赖于确定性嵌入，假设音频和文本之间存在一一对应关系。然而，在现实世界中，语言-音频关系本质上是多对多的：一个音频片段可以被多个字幕描述，反之亦然。为此，我们提出了概率语言-音频预训练（ProLAP），它将多样性建模为联合语言-嵌入空间中概率分布的扩散。为了有效训练模态内的层次关系，我们还引入了两个目标：（i）层次包含损失，以促进对输入的语义层次理解；（ii）掩码排斥损失，以提高优化层次包含损失时的学习效率。通过这种训练策略，我们的模型即使从小数据集中也能学习数据固有的层次结构，这与依赖大规模数据集的先前概率方法形成对比。在我们的实验中，ProLAP在音频-文本检索任务上优于现有的确定性方法。此外，通过本文提出的音频遍历任务的实验，我们证明了ProLAP能够捕捉合理的语义层次结构。",
    "subjects": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2025-10-22",
    "paper_authors": "Toranosuke Manabe, Yuchi Ishikawa, Hokuto Munakata, Tatsuya Komatsu",
    "topic": [
      "Audio Representation Learning"
    ],
    "category": [
      "Other"
    ]
  },
  {
    "paper_title": "Diffusion Buffer for Online Generative Speech Enhancement",
    "paper_title_zh": "用于在线生成式语音增强的扩散缓冲区",
    "paper_id": "2510.18744",
    "paper_abstract": "Online Speech Enhancement was mainly reserved for predictive models. A key advantage of these models is that for an incoming signal frame from a stream of data, the model is called only once for enhancement. In contrast, generative Speech Enhancement models often require multiple calls, resulting in a computational complexity that is too high for many online speech enhancement applications. This work presents the Diffusion Buffer, a generative diffusion-based Speech Enhancement model which only requires one neural network call per incoming signal frame from a stream of data and performs enhancement in an online fashion on a consumer-grade GPU. The key idea of the Diffusion Buffer is to align physical time with Diffusion time-steps. The approach progressively denoises frames through physical time, where past frames have more noise removed. Consequently, an enhanced frame is output to the listener with a delay defined by the Diffusion Buffer, and the output frame has a corresponding look-ahead. In this work, we extend upon our previous work by carefully designing a 2D convolutional UNet architecture that specifically aligns with the Diffusion Buffer's look-ahead. We observe that the proposed UNet improves performance, particularly when the algorithmic latency is low. Moreover, we show that using a Data Prediction loss instead of Denoising Score Matching loss enables flexible control over the trade-off between algorithmic latency and quality during inference. The extended Diffusion Buffer equipped with a novel NN and loss function drastically reduces the algorithmic latency from 320 - 960 ms to 32 - 176 ms with an even increased performance. While it has been shown before that offline generative diffusion models outperform predictive approaches in unseen noisy speech data, we confirm that the online Diffusion Buffer also outperforms its predictive counterpart on unseen noisy speech data.",
    "paper_abstract_zh": "在线语音增强主要保留用于预测模型。这些模型的一个关键优势是，对于来自数据流的输入信号帧，模型只需调用一次进行增强。相比之下，生成式语音增强模型通常需要多次调用，导致计算复杂度对于许多在线语音增强应用来说过高。本研究提出了扩散缓冲区，这是一种基于生成式扩散的语音增强模型，对于来自数据流的每个输入信号帧只需要一次神经网络调用，并能在消费级GPU上以在线方式执行增强。扩散缓冲区的关键思想是将物理时间与扩散时间步长对齐。该方法通过物理时间逐步去噪帧，其中过去的帧去除了更多噪声。因此，增强后的帧以由扩散缓冲区定义的延迟输出给听众，并且输出帧具有相应的前瞻性。在本研究中，我们通过精心设计一个2D卷积UNet架构来扩展我们之前的工作，该架构专门与扩散缓冲区的前瞻性对齐。我们观察到，提出的UNet提高了性能，特别是在算法延迟较低时。此外，我们表明使用数据预测损失而不是去噪分数匹配损失可以在推理过程中灵活控制算法延迟和质量之间的权衡。配备新颖神经网络和损失函数的扩展扩散缓冲区将算法延迟从320-960毫秒大幅减少到32-176毫秒，同时性能甚至有所提高。虽然之前已经表明离线生成式扩散模型在未见过的嘈杂语音数据上优于预测方法，但我们确认在线扩散缓冲区在未见过的嘈杂语音数据上也优于其预测对应方法。",
    "subjects": [
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "update_time": "2025-10-22",
    "paper_authors": "Bunlong Lay, Rostislav Makarov, Simon Welker, Maris Hillemann, Timo Gerkmann",
    "topic": [
      "Speech Enhancement"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Transformer Redesign for Late Fusion of Audio-Text Features on Ultra-Low-Power Edge Hardware",
    "paper_title_zh": "面向超低功耗边缘硬件的音频-文本特征后期融合Transformer重新设计",
    "paper_id": "2510.18036",
    "paper_abstract": "Deploying emotion recognition systems in real-world environments where devices must be small, low-power, and private remains a significant challenge. This is especially relevant for applications such as tension monitoring, conflict de-escalation, and responsive wearables, where cloud-based solutions are impractical. Multimodal emotion recognition has advanced through deep learning, but most systems remain unsuitable for deployment on ultra-constrained edge devices. Prior work typically relies on powerful hardware, lacks real-time performance, or uses unimodal input. This paper addresses that gap by presenting a hardware-aware emotion recognition system that combines acoustic and linguistic features using a late-fusion architecture optimised for Edge TPU. The design integrates a quantised transformer-based acoustic model with frozen keyword embeddings from a DSResNet-SE network, enabling real-time inference within a 1.8MB memory budget and 21-23ms latency. The pipeline ensures spectrogram alignment between training and deployment using MicroFrontend and MLTK. Evaluation on re-recorded, segmented IEMOCAP samples captured through the Coral Dev Board Micro microphone shows a 6.3% macro F1 improvement over unimodal baselines. This work demonstrates that accurate, real-time multimodal emotion inference is achievable on microcontroller-class edge platforms through task-specific fusion and hardware-guided model design.",
    "paper_abstract_zh": "在现实环境中部署情感识别系统，其中设备必须小巧、低功耗且保护隐私，仍然是一个重大挑战。这对于诸如紧张监测、冲突降级和响应式可穿戴设备等应用尤为重要，因为基于云的解决方案在这些场景中并不实用。多模态情感识别通过深度学习取得了进展，但大多数系统仍不适合部署在超受限的边缘设备上。先前的工作通常依赖强大的硬件，缺乏实时性能，或使用单模态输入。本文通过提出一种硬件感知的情感识别系统来填补这一空白，该系统采用针对Edge TPU优化的后期融合架构，结合声学和语言学特征。该设计集成了量化的基于Transformer的声学模型和来自DSResNet-SE网络的冻结关键词嵌入，在1.8MB内存预算和21-23ms延迟内实现实时推理。该流水线使用MicroFrontend和MLTK确保训练和部署之间的频谱图对齐。通过Coral Dev Board Micro麦克风重新录制和分割的IEMOCAP样本评估显示，与单模态基线相比，宏观F1提高了6.3%。这项工作表明，通过任务特定的融合和硬件引导的模型设计，在微控制器类边缘平台上可以实现准确、实时的多模态情感推理。",
    "subjects": [
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "update_time": "2025-10-22",
    "paper_authors": "Stavros Mitsis, Ermos Hadjikyriakos, Humaid Ibrahim, Savvas Neofytou, Shashwat Raman, James Myles, Eiman Kanjo",
    "topic": [
      "Audio Representation Learning",
      "Audio Classification"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation",
    "paper_title_zh": "ParaStyleTTS：面向高效且鲁棒的副语言风格控制的富有表现力文本转语音生成",
    "paper_id": "2510.18308",
    "paper_abstract": "Controlling speaking style in text-to-speech (TTS) systems has become a growing focus in both academia and industry. While many existing approaches rely on reference audio to guide style generation, such methods are often impractical due to privacy concerns and limited accessibility. More recently, large language models (LLMs) have been used to control speaking style through natural language prompts; however, their high computational cost, lack of interpretability, and sensitivity to prompt phrasing limit their applicability in real-time and resource-constrained environments. In this work, we propose ParaStyleTTS, a lightweight and interpretable TTS framework that enables expressive style control from text prompts alone. ParaStyleTTS features a novel two-level style adaptation architecture that separates prosodic and paralinguistic speech style modeling. It allows fine-grained and robust control over factors such as emotion, gender, and age. Unlike LLM-based methods, ParaStyleTTS maintains consistent style realization across varied prompt formulations and is well-suited for real-world applications, including on-device and low-resource deployment. Experimental results show that ParaStyleTTS generates high-quality speech with performance comparable to state-of-the-art LLM-based systems while being 30x faster, using 8x fewer parameters, and requiring 2.5x less CUDA memory. Moreover, ParaStyleTTS exhibits superior robustness and controllability over paralinguistic speaking styles, providing a practical and efficient solution for style-controllable text-to-speech generation. Demo can be found at this https URL. Code can be found at this https URL.",
    "paper_abstract_zh": "在文本转语音（TTS）系统中控制说话风格已成为学术界和产业界日益关注的焦点。尽管现有方法通常依赖参考音频来引导风格生成，但由于隐私顾虑和可访问性限制，这些方法往往不切实际。近期，大型语言模型（LLMs）通过自然语言提示控制说话风格的方法逐渐兴起；然而，其高计算成本、缺乏可解释性以及对提示措辞的敏感性限制了其在实时和资源受限环境中的应用。本文提出ParaStyleTTS，一种轻量级且可解释的TTS框架，仅通过文本提示即可实现富有表现力的风格控制。该框架采用创新的两级风格适应架构，将韵律和副语言语音风格建模分离，能够对情感、性别和年龄等因素进行细粒度且鲁棒的控制。与基于LLM的方法不同，ParaStyleTTS在不同提示表述下均能保持稳定的风格实现，适用于实际应用场景，包括设备端和低资源部署。实验结果表明，ParaStyleTTS生成的语音质量与最先进LLM系统相当，同时具备30倍速度优势、参数量减少8倍、CUDA内存占用降低2.5倍等特性。此外，ParaStyleTTS在副语言说话风格方面展现出更优的鲁棒性和可控性，为风格可控的文本转语音生成提供了实用高效的解决方案。演示地址和代码仓库详见论文提供的URL。",
    "subjects": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2025-10-22",
    "paper_authors": "Haowei Lou, Hye-Young Paik, Wen Hu, Lina Yao",
    "topic": [
      "Speech Synthesis",
      "Audio Representation Learning"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "A Stage-Wise Learning Strategy with Fixed Anchors for Robust Speaker Verification",
    "paper_title_zh": "基于固定锚点的阶段性学习策略用于鲁棒的说话人验证",
    "paper_id": "2510.18530",
    "paper_abstract": "Learning robust speaker representations under noisy conditions presents significant challenges, which requires careful handling of both discriminative and noise-invariant properties. In this work, we proposed an anchor-based stage-wise learning strategy for robust speaker representation learning. Specifically, our approach begins by training a base model to establish discriminative speaker boundaries, and then extract anchor embeddings from this model as stable references. Finally, a copy of the base model is fine-tuned on noisy inputs, regularized by enforcing proximity to their corresponding fixed anchor embeddings to preserve speaker identity under distortion. Experimental results suggest that this strategy offers advantages over conventional joint optimization, particularly in maintaining discrimination while improving noise robustness. The proposed method demonstrates consistent improvements across various noise conditions, potentially due to its ability to handle boundary stabilization and variation suppression separately.",
    "paper_abstract_zh": "在嘈杂条件下学习鲁棒的说话人表示具有重大挑战，这需要谨慎处理判别性和噪声不变性。在这项工作中，我们提出了一种基于锚点的阶段性学习策略，用于鲁棒的说话人表示学习。具体而言，我们的方法首先训练一个基础模型来建立判别性的说话人边界，然后从该模型中提取锚嵌入作为稳定的参考。最后，在嘈杂输入上对基础模型的一个副本进行微调，通过强制其接近相应的固定锚嵌入来正则化，以在失真情况下保持说话人身份。实验结果表明，与传统联合优化相比，这种策略在保持判别性的同时提高噪声鲁棒性方面具有优势。所提出的方法在各种噪声条件下都表现出一致的改进，这可能是因为它能够分别处理边界稳定性和变化抑制。",
    "subjects": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2025-10-22",
    "paper_authors": "Bin Gu, Lipeng Dai, Huipeng Du, Haitao Zhao, Jibo Wei",
    "topic": [
      "Audio Representation Learning",
      "Speech Recognition"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Noise-Conditioned Mixture-of-Experts Framework for Robust Speaker Verification",
    "paper_title_zh": "基于噪声条件混合专家框架的鲁棒说话人验证",
    "paper_id": "2510.18533",
    "paper_abstract": "Robust speaker verification under noisy conditions remains an open challenge. Conventional deep learning methods learn a robust unified speaker representation space against diverse background noise and achieve significant improvement. In contrast, this paper presents a noise-conditioned mixture-ofexperts framework that decomposes the feature space into specialized noise-aware subspaces for speaker verification. Specifically, we propose a noise-conditioned expert routing mechanism, a universal model based expert specialization strategy, and an SNR-decaying curriculum learning protocol, collectively improving model robustness and generalization under diverse noise conditions. The proposed method can automatically route inputs to expert networks based on noise information derived from the inputs, where each expert targets distinct noise characteristics while preserving speaker identity information. Comprehensive experiments demonstrate consistent superiority over baselines, confirming that explicit noise-dependent feature modeling significantly enhances robustness without sacrificing verification accuracy.",
    "paper_abstract_zh": "在噪声条件下的鲁棒说话人验证仍然是一个开放性挑战。传统的深度学习方法学习一个统一的鲁棒说话人表示空间来对抗各种背景噪声，并取得了显著改进。相比之下，本文提出了一种噪声条件混合专家框架，将特征空间分解为专门的噪声感知子空间用于说话人验证。具体来说，我们提出了一种噪声条件专家路由机制、基于通用模型的专家专业化策略以及信噪比衰减课程学习协议，共同提高了模型在各种噪声条件下的鲁棒性和泛化能力。所提出的方法可以根据从输入中导出的噪声信息自动将输入路由到专家网络，其中每个专家针对不同的噪声特征，同时保留说话人身份信息。全面的实验表明，该方法始终优于基线方法，证实了显式的噪声依赖特征建模显著增强了鲁棒性，同时不牺牲验证准确性。",
    "subjects": [
      "Multimedia (cs.MM)",
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "update_time": "2025-10-22",
    "paper_authors": "Bin Gu, Lipeng Dai, Huipeng Du, Haitao Zhao, Jibo Wei",
    "topic": [
      "Audio Representation Learning",
      "Speech Recognition"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Bayesian Low-Rank Factorization for Robust Model Adaptation",
    "paper_title_zh": "用于鲁棒模型适配的贝叶斯低秩分解",
    "paper_id": "2510.18723",
    "paper_abstract": "Large speech foundation models achieve strong performance across many domains, but they often require adaptation to handle local needs such as code-switching, where speakers mix languages within the same utterance. Direct fine-tuning of these models risks overfitting to the target domain and overwriting the broad capabilities of the base model. To address this challenge, we explore Bayesian factorized adapters for speech foundation models, which place priors near zero to achieve sparser adaptation matrices and thereby retain general performance while adapting to specific domains. We apply our approach to the Whisper model and evaluate on different multilingual code-switching scenarios. Our results show only minimal adaptation loss while significantly reducing catastrophic forgetting of the base model. Compared to LoRA, our method achieves a backward gain of 54% with only a 4% drop on the new domain. These findings highlight the effectiveness of Bayesian adaptation for fine-tuning speech foundation models without sacrificing generalization.",
    "paper_abstract_zh": "大型语音基础模型在许多领域表现出强大的性能，但它们通常需要适应本地需求，如代码切换（code-switching），即说话者在同一话语中混合使用多种语言。直接微调这些模型可能会导致过拟合到目标领域并覆盖基础模型的广泛能力。为解决这一挑战，我们探索了语音基础模型的贝叶斯分解适配器（Bayesian factorized adapters），这些适配器将先验值设置在接近零的位置，以实现更稀疏的适配矩阵，从而在适应特定领域的同时保留通用性能。我们将该方法应用于Whisper模型，并在不同的多语言代码切换场景下进行了评估。结果表明，我们的方法仅带来最小的适配损失，同时显著减少了基础模型的灾难性遗忘。与LoRA相比，我们的方法在新领域仅下降4%的情况下，实现了54%的向后增益。这些发现突显了贝叶斯适配在微调语音基础模型而不牺牲泛化能力方面的有效性。",
    "subjects": [
      "Sound (cs.SD)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2025-10-22",
    "paper_authors": "Enes Yavuz Ugan, Ngoc-Quan Pham, Alexander Waibel",
    "topic": [
      "Speech Recognition",
      "Audio Representation Learning"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "Adapting Language Balance in Code-Switching Speech",
    "paper_title_zh": "代码转换语音中的语言平衡调整",
    "paper_id": "2510.18724",
    "paper_abstract": "Despite achieving impressive results on standard benchmarks, large foundational models still struggle against code-switching test cases. When data scarcity cannot be used as the usual justification for poor performance, the reason may lie in the infrequent occurrence of code-switched moments, where the embedding of the second language appears subtly. Instead of expecting the models to learn this infrequency on their own, it might be beneficial to provide the training process with labels. Evaluating model performance on code-switching data requires careful localization of code-switching points where recognition errors are most consequential, so that the analysis emphasizes mistakes occurring at those moments. Building on this observation, we leverage the difference between the embedded and the main language to highlight those code-switching points and thereby emphasize learning at those locations. This simple yet effective differentiable surrogate mitigates context bias during generation -- the central challenge in code-switching -- thereby improving the model's robustness. Our experiments with Arabic and Chinese-English showed that the models are able to predict the switching places more correctly, reflected by the reduced substitution error.",
    "paper_abstract_zh": "尽管在标准基准测试中取得了令人印象深刻的结果，大型基础模型在面对代码转换测试案例时仍然存在困难。当数据稀缺不能作为性能不佳的常规理由时，问题可能在于代码转换时刻出现频率较低，其中第二语言的嵌入显得较为微妙。与其期望模型自行学习这种低频特性，不如在训练过程中提供标签可能更为有益。评估模型在代码转换数据上的性能需要精确定位代码转换点，这些点处的识别错误最为关键，从而突出分析这些时刻的错误。基于此观察，我们利用嵌入语言与主要语言之间的差异来突出代码转换点，从而强调在这些位置的学习。这种简单而有效的可微分代理缓解了生成过程中的上下文偏差——这是代码转换的核心挑战——从而提高了模型的鲁棒性。我们在阿拉伯语和中英混合语料上的实验表明，模型能够更准确地预测切换位置，替换错误率显著降低。",
    "subjects": [
      "Sound (cs.SD)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "update_time": "2025-10-22",
    "paper_authors": "Enes Yavuz Ugan, Ngoc-Quan Pham, Alexander Waibel",
    "topic": [
      "Speech Recognition"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "SegTune: Structured and Fine-Grained Control for Song Generation",
    "paper_title_zh": "SegTune：用于歌曲生成的结构化和细粒度控制",
    "paper_id": "2510.18416",
    "paper_abstract": "Recent advancements in song generation have shown promising results in generating songs from lyrics and/or global text prompts. However, most existing systems lack the ability to model the temporally varying attributes of songs, limiting fine-grained control over musical structure and dynamics. In this paper, we propose SegTune, a non-autoregressive framework for structured and controllable song generation. SegTune enables segment-level control by allowing users or large language models to specify local musical descriptions aligned to song this http URL segmental prompts are injected into the model by temporally broadcasting them to corresponding time windows, while global prompts influence the whole song to ensure stylistic coherence. To obtain accurate segment durations and enable precise lyric-to-music alignment, we introduce an LLM-based duration predictor that autoregressively generates sentence-level timestamped lyrics in LRC format. We further construct a large-scale data pipeline for collecting high-quality songs with aligned lyrics and prompts, and propose new evaluation metrics to assess segment-level alignment and vocal attribute consistency. Experimental results show that SegTune achieves superior controllability and musical coherence compared to existing baselines. See this https URL for demos of our work.",
    "paper_abstract_zh": "最近的歌曲生成研究在根据歌词和/或全局文本提示生成歌曲方面取得了 promising 的结果。然而，大多数现有系统缺乏对歌曲时变属性建模的能力，限制了音乐结构和动态的细粒度控制。在本文中，我们提出了 SegTune，一种用于结构化和可控歌曲生成的非自回归框架。SegTune 通过允许用户或大型语言模型指定与歌曲片段对齐的局部音乐描述，实现了片段级别的控制。片段提示通过时间广播方式注入模型到相应的时间窗口，而全局提示则影响整个歌曲以确保风格一致性。为了获得准确的片段持续时间并实现精确的歌词与音乐对齐，我们引入了一个基于 LLM 的持续时间预测器，该预测器自回归生成 LRC 格式的句子级带时间戳的歌词。我们进一步构建了一个大规模数据管道，用于收集带有对齐歌词和提示的高质量歌曲，并提出了新的评估指标来评估片段级对齐和声乐属性一致性。实验结果表明，与现有基线相比，SegTune 实现了更好的可控性和音乐连贯性。请访问此链接查看我们工作的演示。",
    "subjects": [
      "Sound (cs.SD)"
    ],
    "update_time": "2025-10-22",
    "paper_authors": "Pengfei Cai, Joanna Wang, Haorui Zheng, Xu Li, Zihao Ji, Teng Ma, Zhongliang Liu, Chen Zhang, Pengfei Wan",
    "topic": [
      "Music Generation",
      "Audio Representation Learning"
    ],
    "category": [
      "Music"
    ]
  },
  {
    "paper_title": "SAC: Neural Speech Codec with Semantic-Acoustic Dual-Stream Quantization",
    "paper_title_zh": "SAC：具有语义-声学双流量化的神经语音编解码器",
    "paper_id": "2510.16841",
    "paper_abstract": "Speech codecs that convert continuous speech signals into discrete tokens have become essential for speech language models (SLMs). However, existing codecs struggle to balance high-quality reconstruction with semantically rich representations, limiting their effectiveness in both generative and understanding tasks. In this work, we propose SAC, a neural speech codec with semantic-acoustic dual-stream quantization. By disentangling semantic and acoustic modeling into two dedicated streams, SAC enables each to be optimized for its respective role. Comprehensive evaluations show that SAC achieves strong reconstruction performance across diverse bitrates under both clean and noisy conditions, with particularly high scores on UTMOS and WER, demonstrating superior perceptual quality and intelligibility. Moreover, SAC substantially outperforms state-of-the-art codecs in semantic representation, achieving a level comparable to that of self-supervised learning (SSL) continuous embeddings. Finally, our analysis of speech disentanglement highlights the effectiveness of the dual-stream design, offering new potential for controllable speech applications.",
    "paper_abstract_zh": "将连续语音信号转换为离散标记的语音编解码器已成为语音语言模型(SLM)的重要组成部分。然而，现有的编解码器难以在高质量重建和语义丰富表示之间取得平衡，限制了它们在生成和理解任务中的有效性。在这项工作中，我们提出了SAC，一种具有语义-声学双流量化的神经语音编解码器。通过将语义和声学建模分离到两个专用流中，SAC使每个流都能针对其各自的角色进行优化。全面的评估表明，SAC在干净和噪声条件下，各种比特率下都实现了强大的重建性能，特别是在UTMOS和WER上获得了高分，展示了卓越的感知质量和可懂度。此外，SAC在语义表示方面显著优于最先进的编解码器，达到了与自监督学习(SSL)连续嵌入相当的水平。最后，我们对语音解耦的分析突显了双流设计的有效性，为可控语音应用提供了新的潜力。",
    "subjects": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "update_time": "2025-10-22",
    "paper_authors": "Wenxi Chen, Xinsheng Wang, Ruiqi Yan, Yushen Chen, Zhikang Niu, Ziyang Ma, Xiquan Li, Yuzhe Liang, Hanlin Wen, Shunshun Yin, Ming Tao, Xie Chen",
    "topic": [
      "Audio Codec",
      "Audio Representation Learning"
    ],
    "category": [
      "Speech"
    ]
  },
  {
    "paper_title": "MLMA: Towards Multilingual with Mamba Based Architectures",
    "paper_title_zh": "MLMA：迈向基于Mamba架构的多语言处理",
    "paper_id": "2510.18684",
    "paper_abstract": "Multilingual automatic speech recognition (ASR) remains a challenging task, especially when balancing performance across high- and low-resource languages. Recent advances in sequence modeling suggest that architectures beyond Transformers may offer better scalability and efficiency. In this work, we introduce MLMA (Multilingual Language Modeling with Mamba for ASR), a new approach that leverages the Mamba architecture--an efficient state-space model optimized for long-context sequence processing--for multilingual ASR. Using Mamba, MLMA implicitly incorporates language-aware conditioning and shared representations to support robust recognition across diverse languages. Experiments on standard multilingual benchmarks show that MLMA achieves competitive performance compared to Transformer-based architectures. These results highlight Mamba's potential as a strong backbone for scalable, efficient, and accurate multilingual speech recognition.",
    "paper_abstract_zh": "多语言自动语音识别(ASR)仍然是一个具有挑战性的任务，特别是在平衡高资源和低资源语言性能方面。序列建模的最新进展表明，超越Transformer的架构可能提供更好的可扩展性和效率。在这项工作中，我们介绍了MLMA（用于ASR的多语言语言建模与Mamba），这是一种新方法，它利用Mamba架构——一种为长上下文序列处理优化的高效状态空间模型——进行多语言ASR。通过使用Mamba，MLMA隐式地包含语言感知条件和共享表示，以支持对多种语言的鲁棒识别。在标准多语言基准测试上的实验表明，MLMA与基于Transformer的架构相比具有竞争力的性能。这些结果突显了Mamba作为可扩展、高效和准确的多语言语音识别强大骨干的潜力。",
    "subjects": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)"
    ],
    "update_time": "2025-10-22",
    "paper_authors": "Mohamed Nabih Ali, Daniele Falavigna, Alessio Brutti",
    "topic": [
      "Speech Recognition",
      "Audio Representation Learning"
    ],
    "category": [
      "Speech"
    ]
  }
]